{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b61d84d-e9db-4529-a909-d1d62c567aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import doubleml as dml\n",
    "from doubleml.datasets import fetch_401K\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from causal_cocycle.model import cocycle_model,cocycle_outcome_model\n",
    "from causal_cocycle.optimise import *\n",
    "from causal_cocycle.loss_functions import Loss\n",
    "from causal_cocycle.conditioners import Lin_Conditioner,Lin_Conditioner_T,NN_RELU_Conditioner,NN_RELU_Conditioner_T\n",
    "from causal_cocycle.transformers import Transformer,Shift_layer,Scale_layer,RQS_layer,Inverse_layer\n",
    "from causal_cocycle.helper_functions import likelihood_loss,mmd,propensity_score, empirical_KR\n",
    "from causal_cocycle.kernels import *\n",
    "from causal_cocycle.regression_functionals import *\n",
    "from causal_cocycle.distribution_estimation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cc0a7b-462a-41eb-b2bd-aaa36960fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data and names\n",
    "Data = fetch_401K(return_type='DataFrame')\n",
    "data = Data.to_numpy()\n",
    "names = np.array(list(Data[:0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885e07cb-264f-410a-98b3-ec7111cec00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set-up (outside wrapper function)\n",
    "\n",
    "# Names\n",
    "covariates = ['age', 'inc', 'educ', 'fsize', 'marr',\n",
    "                 'twoearn', 'db', 'pira', 'hown', 'e401']\n",
    "treatment = [\"e401\"]\n",
    "outcome = [\"net_tfa\"]\n",
    "\n",
    "# Creating tensors\n",
    "X = Data[Data.columns.intersection(covariates)]\n",
    "names_x = np.array(list(X[:0]))\n",
    "treatment_ind = np.where(names_x == \"e401\")[0][0]\n",
    "X = X.to_numpy()\n",
    "cols_order = ([treatment_ind]+\n",
    "              list(np.linspace(0,treatment_ind-1,treatment_ind).astype(int))+\n",
    "              list(np.linspace(treatment_ind+1,len(X.T)-1,len(X.T)-1-treatment_ind).astype(int)))\n",
    "X = X[:,cols_order]\n",
    "N = len(X)\n",
    "Y = Data[Data.columns.intersection(outcome)].to_numpy().reshape(N,)\n",
    "X,Y = torch.tensor(X),torch.tensor(Y).view(N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4aca97-eab6-42d5-962e-e1d41ce74dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method + opt set up (outside wrapper function)\n",
    "\n",
    "# Treatment effect estimation\n",
    "splits = 5\n",
    "estimator = \"S-estimator\" \n",
    "\n",
    "# RKHS training and CV\n",
    "ls_method = \"med_heuristic\"\n",
    "hyper_lambda = 2**torch.linspace(-10,0,5)\n",
    "hyper_ls = 2**torch.linspace(-1,1,5)\n",
    "train_val_split = 0.8\n",
    "folds = int(1/(1-train_val_split))\n",
    "\n",
    "hyper_grid_lambda = hyper_lambda.repeat(len(hyper_ls))\n",
    "hyper_grid_ls = torch.repeat_interleave(hyper_ls,len(hyper_lambda))   \n",
    "\n",
    "# Propensity model training\n",
    "subsample = True\n",
    "subsamples = 1024\n",
    "miniter = 500\n",
    "maxiter = 500\n",
    "\n",
    "# Setting training optimiser args\n",
    "opt_args = [\"learn_rate\",\n",
    "            \"scheduler\",\n",
    "            \"batch_size\",\n",
    "            \"maxiter\",\n",
    "            \"miniter\",\n",
    "            \"weight_decay\",\n",
    "            \"print_\",\n",
    "            \"val_batch_size\"]\n",
    "opt_argvals = [learn_rate,\n",
    "              scheduler,\n",
    "              batch_size,\n",
    "             maxiter,\n",
    "              miniter,\n",
    "              weight_decay,\n",
    "              True,\n",
    "              val_batch_size]\n",
    "\n",
    "hyper = [\"weight_decay\"]\n",
    "hyper_val = [0]\n",
    "\n",
    "#Shorthand function calls\n",
    "def NN(i,o=1,width=128,layers=2):\n",
    "    return NN_RELU_Conditioner(width = width,\n",
    "                                     layers = layers, \n",
    "                                     input_dims =  i, \n",
    "                                     output_dims = o,\n",
    "                                     bias = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70641014-3c7b-4a32-b97a-3bbabe947ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage objects\n",
    "ATE_PI = torch.zeros(splits)\n",
    "ATE_IPW = torch.zeros(splits)\n",
    "ATE_DR = torch.zeros(splits)\n",
    "weights1,weights0 = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed1964e-16fc-4986-b277-5158bd51a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying dimensions\n",
    "N = len(X)\n",
    "D = len(X.T) - 1*(estimator==\"T-estimator\")\n",
    "P = D - 1*(estimator==\"S-estimator\")\n",
    "\n",
    "# Shuffling data\n",
    "torch.manual_seed(0)\n",
    "shuffled_inds = torch.randperm(Y.size()[0])\n",
    "X = X[shuffled_inds]\n",
    "Y = Y[shuffled_inds]\n",
    "\n",
    "# Scaling data\n",
    "scale_X = torch.ones(len(X.T))\n",
    "for i in range(len(X.T)):\n",
    "    if len(torch.unique(X[:,i])) > 2:\n",
    "        scale_X[i] = X[:,i].var(0)**0.5\n",
    "scale_Y = Y.var(0)**0.5\n",
    "Xscale,Yscale = X/scale_X, Y/scale_Y\n",
    "\n",
    "# Getting sample splits for DR estimation\n",
    "Xsplits = get_CV_splits(Xscale,splits)\n",
    "Ysplits = get_CV_splits(Yscale,splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0890a2f0-4375-47cc-895c-ec18d93cb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying models for cross-validation\n",
    "if cocycle == \"linear\":\n",
    "    conditioners_list = [[Lin_Conditioner(D,1)]]\n",
    "    transformers_list = [Transformer([Shift_layer()])]\n",
    "if cocycle == \"additive\":\n",
    "    conditioners_list = [[NN(D,1,width,layers)]]\n",
    "    transformers_list = [Transformer([Shift_layer()])]\n",
    "if cocycle == \"affine\":\n",
    "    conditioners_list = [[NN(D,1,width,layers)]*2]\n",
    "    transformers_list = [Transformer([Shift_layer(),Scale_layer()])]\n",
    "if cocycle == \"continuous\":\n",
    "    conditioners_list = [[NN(D,1,width,layers)]*3]\n",
    "    transformers_list = [Transformer([Shift_layer(),Scale_layer(),RQS_layer(RQS_bins)])]\n",
    "    \n",
    "models_validation = []\n",
    "for m in range(len(conditioners_list)):\n",
    "    models_validation.append(cocycle_model(conditioners_list[m],transformers_list[m]))\n",
    "hyper_args = [hyper]*len(conditioners_list)\n",
    "hyper_argvals = [hyper_val]*len(conditioners_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d05def3-a23a-4280-9dec-c5027f3be29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/ghome/live/danceh/Cocycles/Package/causal_cocycle/loss_functions.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706712279749/work/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch_inds = torch.tensor([np.random.choice(ind_list,subsamples)]).long().view(subsamples,)\n"
     ]
    }
   ],
   "source": [
    "# Getting loss functon (using CMMD_V as scalable for validation)\n",
    "loss_fn =  Loss(loss_fn = cocycle_loss,kernel = [gaussian_kernel(torch.ones(1),1)]*2)\n",
    "loss_fn_val =  Loss(loss_fn = \"CMMD_V\",kernel = [gaussian_kernel(torch.ones(1),1)]*2)\n",
    "loss_fn.median_heuristic(Xscale,Yscale,subsamples = med_heuristic_samples)\n",
    "loss_fn_val.median_heuristic(Xscale,Yscale,subsamples = med_heuristic_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b745d8-6353-408d-ae71-54cb651f1665",
   "metadata": {},
   "source": [
    "## S-estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfa626-2b55-4df6-b09b-bf774917f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(-0.5159)\n",
      "98.0  % completion\n",
      "Finished optimising final model\n",
      "iter 0 , loss =  tensor(0.2321)\n",
      "iter 10 , loss =  tensor(0.2220)\n",
      "iter 20 , loss =  tensor(0.2190)\n",
      "iter 30 , loss =  tensor(0.2223)\n",
      "iter 40 , loss =  tensor(0.2110)\n",
      "iter 50 , loss =  tensor(0.2153)\n",
      "iter 60 , loss =  tensor(0.2065)\n"
     ]
    }
   ],
   "source": [
    "# Doing ATE estimation\n",
    "if estimator == \"S-estimator\":\n",
    "    for k in range(splits):\n",
    "\n",
    "        # Getting dataset\n",
    "        Xtrain,Ytrain = Xsplits[k][0],Ysplits[k][0]\n",
    "        Xtest,Ytest = Xsplits[k][1],Ysplits[k][1]\n",
    "\n",
    "        # Getting model for kth split\n",
    "        final_model,val_losses = validate(models_validation,\n",
    "                                         loss_fn,\n",
    "                                         Xtrain,\n",
    "                                         Ytrain,\n",
    "                                         loss_fn_val,\n",
    "                                         validation_method,\n",
    "                                         train_val_split,\n",
    "                                         opt_args,\n",
    "                                         opt_argvals,\n",
    "                                         hyper_args,\n",
    "                                         hyper_argvals,\n",
    "                                         choose_best_model,\n",
    "                                         retrain)\n",
    "    \n",
    "        # Getting interventional inputs\n",
    "        Xtest1,Xtest0 = Xtest*1,Xtest*1\n",
    "        Xtest1[:,0],Xtest0[:,0] = 1,0\n",
    "    \n",
    "        # Getting potential outcomes and plug-in estimator\n",
    "        Y1scale = final_model.cocycle(Xtest1,Xtest,Ytest).detach()\n",
    "        Y0scale = final_model.cocycle(Xtest0,Xtest,Ytest).detach()\n",
    "        ATE_PI[k] = (Y1scale-Y0scale).mean()*scale_Y\n",
    "\n",
    "        # Estimating propensity model\n",
    "        kernel = exponential_kernel(lengthscale = torch.ones(P, requires_grad = True),scale = 1)\n",
    "        regressor = NW_functional(kernel)\n",
    "        propensity_model = Conditional_Expectation_Regressor(regressor)\n",
    "        losses = propensity_model.optimise(Xtrain[:,1:],Xtrain[:,:1].float(),\n",
    "                                    subsample = subsample,\n",
    "                                    miniter = miniter,\n",
    "                                    maxiter = maxiter,\n",
    "                                    subsamples = subsamples,\n",
    "                                    nfold = folds)\n",
    "\n",
    "        # Getting IPW weights and estimator\n",
    "        Probs = propensity_model.forward(Xtrain[:,:1],Xtrain[:,1:].float(),Xtest[:,1:].float()).detach()\n",
    "        weights1.append(Xtest[:,:1]/Probs)\n",
    "        weights0.append((1-Xtest[:,:1])/(1-Probs))\n",
    "        ATE_IPW[k] = ((weights1[k]-weights0[k])*Ytest*scale_Y).mean()\n",
    "\n",
    "        # Getting conditional expectations and DR estimator\n",
    "        EY1 = final_model.cocycle_outer(Xtest1,Xtest,Ytest).detach().mean(1)*scale_Y\n",
    "        EY0 = final_model.cocycle_outer(Xtest0,Xtest,Ytest).detach().mean(1)*scale_Y\n",
    "        ATE_DR[k] = ATE_PI[k]+ATE_IPW[k]\n",
    "        ATE_DR[k] += (weights0[k][:,0]*EY0 - weights1[k][:,0]*EY1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d062a-c274-4dea-8756-d5fd0e1d8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Probs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906219c6-8048-490b-bcaa-4a4d7382d8f3",
   "metadata": {},
   "source": [
    "## T-estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642c1b0-f281-4ff4-accb-98609034f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimator == \"T-estimator\":\n",
    "    Utest1_splits,Utest0_splits = [],[]\n",
    "    for k in range(splits):\n",
    "\n",
    "        # Getting dataset\n",
    "        Xtrain,Ytrain = Xsplits[k][0],Ysplits[k][0]\n",
    "        Xtest,Ytest = Xsplits[k][1],Ysplits[k][1]\n",
    "        treated_train,control_train = Xtrain[:,0]==1,Xtrain[:,0]==0\n",
    "        treated_test,control_test = Xtest[:,0]==1,Xtest[:,0]==0\n",
    "    \n",
    "        #  Splitting dataset into treated and control\n",
    "        Y_1train,X_1train = Ytrain[treated_train], Xtrain[treated_train,1:]\n",
    "        Y_0train,X_0train = Ytest[control_train], Xtest[control_train,1:]\n",
    "        Y_1test,X_1test = Ytest[treated_test], Xtest[treated_test,1:]\n",
    "        Y_0test,X_0test = Ytest[control_test], Xtest[control_test,1:]\n",
    "        \n",
    "        # T-estimation\n",
    "        final_model1,val_losses1 = validate(models_validation,\n",
    "                                             loss_fn,\n",
    "                                             X_1train,\n",
    "                                             Y_1train,\n",
    "                                             loss_fn_val,\n",
    "                                             validation_method,\n",
    "                                             train_val_split,\n",
    "                                             opt_args,\n",
    "                                             opt_argvals,\n",
    "                                             hyper_args,\n",
    "                                             hyper_argvals,\n",
    "                                             choose_best_model,\n",
    "                                             retrain)\n",
    "        \n",
    "        final_model0,val_losses0 = validate(models_validation,\n",
    "                                             loss_fn,\n",
    "                                             X_0train,\n",
    "                                             Y_0train,\n",
    "                                             loss_fn_val,\n",
    "                                             validation_method,\n",
    "                                             train_val_split,\n",
    "                                             opt_args,\n",
    "                                             opt_argvals,\n",
    "                                             hyper_args,\n",
    "                                             hyper_argvals,\n",
    "                                             choose_best_model,\n",
    "                                             retrain)\n",
    "        # Estimating KR transport\n",
    "        U_1train = final_model1.inverse_transformation(X_1train,Y_1train).detach()\n",
    "        U_0train = final_model0.inverse_transformation(X_0train,Y_0train).detach()\n",
    "        KR_map = empirical_KR(U_0train,U_1train)\n",
    "\n",
    "        # Getting base distribution test samples for entire fold\n",
    "        U_1test = final_model1.inverse_transformation(X_1test,Y_1test).detach()\n",
    "        U_0test = final_model0.inverse_transformation(X_0test,Y_0test).detach()\n",
    "        Utest1,Utest0 = torch.zeros((len(treated_test),1)), torch.zeros((len(treated_test),1))\n",
    "        Utest1[treated_test],Utest0[control_test] = U_1test,U_0test\n",
    "        Utest1[control_test],Utrain0[treated_test] = KR_map.forward(U_0test),KR_map.forward(U_1test)\n",
    "\n",
    "        # Saving samples for re-use\n",
    "        Utest1_splits.append(Utest1)\n",
    "        Utest0_splits.append(Utest0)\n",
    "        \n",
    "        # Getting potential outcomes and plug-in estimator\n",
    "        Y1scale =  final_model1.transformation(Xtest[:,1:],Utest1).detach()\n",
    "        Y0scale =  final_model0.transformation(Xtest[:,1:],Utest0).detach()\n",
    "        ATE_PI[k] = (Y1scale-Y0scale).mean()*scale_Y\n",
    "\n",
    "        # Estimating propensity model\n",
    "        kernel = exponential_kernel(lengthscale = torch.ones(P, requires_grad = True),scale = 1)\n",
    "        regressor = NW_functional(kernel)\n",
    "        propensity_model = Conditional_Expectation_Regressor(regressor)\n",
    "        losses = propensity_model.optimise(Xtrain[:,1:],Xtrain[:,:1].float(),\n",
    "                                    subsample = True,\n",
    "                                    miniter = 200,\n",
    "                                    maxiter = 200,\n",
    "                                    subsamples = 1024,\n",
    "                                    nfold = folds)\n",
    "\n",
    "        # Getting IPW weights and estimator\n",
    "        Probs = propensity_model.forward(Xtrain[:,:1],Xtrain[:,1:],Xtest[:,1:])\n",
    "        weights1.append(Xtest[:,:1]/Probs)\n",
    "        weights0 .append((1-Xtest[:,:1])/(1-Probs))\n",
    "        ATE_IPW[k] = ((weights1[k]-weights0[k])*Ytest*scale_Y).mean()\n",
    "\n",
    "        EY1 =  final_model1.transformation_outer(Xtest[:,1:],Utest1).detach().mean(1)*scale_Y\n",
    "        EY0 =  final_model0.transformation_outer(Xtest[:,1:],Utest0).detach().mean(1)*scale_Y\n",
    "\n",
    "        ATE_DR[k] = ATE_PI[k]+ATE_IPW[k]\n",
    "        ATE_DR[k] += (weights0[k][:,0]*EY0 - weights1[k][:,0]*EY1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d9759-6349-4079-bdf4-d09d98c62a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ATE_PI.mean(),ATE_DR.mean(),ATE_IPW.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
