{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c5a671-3cef-4526-a8c1-745cff7b0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import econml\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICSCORE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "from causal_cocycle.model import cocycle_model,flow_model,cocycle_outcome_model\n",
    "from causal_cocycle.optimise import *\n",
    "from causal_cocycle.loss_functions import Loss\n",
    "from causal_cocycle.conditioners import Empty_Conditioner,Constant_Conditioner,Lin_Conditioner,NN_RELU_Conditioner\n",
    "from causal_cocycle.transformers import Transformer,Shift_layer,Scale_layer,RQS_layer,Inverse_layer\n",
    "from causal_cocycle.helper_functions import likelihood_loss,mmd,propensity_score\n",
    "from causal_cocycle.kernels import *\n",
    "from causal_cocycle.kde import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6543c44a-c307-4b69-8532-03c03339f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30893/3195378115.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  psid = pd.concat(\n",
      "/tmp/ipykernel_30893/3195378115.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  psid = pd.concat(\n",
      "/tmp/ipykernel_30893/3195378115.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  nsw = pd.concat(\n",
      "/tmp/ipykernel_30893/3195378115.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  nsw = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "psid = pd.concat( \n",
    "    map(partial(pd.read_csv,sep = \"  \", header = None), ['psid_controls.txt',\n",
    "                                                         'psid2_controls.txt']), ignore_index=True)\n",
    "nsw = pd.concat( \n",
    "    map(partial(pd.read_csv,sep = \"  \", header = None), ['nsw_control.txt',\n",
    "                                                         'nsw_treated.txt']), ignore_index=True)\n",
    "\n",
    "psid_columns = [\"treat\",\"age\",\"educ\",\"black\",\"hispan\",\"married\",\"nodegree\",\"re74\",\"re75\",\"re78\"]\n",
    "nsw_columns = [\"treat\",\"age\",\"educ\",\"black\",\"hispan\",\"married\",\"nodegree\",\"re75\",\"re78\"]\n",
    "\n",
    "psid.columns = psid_columns\n",
    "nsw.columns = nsw_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98abb2ec-bf26-4a03-99d6-4c23e501cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting empirical ATE\n",
    "X,Y = torch.tensor(nsw['treat'].values),torch.tensor(nsw['re78'].values)\n",
    "ATE = Y[X==1].mean()-Y[X==0].mean()\n",
    "Prob_increase = (Y[X==1][:,None] >= Y[X==0][None]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c61a963-82ab-41ca-a70e-8e8981b0a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing data to train on\n",
    "N = len(psid)+len(nsw)\n",
    "D = len(nsw.T[:-1])\n",
    "Xtrain = torch.zeros((N,len(nsw_columns)-1))\n",
    "Ytrain = torch.row_stack((torch.tensor(psid['re78'].values)[:,None],torch.tensor(nsw['re78'].values)[:,None])).float()\n",
    "for i in range(len(nsw_columns)-1):\n",
    "    Xtrain[:,i] = torch.row_stack((torch.tensor(psid[nsw_columns[i]].values)[:,None],torch.tensor(nsw[nsw_columns[i]].values)[:,None])).T.float()\n",
    "\n",
    "# Shuffling data\n",
    "shuffled_inds = torch.randperm(Xtrain.size()[0])\n",
    "Xtrain = Xtrain[shuffled_inds]\n",
    "Ytrain = Ytrain[shuffled_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924d2e49-1e0e-4afb-9788-94ee3552a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method + opt set up\n",
    "cocycle_loss = \"CMMD_U\"\n",
    "batch_size =64\n",
    "validation_method = \"CV\"\n",
    "choose_best_model = \"overall\"\n",
    "layers = 2\n",
    "width = 64\n",
    "train_val_split = 0.8\n",
    "learn_rate = [1e-3]\n",
    "scheduler = True\n",
    "maxiter = 10000\n",
    "miniter = 10000\n",
    "weight_decay = 1e-3\n",
    "RQS_bins = 8\n",
    "val_batch_size = N\n",
    "\n",
    "# Setting training optimiser args\n",
    "opt_args = [\"learn_rate\",\n",
    "            \"scheduler\",\n",
    "            \"batch_size\",\n",
    "            \"maxiter\",\n",
    "            \"miniter\",\n",
    "            \"weight_decay\",\n",
    "            \"print_\",\n",
    "            \"val_batch_size\"]\n",
    "opt_argvals = [learn_rate,\n",
    "              scheduler,\n",
    "              batch_size,\n",
    "             maxiter,\n",
    "              miniter,\n",
    "              weight_decay,\n",
    "              True,\n",
    "              val_batch_size]\n",
    "\n",
    "hyper = []\n",
    "hyper_val = []\n",
    "\n",
    "#Shorthand function calls\n",
    "def NN(i,o=1,width=128,layers=2):\n",
    "    return NN_RELU_Conditioner(width = width,\n",
    "                                     layers = layers, \n",
    "                                     input_dims =  i, \n",
    "                                     output_dims = o,\n",
    "                                     bias = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ea57c5-e75f-450f-a93f-f7896062b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying models for cross-validation\n",
    "conditioners_list = [[Lin_Conditioner(D,1)]]#,\n",
    "                     #[NN(D,1,width,layers)]]\n",
    "transformers_list = [Transformer([Shift_layer()])]#,\n",
    "                     #Transformer([Shift_layer()])]\n",
    "models_validation = []\n",
    "for m in range(len(conditioners_list)):\n",
    "    models_validation.append(cocycle_model(conditioners_list[m],transformers_list[m]))\n",
    "hyper_args = [hyper]*len(conditioners_list)\n",
    "hyper_argvals = [hyper_val]*len(conditioners_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b3683c-1234-4b5e-9568-b348dd8c521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/ghome/live/danceh/Cocycles/Package/causal_cocycle/loss_functions.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706712279749/work/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch_inds = torch.tensor([np.random.choice(ind_list,subsamples)]).long().view(subsamples,)\n"
     ]
    }
   ],
   "source": [
    "# Scaling data\n",
    "Ymu,Ysc =  Ytrain.mean(),Ytrain.var()**0.5\n",
    "Xmu,Xsc =  Xtrain.mean(0),Xtrain.var(0)**0.5\n",
    "Yscale = (Ytrain - Ymu)/Ysc\n",
    "Xscale = (Xtrain - Xmu)/Xsc\n",
    "\n",
    "# Getting loss functon (using CMMD_V as scalable for validation)\n",
    "loss_fn =  Loss(loss_fn = cocycle_loss,kernel = [gaussian_kernel(torch.ones(1),1)]*2)\n",
    "loss_fn_val =  Loss(loss_fn = \"CMMD_V\",kernel = [gaussian_kernel(torch.ones(1),1)]*2)\n",
    "loss_fn.median_heuristic(Xscale,Yscale,subsamples = 10**3)\n",
    "loss_fn_val.median_heuristic(Xscale,Yscale,subsamples = 10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c81657-8479-4b82-beca-15cdffac8d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(-0.6778)\n",
      "99.9  % completion\n",
      "Currently optimising model  0 , for fold  4\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "final_models,val_losses = validate(models_validation,\n",
    "                                     loss_fn,\n",
    "                                     Xscale,\n",
    "                                     Yscale,\n",
    "                                     loss_fn_val,\n",
    "                                     validation_method,\n",
    "                                     train_val_split,\n",
    "                                     opt_args,\n",
    "                                     opt_argvals,\n",
    "                                     hyper_args,\n",
    "                                     hyper_argvals,\n",
    "                                     choose_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c99d6c-8e1e-473b-8787-1bdeb4801d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('stack.0.weight',\n",
       "              tensor([[-0.0118, -0.0275,  0.1165, -0.0555, -0.0024,  0.0742,  0.0048,  0.6574]])),\n",
       "             ('stack.0.bias', tensor([4.6041e-08]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_models[0].conditioner[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbfd7419-1096-4ebe-ba1f-7e18d323b796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-651.1846, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X1,X0 = Xtrain*1, Xtrain*1\n",
    "X1[:,0],X0[:,0] = 1,0\n",
    "X1scale = (X1 - Xmu)/Xsc\n",
    "X0scale = (X0 - Xmu)/Xsc\n",
    "effect = final_models[0].cocycle(X1scale,X0scale,Ytrain*0)-Ytrain*0\n",
    "print((effect*Ysc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1a66317-e7c6-4a24-8318-07f0b1e5be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dists = (Yscale[:100]- Yscale[:100].T)[...,None]\n",
    "Dists_pred = Yscale[:100,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "802298b9-60b5-42c2-8a86-cbb8987b6543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8380)\n",
      "10\n",
      "tensor(-0.8380)\n"
     ]
    }
   ],
   "source": [
    "kernel = gaussian_kernel(torch.ones(1),1)\n",
    "K1 = kernel.get_gram(Dists,Dists).mean()\n",
    "K2 = -2*kernel.get_gram(Dists_pred,Dists).mean()\n",
    "print(K1+K2)\n",
    "\n",
    "batchsize = max(1,min(100,int(10**5/100**2)))\n",
    "print(batchsize)\n",
    "nbatch = int(100/batchsize)\n",
    "K = 0\n",
    "for i in range(nbatch):\n",
    "    K += kernel.get_gram(Dists[i*batchsize:(i+1)*batchsize],Dists[i*batchsize:(i+1)*batchsize]).sum()/100**3\n",
    "    K += -2*kernel.get_gram(Dists_pred[i*batchsize:(i+1)*batchsize],Dists[i*batchsize:(i+1)*batchsize]).sum()/100**2\n",
    "print(K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
