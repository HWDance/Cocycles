{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0685d06-0217-48c8-bffa-786868198411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User\n",
    "user = \"nk1922\"\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal,Laplace,Uniform,Gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "os.chdir('C:/Users/{0}/OneDrive/Documents/Cocycles project/Cocycle_code'.format(user))\n",
    "from Cocycle_CDAGM import *\n",
    "from Cocycle_model import *\n",
    "from Cocycle_optimise import *\n",
    "from Cocycle_loss_functions import *\n",
    "from Conditioners import *\n",
    "from Transformers import *\n",
    "from KDE_estimation import *\n",
    "from Kernels import *\n",
    "from Helper_functions import *\n",
    "os.chdir('C:/Users/{0}/OneDrive/Documents/Cocycles project/Experiments_code'.format(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcee5f2-352f-4652-813a-873d3bc47c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set up\n",
    "RFF_features = False\n",
    "n_RFF =100\n",
    "median_heuristic = True\n",
    "train_val_split = 1\n",
    "conditioner_learn_rate = 1e-3\n",
    "transformer_learn_rate = 1e-3\n",
    "scheduler = True\n",
    "val_tol = 1e-3\n",
    "batch_size = 128\n",
    "val_loss = False\n",
    "maxiter = 5000\n",
    "miniter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6afaf72-a53b-4837-9d4f-574b30f17a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8914)\n"
     ]
    }
   ],
   "source": [
    "# Data generation\n",
    "torch.manual_seed(1)\n",
    "N,D,P = 100,10,10\n",
    "ntrain = int(train_val_split*N)\n",
    "alpha_X = 0.01\n",
    "alpha_U = 100\n",
    "R2 = 0.9\n",
    "X = Gamma(alpha_X,alpha_X**0.5).sample((N,D))-alpha_X**0.5\n",
    "B = torch.ones((D,1))*(torch.linspace(0,D-1,D)<P)[:,None]\n",
    "F = X @ B\n",
    "U = Gamma(alpha_U,alpha_U**0.5).sample((N,1))-alpha_U**0.5\n",
    "U *= 1/U.var()**0.5*((1-R2)/R2*F.var())**0.5\n",
    "Y = F + U\n",
    "print(1-(U**2).mean()/(Y**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3f3bcca-7103-4cdf-843c-e6a6f7728f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with L2\n",
    "LS_model = torch.linalg.solve(X.T @ X, X.T @ Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae07437d-02f3-4b52-914a-36d73d7f8200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(0.7900)\n",
      "Completion % : 99.82\n"
     ]
    }
   ],
   "source": [
    "# Training with L1\n",
    "inputs_train,outputs_train, inputs_val,outputs_val  = X[:ntrain],Y[:ntrain],X[ntrain:],Y[ntrain:]\n",
    "loss_fn = Loss(loss_fn = \"L1\",kernel = [gaussian_kernel(torch.ones(1),1),gaussian_kernel(torch.ones(1),1)])\n",
    "if RFF_features:\n",
    "    loss_fn.get_RFF_features(n_RFF)\n",
    "if median_heuristic:\n",
    "    loss_fn.median_heuristic(X,Y, subsamples = 10**4)\n",
    "conditioner = Lin_Conditioner(D,1)\n",
    "transformer = Shift_Transformer()\n",
    "L1_model = cocycle_model([conditioner],transformer)\n",
    "L1_model = Train(L1_model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,val_loss = val_loss,\n",
    "                             scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be30c6ac-b845-4ed8-beb6-8a9ee3da7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(0.0016)\n",
      "Completion % : 99.82\n"
     ]
    }
   ],
   "source": [
    "# Training with HSIC\n",
    "inputs_train,outputs_train, inputs_val,outputs_val  = X[:ntrain],Y[:ntrain],X[ntrain:],Y[ntrain:]\n",
    "loss_fn = Loss(loss_fn = \"HSIC\",kernel = [gaussian_kernel(torch.ones(1),1),gaussian_kernel(torch.ones(1),1)])\n",
    "if RFF_features:\n",
    "    loss_fn.get_RFF_features(n_RFF)\n",
    "if median_heuristic:\n",
    "    loss_fn.median_heuristic(X,Y, subsamples = 10**4)\n",
    "conditioner = Lin_Conditioner(D,1)\n",
    "transformer = Shift_Transformer()\n",
    "HSIC_model = cocycle_model([conditioner],transformer)\n",
    "HSIC_model = Train(HSIC_model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,val_loss = val_loss,\n",
    "                             scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd3f9ed6-0938-42a1-b905-3e2fab0638fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(0.2138)\n",
      "Completion % : 99.82\n"
     ]
    }
   ],
   "source": [
    "# Training with JMMD\n",
    "RFF_features = True\n",
    "inputs_train,outputs_train, inputs_val,outputs_val  = X[:ntrain],Y[:ntrain],X[ntrain:],Y[ntrain:]\n",
    "loss_fn = Loss(loss_fn = \"JMMD_M_RFF\",kernel = [gaussian_kernel(torch.ones(1),1),gaussian_kernel(torch.ones(1),1)])\n",
    "if RFF_features:\n",
    "    loss_fn.get_RFF_features(100)\n",
    "if median_heuristic:\n",
    "    loss_fn.median_heuristic(X,Y, subsamples = 10**4)\n",
    "conditioner = Lin_Conditioner(D,1)\n",
    "transformer = Shift_Transformer()\n",
    "JMMD_model = cocycle_model([conditioner],transformer)\n",
    "JMMD_model = Train(JMMD_model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,val_loss = val_loss,\n",
    "                             scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6556ffe7-d137-4298-994c-499c9ab35043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(-57.8958)\n",
      "Completion % : 99.82\n"
     ]
    }
   ],
   "source": [
    "# Training with CMMD\n",
    "RFF_features = False\n",
    "inputs_train,outputs_train, inputs_val,outputs_val  = X[:ntrain],Y[:ntrain],X[ntrain:],Y[ntrain:]\n",
    "loss_fn = Loss(loss_fn = \"CMMD_M\",kernel = [gaussian_kernel(torch.ones(1),1),gaussian_kernel(torch.ones(1),1)])\n",
    "if RFF_features:\n",
    "    loss_fn.get_RFF_features(n_RFF)\n",
    "if median_heuristic:\n",
    "    loss_fn.median_heuristic(X,Y, subsamples = 10**4)\n",
    "conditioner = Lin_Conditioner(D,1)\n",
    "transformer = Shift_Transformer()\n",
    "CMMD_model = cocycle_model([conditioner],transformer)\n",
    "CMMD_model = Train(CMMD_model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,val_loss = val_loss,\n",
    "                             scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a568527-8526-42d9-91b0-4710b0bbcc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5661) tensor(0.6708) tensor(0.6803) tensor(1.3782) tensor(1.7601)\n"
     ]
    }
   ],
   "source": [
    "print(((LS_model-B[:,0])**2).mean()**0.5,\n",
    "    ((L1_model.conditioner[0].state_dict()['stack.0.weight']-B[:,0])**2).mean()**0.5,\n",
    "    ((HSIC_model.conditioner[0].state_dict()['stack.0.weight']-B[:,0])**2).mean()**0.5,\n",
    "    ((JMMD_model.conditioner[0].state_dict()['stack.0.weight']-B[:,0])**2).mean()**0.5,\n",
    "    ((CMMD_model.conditioner[0].state_dict()['stack.0.weight']-B[:,0])**2).mean()**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a6a209-aa3f-4fab-8b3c-b81a40fdbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "#N,D = 100,10\n",
    "#alpha = 0.01\n",
    "#X = Gamma(alpha,alpha**0.5).sample((N,D))-alpha**0.5\n",
    "#X = Normal(0,1).sample((N,D))\n",
    "#lengthscale = median_heuristic(X)\n",
    "#print(lengthscale)\n",
    "#K = gaussian_kernel(lengthscale = lengthscale).get_gram(X,X)\n",
    "#H = torch.eye(N)#-torch.ones((N,N))/N\n",
    "#Lower_tri = torch.tril(K@H, diagonal=-1).view(N**2).sort(descending = True)[0]\n",
    "#Lower_tri = Lower_tri[Lower_tri!=0]\n",
    "\n",
    "#plt.hist(Lower_tri.detach().numpy(), bins = 100)\n",
    "#plt.show()\n",
    "#plt.hist((K@H).view(N**2,).detach().numpy(), bins = 100)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
