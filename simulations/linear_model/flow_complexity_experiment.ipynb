{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9711c08b-f56e-4d14-a27b-ec3b233d91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal,Laplace,Uniform,Gamma, Beta, Cauchy\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "# Causal_cocycle imports\n",
    "from causal_cocycle.model_factory import CocycleFactory\n",
    "from causal_cocycle.model_factory import FlowFactory\n",
    "from causal_cocycle.loss_factory import CocycleLossFactory\n",
    "from causal_cocycle.loss import FlowLoss\n",
    "from causal_cocycle.optimise_new import validate, optimise\n",
    "from causal_cocycle.kernels import gaussian_kernel\n",
    "from causal_cocycle.helper_functions import kolmogorov_distance\n",
    "from lm_config import opt_config, model_config\n",
    "\n",
    "\"\"\"\n",
    "Configs\n",
    "\"\"\"\n",
    "# Experimental set up\n",
    "seed = 3\n",
    "N,D,P = 1000,1,1\n",
    "sig_noise_ratio = 1\n",
    "\n",
    "\"\"\"\n",
    "Main\n",
    "\"\"\"\n",
    "# Object storage\n",
    "names = [\"L2\",\"L1\",\"HSIC\",\"URR\",\"CMMD-V\",\"CMMD-U\",\"True\"]\n",
    "Coeffs = torch.zeros((1,len(names),P))\n",
    "\n",
    "# Data generation\n",
    "\n",
    "# Drawing data\n",
    "torch.manual_seed(seed)\n",
    "X = Normal(1,1).sample((N,D))\n",
    "X *= 1/(D)**0.5\n",
    "B = torch.ones((D,1))*(torch.linspace(0,D-1,D)<P)[:,None]\n",
    "F = X @ B\n",
    "Utot = Normal(0,1).sample((2*N,1))/sig_noise_ratio**0.5\n",
    "#Utot = torch.sign(Uniform(-1,1).sample((2*N,1)))/sig_noise_ratio**0.5\n",
    "#Utot = Cauchy(0,1).sample((2*N,1))/sig_noise_ratio**0.5\n",
    "#Utot = (1/Gamma(1,1).sample((2*N,1))-1)/sig_noise_ratio**0.5\n",
    "U,Uint = Utot[:N], Utot[N:]\n",
    "Y = F + U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d72c59-3dce-48a3-b6b3-70042141bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed 4 candidate cocycle models.\n"
     ]
    }
   ],
   "source": [
    "# Cocycle model construction\n",
    "factory = CocycleFactory(1, model_config)\n",
    "models, hyper_args = factory.build_models()\n",
    "print(f\"Constructed {len(models)} candidate cocycle models.\")\n",
    "\n",
    "gauss_config,laplace_config = model_config.copy(),model_config.copy()\n",
    "gauss_config['base_distribution_configs'], laplace_config['base_distribution_configs'] = [\"Normal\"]*4, [\"Laplace\"]*4\n",
    "models_gauss, hyper_args  = FlowFactory(1, gauss_config).build_models()\n",
    "models_laplace, hyper_args  = FlowFactory(1, laplace_config).build_models()\n",
    "\n",
    "models_urr_gauss,models_urr_laplace = deepcopy(models_gauss),deepcopy(models_laplace)\n",
    "for i in range(len(models_urr_gauss)):\n",
    "    models_urr_gauss[i].transformer.logdet = False\n",
    "    models_urr_laplace[i].transformer.logdet = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756e72a0-62c5-4176-b287-b2f5e2e74c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_config['learn_rate'] = 0.01\n",
    "opt_config['epochs'] = 500\n",
    "opt_config['scheduler'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018af81a-897b-4b61-957d-78209c50ec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0036],\n",
       "        [ 0.9737]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtild = torch.column_stack((torch.ones((N,1)),X))\n",
    "torch.linalg.solve( Xtild.T @ Xtild, Xtild.T @ Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38df088f-f14d-43e6-b382-37a8523fbe5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Training Loss: 196.2502\n",
      "Epoch 2/500, Training Loss: 172.4278\n",
      "Epoch 3/500, Training Loss: 159.1290\n",
      "Epoch 4/500, Training Loss: 96.6615\n",
      "Epoch 5/500, Training Loss: 82.0629\n",
      "Epoch 6/500, Training Loss: 121.5600\n",
      "Epoch 7/500, Training Loss: 101.8954\n",
      "Epoch 8/500, Training Loss: 168.0363\n",
      "Epoch 9/500, Training Loss: 73.9710\n",
      "Epoch 10/500, Training Loss: 19.7233\n",
      "Epoch 11/500, Training Loss: 87.5817\n",
      "Epoch 12/500, Training Loss: 141.6195\n",
      "Epoch 13/500, Training Loss: 44.3104\n",
      "Epoch 14/500, Training Loss: 66.3462\n",
      "Epoch 15/500, Training Loss: 78.2021\n",
      "Epoch 16/500, Training Loss: 97.8843\n",
      "Epoch 17/500, Training Loss: 94.7571\n",
      "Epoch 18/500, Training Loss: 99.0725\n",
      "Epoch 19/500, Training Loss: 22.5608\n",
      "Epoch 20/500, Training Loss: 107.7922\n",
      "Epoch 21/500, Training Loss: 49.6228\n",
      "Epoch 22/500, Training Loss: 83.8626\n",
      "Epoch 23/500, Training Loss: 97.0127\n",
      "Epoch 24/500, Training Loss: 58.2120\n",
      "Epoch 25/500, Training Loss: 46.9381\n",
      "Epoch 26/500, Training Loss: 45.7694\n",
      "Epoch 27/500, Training Loss: 26.0870\n",
      "Epoch 28/500, Training Loss: 37.3215\n",
      "Epoch 29/500, Training Loss: 43.1932\n",
      "Epoch 30/500, Training Loss: 64.7536\n",
      "Epoch 31/500, Training Loss: 43.1200\n",
      "Epoch 32/500, Training Loss: 45.3912\n",
      "Epoch 33/500, Training Loss: 50.4327\n",
      "Epoch 34/500, Training Loss: 52.8975\n",
      "Epoch 35/500, Training Loss: 57.4768\n",
      "Epoch 36/500, Training Loss: 41.1496\n",
      "Epoch 37/500, Training Loss: 31.1795\n",
      "Epoch 38/500, Training Loss: 53.4980\n",
      "Epoch 39/500, Training Loss: 26.8313\n",
      "Epoch 40/500, Training Loss: 68.6230\n",
      "Epoch 41/500, Training Loss: 26.7086\n",
      "Epoch 42/500, Training Loss: 21.6128\n",
      "Epoch 43/500, Training Loss: 66.5038\n",
      "Epoch 44/500, Training Loss: 22.8769\n",
      "Epoch 45/500, Training Loss: 27.2448\n",
      "Epoch 46/500, Training Loss: 12.7431\n",
      "Epoch 47/500, Training Loss: 27.4966\n",
      "Epoch 48/500, Training Loss: 14.1712\n",
      "Epoch 49/500, Training Loss: 50.2332\n",
      "Epoch 50/500, Training Loss: 36.3657\n",
      "Epoch 51/500, Training Loss: 49.5730\n",
      "Epoch 52/500, Training Loss: 28.0606\n",
      "Epoch 53/500, Training Loss: 18.5119\n",
      "Epoch 54/500, Training Loss: 33.1542\n",
      "Epoch 55/500, Training Loss: 49.0063\n",
      "Epoch 56/500, Training Loss: 52.1043\n",
      "Epoch 57/500, Training Loss: 36.9313\n",
      "Epoch 58/500, Training Loss: 22.9275\n",
      "Epoch 59/500, Training Loss: 46.6754\n",
      "Epoch 60/500, Training Loss: 19.0304\n",
      "Epoch 61/500, Training Loss: 23.2717\n",
      "Epoch 62/500, Training Loss: 39.6213\n",
      "Epoch 63/500, Training Loss: 53.2821\n",
      "Epoch 64/500, Training Loss: 24.0033\n",
      "Epoch 65/500, Training Loss: 15.6690\n",
      "Epoch 66/500, Training Loss: 43.3174\n",
      "Epoch 67/500, Training Loss: 22.4617\n",
      "Epoch 68/500, Training Loss: 32.2584\n",
      "Epoch 69/500, Training Loss: 29.7737\n",
      "Epoch 70/500, Training Loss: 14.1816\n",
      "Epoch 71/500, Training Loss: 32.2422\n",
      "Epoch 72/500, Training Loss: 32.8205\n",
      "Epoch 73/500, Training Loss: 13.5787\n",
      "Epoch 74/500, Training Loss: 9.9796\n",
      "Epoch 75/500, Training Loss: 20.3992\n",
      "Epoch 76/500, Training Loss: 24.3412\n",
      "Epoch 77/500, Training Loss: 14.3063\n",
      "Epoch 78/500, Training Loss: 37.0980\n",
      "Epoch 79/500, Training Loss: 22.2207\n",
      "Epoch 80/500, Training Loss: 15.7957\n",
      "Epoch 81/500, Training Loss: 23.7806\n",
      "Epoch 82/500, Training Loss: 11.7740\n",
      "Epoch 83/500, Training Loss: 22.6656\n",
      "Epoch 84/500, Training Loss: 14.4820\n",
      "Epoch 85/500, Training Loss: 13.7995\n",
      "Epoch 86/500, Training Loss: 25.2200\n",
      "Epoch 87/500, Training Loss: 25.5626\n",
      "Epoch 88/500, Training Loss: 14.6274\n",
      "Epoch 89/500, Training Loss: 23.6845\n",
      "Epoch 90/500, Training Loss: 25.4097\n",
      "Epoch 91/500, Training Loss: 20.7461\n",
      "Epoch 92/500, Training Loss: 23.5752\n",
      "Epoch 93/500, Training Loss: 12.3797\n",
      "Epoch 94/500, Training Loss: 19.2225\n",
      "Epoch 95/500, Training Loss: 14.1609\n",
      "Epoch 96/500, Training Loss: 5.3815\n",
      "Epoch 97/500, Training Loss: 8.8905\n",
      "Epoch 98/500, Training Loss: 23.5507\n",
      "Epoch 99/500, Training Loss: 22.1237\n",
      "Epoch 100/500, Training Loss: 20.0444\n",
      "Epoch 101/500, Training Loss: 7.6328\n",
      "Epoch 102/500, Training Loss: 12.0069\n",
      "Epoch 103/500, Training Loss: 15.1124\n",
      "Epoch 104/500, Training Loss: 9.6851\n",
      "Epoch 105/500, Training Loss: 9.1141\n",
      "Epoch 106/500, Training Loss: 23.6236\n",
      "Epoch 107/500, Training Loss: 11.2449\n",
      "Epoch 108/500, Training Loss: 9.2694\n",
      "Epoch 109/500, Training Loss: 10.7513\n",
      "Epoch 110/500, Training Loss: 10.3974\n",
      "Epoch 111/500, Training Loss: 15.0278\n",
      "Epoch 112/500, Training Loss: 14.5411\n",
      "Epoch 113/500, Training Loss: 14.3108\n",
      "Epoch 114/500, Training Loss: 15.2319\n",
      "Epoch 115/500, Training Loss: 5.4663\n",
      "Epoch 116/500, Training Loss: 15.7616\n",
      "Epoch 117/500, Training Loss: 21.8552\n",
      "Epoch 118/500, Training Loss: 16.7708\n",
      "Epoch 119/500, Training Loss: 20.5128\n",
      "Epoch 120/500, Training Loss: 9.5031\n",
      "Epoch 121/500, Training Loss: 14.2893\n",
      "Epoch 122/500, Training Loss: 19.6125\n",
      "Epoch 123/500, Training Loss: 13.3703\n",
      "Epoch 124/500, Training Loss: 13.0455\n",
      "Epoch 125/500, Training Loss: 6.9407\n",
      "Epoch 126/500, Training Loss: 17.4380\n",
      "Epoch 127/500, Training Loss: 10.6890\n",
      "Epoch 128/500, Training Loss: 15.7657\n",
      "Epoch 129/500, Training Loss: 11.2117\n",
      "Epoch 130/500, Training Loss: 8.6880\n",
      "Epoch 131/500, Training Loss: 17.7794\n",
      "Epoch 132/500, Training Loss: 12.1102\n",
      "Epoch 133/500, Training Loss: 5.6625\n",
      "Epoch 134/500, Training Loss: 5.2927\n",
      "Epoch 135/500, Training Loss: 13.4086\n",
      "Epoch 136/500, Training Loss: 12.3657\n",
      "Epoch 137/500, Training Loss: 6.6511\n",
      "Epoch 138/500, Training Loss: 10.3326\n",
      "Epoch 139/500, Training Loss: 11.1659\n",
      "Epoch 140/500, Training Loss: 9.3665\n",
      "Epoch 141/500, Training Loss: 8.9983\n",
      "Epoch 142/500, Training Loss: 15.7842\n",
      "Epoch 143/500, Training Loss: 11.4826\n",
      "Epoch 144/500, Training Loss: 7.5309\n",
      "Epoch 145/500, Training Loss: 13.8815\n",
      "Epoch 146/500, Training Loss: 10.0979\n",
      "Epoch 147/500, Training Loss: 21.8962\n",
      "Epoch 148/500, Training Loss: 6.8381\n",
      "Epoch 149/500, Training Loss: 8.8249\n",
      "Epoch 150/500, Training Loss: 10.1802\n",
      "Epoch 151/500, Training Loss: 7.5802\n",
      "Epoch 152/500, Training Loss: 12.3931\n",
      "Epoch 153/500, Training Loss: 13.6276\n",
      "Epoch 154/500, Training Loss: 12.3393\n",
      "Epoch 155/500, Training Loss: 10.8730\n",
      "Epoch 156/500, Training Loss: 11.9295\n",
      "Epoch 157/500, Training Loss: 8.6324\n",
      "Epoch 158/500, Training Loss: 12.7113\n",
      "Epoch 159/500, Training Loss: 8.9888\n",
      "Epoch 160/500, Training Loss: 9.0616\n",
      "Epoch 161/500, Training Loss: 12.7691\n",
      "Epoch 162/500, Training Loss: 10.5425\n",
      "Epoch 163/500, Training Loss: 5.7227\n",
      "Epoch 164/500, Training Loss: 10.3931\n",
      "Epoch 165/500, Training Loss: 10.0725\n",
      "Epoch 166/500, Training Loss: 5.5858\n",
      "Epoch 167/500, Training Loss: 11.3487\n",
      "Epoch 168/500, Training Loss: 11.9645\n",
      "Epoch 169/500, Training Loss: 9.8882\n",
      "Epoch 170/500, Training Loss: 9.6178\n",
      "Epoch 171/500, Training Loss: 9.8473\n",
      "Epoch 172/500, Training Loss: 13.0317\n",
      "Epoch 173/500, Training Loss: 7.7593\n",
      "Epoch 174/500, Training Loss: 5.8866\n",
      "Epoch 175/500, Training Loss: 6.0977\n",
      "Epoch 176/500, Training Loss: 8.1805\n",
      "Epoch 177/500, Training Loss: 14.3440\n",
      "Epoch 178/500, Training Loss: 7.7495\n",
      "Epoch 179/500, Training Loss: 16.7735\n",
      "Epoch 180/500, Training Loss: 7.4812\n",
      "Epoch 181/500, Training Loss: 13.2615\n",
      "Epoch 182/500, Training Loss: 6.8205\n",
      "Epoch 183/500, Training Loss: 11.1813\n",
      "Epoch 184/500, Training Loss: 9.1572\n",
      "Epoch 185/500, Training Loss: 8.4674\n",
      "Epoch 186/500, Training Loss: 9.7049\n",
      "Epoch 187/500, Training Loss: 7.2235\n",
      "Epoch 188/500, Training Loss: 12.4699\n",
      "Epoch 189/500, Training Loss: 4.7377\n",
      "Epoch 190/500, Training Loss: 11.2944\n",
      "Epoch 191/500, Training Loss: 6.2768\n",
      "Epoch 192/500, Training Loss: 13.4197\n",
      "Epoch 193/500, Training Loss: 7.0692\n",
      "Epoch 194/500, Training Loss: 10.5717\n",
      "Epoch 195/500, Training Loss: 8.7781\n",
      "Epoch 196/500, Training Loss: 8.6726\n",
      "Epoch 197/500, Training Loss: 5.8789\n",
      "Epoch 198/500, Training Loss: 7.5244\n",
      "Epoch 199/500, Training Loss: 11.1183\n",
      "Epoch 200/500, Training Loss: 6.0319\n",
      "Epoch 201/500, Training Loss: 4.8548\n",
      "Epoch 202/500, Training Loss: 5.9549\n",
      "Epoch 203/500, Training Loss: 5.1895\n",
      "Epoch 204/500, Training Loss: 7.5175\n",
      "Epoch 205/500, Training Loss: 8.2071\n",
      "Epoch 206/500, Training Loss: 5.7349\n",
      "Epoch 207/500, Training Loss: 6.8657\n",
      "Epoch 208/500, Training Loss: 7.7503\n",
      "Epoch 209/500, Training Loss: 7.3776\n",
      "Epoch 210/500, Training Loss: 8.0429\n",
      "Epoch 211/500, Training Loss: 5.9796\n",
      "Epoch 212/500, Training Loss: 7.8530\n",
      "Epoch 213/500, Training Loss: 7.4148\n",
      "Epoch 214/500, Training Loss: 6.2642\n",
      "Epoch 215/500, Training Loss: 5.7037\n",
      "Epoch 216/500, Training Loss: 8.3851\n",
      "Epoch 217/500, Training Loss: 7.1275\n",
      "Epoch 218/500, Training Loss: 7.8596\n",
      "Epoch 219/500, Training Loss: 5.0913\n",
      "Epoch 220/500, Training Loss: 4.9139\n",
      "Epoch 221/500, Training Loss: 7.3095\n",
      "Epoch 222/500, Training Loss: 9.1444\n",
      "Epoch 223/500, Training Loss: 9.5585\n",
      "Epoch 224/500, Training Loss: 8.5606\n",
      "Epoch 225/500, Training Loss: 6.6035\n",
      "Epoch 226/500, Training Loss: 7.8269\n",
      "Epoch 227/500, Training Loss: 7.0158\n",
      "Epoch 228/500, Training Loss: 5.7067\n",
      "Epoch 229/500, Training Loss: 5.6326\n",
      "Epoch 230/500, Training Loss: 5.7825\n",
      "Epoch 231/500, Training Loss: 5.9709\n",
      "Epoch 232/500, Training Loss: 8.8545\n",
      "Epoch 233/500, Training Loss: 6.0490\n",
      "Epoch 234/500, Training Loss: 5.1278\n",
      "Epoch 235/500, Training Loss: 7.3441\n",
      "Epoch 236/500, Training Loss: 8.6742\n",
      "Epoch 237/500, Training Loss: 5.3572\n",
      "Epoch 238/500, Training Loss: 10.3065\n",
      "Epoch 239/500, Training Loss: 7.4852\n",
      "Epoch 240/500, Training Loss: 10.6925\n",
      "Epoch 241/500, Training Loss: 4.3241\n",
      "Epoch 242/500, Training Loss: 7.3475\n",
      "Epoch 243/500, Training Loss: 7.5856\n",
      "Epoch 244/500, Training Loss: 10.3195\n",
      "Epoch 245/500, Training Loss: 7.7684\n",
      "Epoch 246/500, Training Loss: 7.7035\n",
      "Epoch 247/500, Training Loss: 7.3613\n",
      "Epoch 248/500, Training Loss: 5.8223\n",
      "Epoch 249/500, Training Loss: 5.9144\n",
      "Epoch 250/500, Training Loss: 5.1878\n",
      "Epoch 251/500, Training Loss: 7.7517\n",
      "Epoch 252/500, Training Loss: 6.0884\n",
      "Epoch 253/500, Training Loss: 7.2832\n",
      "Epoch 254/500, Training Loss: 4.1193\n",
      "Epoch 255/500, Training Loss: 8.8016\n",
      "Epoch 256/500, Training Loss: 7.3653\n",
      "Epoch 257/500, Training Loss: 10.3820\n",
      "Epoch 258/500, Training Loss: 8.0585\n",
      "Epoch 259/500, Training Loss: 6.3528\n",
      "Epoch 260/500, Training Loss: 6.6666\n",
      "Epoch 261/500, Training Loss: 5.4374\n",
      "Epoch 262/500, Training Loss: 8.8799\n",
      "Epoch 263/500, Training Loss: 5.1169\n",
      "Epoch 264/500, Training Loss: 5.7611\n",
      "Epoch 265/500, Training Loss: 9.1835\n",
      "Epoch 266/500, Training Loss: 4.8036\n",
      "Epoch 267/500, Training Loss: 8.5517\n",
      "Epoch 268/500, Training Loss: 8.7141\n",
      "Epoch 269/500, Training Loss: 6.8657\n",
      "Epoch 270/500, Training Loss: 5.8249\n",
      "Epoch 271/500, Training Loss: 6.6874\n",
      "Epoch 272/500, Training Loss: 5.9496\n",
      "Epoch 273/500, Training Loss: 5.3910\n",
      "Epoch 274/500, Training Loss: 5.0005\n",
      "Epoch 275/500, Training Loss: 6.3175\n",
      "Epoch 276/500, Training Loss: 4.3938\n",
      "Epoch 277/500, Training Loss: 4.7763\n",
      "Epoch 278/500, Training Loss: 5.2902\n",
      "Epoch 279/500, Training Loss: 4.8272\n",
      "Epoch 280/500, Training Loss: 7.9215\n",
      "Epoch 281/500, Training Loss: 4.9107\n",
      "Epoch 282/500, Training Loss: 4.7605\n",
      "Epoch 283/500, Training Loss: 7.5929\n",
      "Epoch 284/500, Training Loss: 6.8818\n",
      "Epoch 285/500, Training Loss: 5.4846\n",
      "Epoch 286/500, Training Loss: 6.1398\n",
      "Epoch 287/500, Training Loss: 5.2912\n",
      "Epoch 288/500, Training Loss: 4.1977\n",
      "Epoch 289/500, Training Loss: 6.7966\n",
      "Epoch 290/500, Training Loss: 4.9341\n",
      "Epoch 291/500, Training Loss: 7.6061\n",
      "Epoch 292/500, Training Loss: 7.8134\n",
      "Epoch 293/500, Training Loss: 7.1281\n",
      "Epoch 294/500, Training Loss: 6.8420\n",
      "Epoch 295/500, Training Loss: 8.5187\n",
      "Epoch 296/500, Training Loss: 6.6344\n",
      "Epoch 297/500, Training Loss: 5.6309\n",
      "Epoch 298/500, Training Loss: 5.0328\n",
      "Epoch 299/500, Training Loss: 6.9189\n",
      "Epoch 300/500, Training Loss: 7.2317\n",
      "Epoch 301/500, Training Loss: 7.1268\n",
      "Epoch 302/500, Training Loss: 9.1910\n",
      "Epoch 303/500, Training Loss: 5.0814\n",
      "Epoch 304/500, Training Loss: 5.2612\n",
      "Epoch 305/500, Training Loss: 6.4900\n",
      "Epoch 306/500, Training Loss: 5.9277\n",
      "Epoch 307/500, Training Loss: 6.5001\n",
      "Epoch 308/500, Training Loss: 5.8099\n",
      "Epoch 309/500, Training Loss: 6.2057\n",
      "Epoch 310/500, Training Loss: 7.5779\n",
      "Epoch 311/500, Training Loss: 5.2521\n",
      "Epoch 312/500, Training Loss: 4.6280\n",
      "Epoch 313/500, Training Loss: 3.7420\n",
      "Epoch 314/500, Training Loss: 7.8646\n",
      "Epoch 315/500, Training Loss: 6.2147\n",
      "Epoch 316/500, Training Loss: 4.7549\n",
      "Epoch 317/500, Training Loss: 4.7762\n",
      "Epoch 318/500, Training Loss: 7.4048\n",
      "Epoch 319/500, Training Loss: 4.6549\n",
      "Epoch 320/500, Training Loss: 3.4685\n",
      "Epoch 321/500, Training Loss: 5.9599\n",
      "Epoch 322/500, Training Loss: 6.6874\n",
      "Epoch 323/500, Training Loss: 5.3484\n",
      "Epoch 324/500, Training Loss: 4.0574\n",
      "Epoch 325/500, Training Loss: 5.3914\n",
      "Epoch 326/500, Training Loss: 6.3173\n",
      "Epoch 327/500, Training Loss: 7.0933\n",
      "Epoch 328/500, Training Loss: 4.7111\n",
      "Epoch 329/500, Training Loss: 4.5908\n",
      "Epoch 330/500, Training Loss: 5.3373\n",
      "Epoch 331/500, Training Loss: 5.2221\n",
      "Epoch 332/500, Training Loss: 6.6960\n",
      "Epoch 333/500, Training Loss: 3.8354\n",
      "Epoch 334/500, Training Loss: 7.3272\n",
      "Epoch 335/500, Training Loss: 7.6471\n",
      "Epoch 336/500, Training Loss: 6.0704\n",
      "Epoch 337/500, Training Loss: 3.6021\n",
      "Epoch 338/500, Training Loss: 6.1467\n",
      "Epoch 339/500, Training Loss: 4.8202\n",
      "Epoch 340/500, Training Loss: 5.1594\n",
      "Epoch 341/500, Training Loss: 7.1032\n",
      "Epoch 342/500, Training Loss: 3.6122\n",
      "Epoch 343/500, Training Loss: 5.7920\n",
      "Epoch 344/500, Training Loss: 4.5937\n",
      "Epoch 345/500, Training Loss: 3.6935\n",
      "Epoch 346/500, Training Loss: 7.8707\n",
      "Epoch 347/500, Training Loss: 6.3566\n",
      "Epoch 348/500, Training Loss: 6.1474\n",
      "Epoch 349/500, Training Loss: 7.0114\n",
      "Epoch 350/500, Training Loss: 6.3670\n",
      "Epoch 351/500, Training Loss: 7.3103\n",
      "Epoch 352/500, Training Loss: 3.9408\n",
      "Epoch 353/500, Training Loss: 6.3216\n",
      "Epoch 354/500, Training Loss: 4.6692\n",
      "Epoch 355/500, Training Loss: 5.8809\n",
      "Epoch 356/500, Training Loss: 6.1271\n",
      "Epoch 357/500, Training Loss: 4.4707\n",
      "Epoch 358/500, Training Loss: 7.6862\n",
      "Epoch 359/500, Training Loss: 4.6909\n",
      "Epoch 360/500, Training Loss: 5.3804\n",
      "Epoch 361/500, Training Loss: 5.7503\n",
      "Epoch 362/500, Training Loss: 6.4606\n",
      "Epoch 363/500, Training Loss: 5.5132\n",
      "Epoch 364/500, Training Loss: 5.5387\n",
      "Epoch 365/500, Training Loss: 5.1765\n",
      "Epoch 366/500, Training Loss: 5.5276\n",
      "Epoch 367/500, Training Loss: 6.7350\n",
      "Epoch 368/500, Training Loss: 4.2230\n",
      "Epoch 369/500, Training Loss: 7.0159\n",
      "Epoch 370/500, Training Loss: 5.0911\n",
      "Epoch 371/500, Training Loss: 4.5986\n",
      "Epoch 372/500, Training Loss: 5.6184\n",
      "Epoch 373/500, Training Loss: 5.1260\n",
      "Epoch 374/500, Training Loss: 5.6329\n",
      "Epoch 375/500, Training Loss: 4.9151\n",
      "Epoch 376/500, Training Loss: 6.0350\n",
      "Epoch 377/500, Training Loss: 5.0552\n",
      "Epoch 378/500, Training Loss: 6.0951\n",
      "Epoch 379/500, Training Loss: 5.1906\n",
      "Epoch 380/500, Training Loss: 4.9790\n",
      "Epoch 381/500, Training Loss: 4.6329\n",
      "Epoch 382/500, Training Loss: 3.5815\n",
      "Epoch 383/500, Training Loss: 3.7759\n",
      "Epoch 384/500, Training Loss: 5.0021\n",
      "Epoch 385/500, Training Loss: 5.4448\n",
      "Epoch 386/500, Training Loss: 4.5718\n",
      "Epoch 387/500, Training Loss: 4.8776\n",
      "Epoch 388/500, Training Loss: 4.5015\n",
      "Epoch 389/500, Training Loss: 5.4398\n",
      "Epoch 390/500, Training Loss: 7.3602\n",
      "Epoch 391/500, Training Loss: 4.1560\n",
      "Epoch 392/500, Training Loss: 7.2839\n",
      "Epoch 393/500, Training Loss: 3.8302\n",
      "Epoch 394/500, Training Loss: 6.0094\n",
      "Epoch 395/500, Training Loss: 5.2444\n",
      "Epoch 396/500, Training Loss: 4.6392\n",
      "Epoch 397/500, Training Loss: 3.9871\n",
      "Epoch 398/500, Training Loss: 7.2907\n",
      "Epoch 399/500, Training Loss: 5.7476\n",
      "Epoch 400/500, Training Loss: 6.3125\n",
      "Epoch 401/500, Training Loss: 4.9170\n",
      "Epoch 402/500, Training Loss: 6.0274\n",
      "Epoch 403/500, Training Loss: 4.7217\n",
      "Epoch 404/500, Training Loss: 4.3763\n",
      "Epoch 405/500, Training Loss: 5.1833\n",
      "Epoch 406/500, Training Loss: 4.6148\n",
      "Epoch 407/500, Training Loss: 4.6482\n",
      "Epoch 408/500, Training Loss: 4.8433\n",
      "Epoch 409/500, Training Loss: 4.8282\n",
      "Epoch 410/500, Training Loss: 4.4552\n",
      "Epoch 411/500, Training Loss: 5.6546\n",
      "Epoch 412/500, Training Loss: 5.1917\n",
      "Epoch 413/500, Training Loss: 3.6998\n",
      "Epoch 414/500, Training Loss: 5.0512\n",
      "Epoch 415/500, Training Loss: 5.5402\n",
      "Epoch 416/500, Training Loss: 3.8706\n",
      "Epoch 417/500, Training Loss: 3.8241\n",
      "Epoch 418/500, Training Loss: 6.1169\n",
      "Epoch 419/500, Training Loss: 4.5855\n",
      "Epoch 420/500, Training Loss: 5.2368\n",
      "Epoch 421/500, Training Loss: 4.7681\n",
      "Epoch 422/500, Training Loss: 4.8401\n",
      "Epoch 423/500, Training Loss: 5.2965\n",
      "Epoch 424/500, Training Loss: 5.4959\n",
      "Epoch 425/500, Training Loss: 4.3268\n",
      "Epoch 426/500, Training Loss: 5.7456\n",
      "Epoch 427/500, Training Loss: 3.9963\n",
      "Epoch 428/500, Training Loss: 4.9048\n",
      "Epoch 429/500, Training Loss: 4.4848\n",
      "Epoch 430/500, Training Loss: 4.2667\n",
      "Epoch 431/500, Training Loss: 5.1673\n",
      "Epoch 432/500, Training Loss: 6.2422\n",
      "Epoch 433/500, Training Loss: 5.3304\n",
      "Epoch 434/500, Training Loss: 4.1372\n",
      "Epoch 435/500, Training Loss: 4.1245\n",
      "Epoch 436/500, Training Loss: 4.9389\n",
      "Epoch 437/500, Training Loss: 4.9235\n",
      "Epoch 438/500, Training Loss: 4.5913\n",
      "Epoch 439/500, Training Loss: 4.7665\n",
      "Epoch 440/500, Training Loss: 5.3853\n",
      "Epoch 441/500, Training Loss: 4.3501\n",
      "Epoch 442/500, Training Loss: 5.5132\n",
      "Epoch 443/500, Training Loss: 5.3515\n",
      "Epoch 444/500, Training Loss: 4.4794\n",
      "Epoch 445/500, Training Loss: 6.0212\n",
      "Epoch 446/500, Training Loss: 3.9681\n",
      "Epoch 447/500, Training Loss: 3.9590\n",
      "Epoch 448/500, Training Loss: 5.9531\n",
      "Epoch 449/500, Training Loss: 4.6734\n",
      "Epoch 450/500, Training Loss: 5.4589\n",
      "Epoch 451/500, Training Loss: 4.0116\n",
      "Epoch 452/500, Training Loss: 6.1006\n",
      "Epoch 453/500, Training Loss: 5.8473\n",
      "Epoch 454/500, Training Loss: 5.9013\n",
      "Epoch 455/500, Training Loss: 5.8742\n",
      "Epoch 456/500, Training Loss: 5.1934\n",
      "Epoch 457/500, Training Loss: 4.6536\n",
      "Epoch 458/500, Training Loss: 4.9539\n",
      "Epoch 459/500, Training Loss: 5.9091\n",
      "Epoch 460/500, Training Loss: 4.0958\n",
      "Epoch 461/500, Training Loss: 5.0972\n",
      "Epoch 462/500, Training Loss: 4.6410\n",
      "Epoch 463/500, Training Loss: 4.2933\n",
      "Epoch 464/500, Training Loss: 6.2741\n",
      "Epoch 465/500, Training Loss: 4.3305\n",
      "Epoch 466/500, Training Loss: 4.9148\n",
      "Epoch 467/500, Training Loss: 6.2728\n",
      "Epoch 468/500, Training Loss: 5.1972\n",
      "Epoch 469/500, Training Loss: 5.1307\n",
      "Epoch 470/500, Training Loss: 4.0405\n",
      "Epoch 471/500, Training Loss: 4.8915\n",
      "Epoch 472/500, Training Loss: 4.8719\n",
      "Epoch 473/500, Training Loss: 4.5331\n",
      "Epoch 474/500, Training Loss: 4.8102\n",
      "Epoch 475/500, Training Loss: 4.9716\n",
      "Epoch 476/500, Training Loss: 5.1500\n",
      "Epoch 477/500, Training Loss: 4.1465\n",
      "Epoch 478/500, Training Loss: 4.6697\n",
      "Epoch 479/500, Training Loss: 4.1825\n",
      "Epoch 480/500, Training Loss: 4.5274\n",
      "Epoch 481/500, Training Loss: 5.3566\n",
      "Epoch 482/500, Training Loss: 4.5604\n",
      "Epoch 483/500, Training Loss: 6.2760\n",
      "Epoch 484/500, Training Loss: 5.5994\n",
      "Epoch 485/500, Training Loss: 4.2399\n",
      "Epoch 486/500, Training Loss: 4.0638\n",
      "Epoch 487/500, Training Loss: 5.1183\n",
      "Epoch 488/500, Training Loss: 4.4321\n",
      "Epoch 489/500, Training Loss: 4.5407\n",
      "Epoch 490/500, Training Loss: 4.1602\n",
      "Epoch 491/500, Training Loss: 4.6728\n",
      "Epoch 492/500, Training Loss: 5.3748\n",
      "Epoch 493/500, Training Loss: 4.1831\n",
      "Epoch 494/500, Training Loss: 4.9488\n",
      "Epoch 495/500, Training Loss: 4.2737\n",
      "Epoch 496/500, Training Loss: 3.6583\n",
      "Epoch 497/500, Training Loss: 4.3074\n",
      "Epoch 498/500, Training Loss: 4.4476\n",
      "Epoch 499/500, Training Loss: 5.5727\n",
      "Epoch 500/500, Training Loss: 3.6690\n",
      "Validation Loss: 4.6144\n",
      "Model 0, Fold 0: Validation Loss = 4.6144\n",
      "Epoch 1/500, Training Loss: 24.1148\n",
      "Epoch 2/500, Training Loss: 165.9394\n",
      "Epoch 3/500, Training Loss: 164.3281\n",
      "Epoch 4/500, Training Loss: 249.0826\n",
      "Epoch 5/500, Training Loss: 141.8686\n",
      "Epoch 6/500, Training Loss: 52.6756\n",
      "Epoch 7/500, Training Loss: 150.6052\n",
      "Epoch 8/500, Training Loss: 50.1954\n",
      "Epoch 9/500, Training Loss: 138.3431\n",
      "Epoch 10/500, Training Loss: 28.6358\n",
      "Epoch 11/500, Training Loss: 104.1425\n",
      "Epoch 12/500, Training Loss: 170.3312\n",
      "Epoch 13/500, Training Loss: 22.0772\n",
      "Epoch 14/500, Training Loss: 125.8513\n",
      "Epoch 15/500, Training Loss: 63.8051\n",
      "Epoch 16/500, Training Loss: 102.4980\n",
      "Epoch 17/500, Training Loss: 164.0242\n",
      "Epoch 18/500, Training Loss: 90.9872\n",
      "Epoch 19/500, Training Loss: 74.0472\n",
      "Epoch 20/500, Training Loss: 43.2341\n",
      "Epoch 21/500, Training Loss: 83.4039\n",
      "Epoch 22/500, Training Loss: 73.6713\n",
      "Epoch 23/500, Training Loss: 87.4239\n",
      "Epoch 24/500, Training Loss: 87.5532\n",
      "Epoch 25/500, Training Loss: 115.7234\n",
      "Epoch 26/500, Training Loss: 108.3493\n",
      "Epoch 27/500, Training Loss: 67.8293\n",
      "Epoch 28/500, Training Loss: 14.2135\n",
      "Epoch 29/500, Training Loss: 140.0299\n",
      "Epoch 30/500, Training Loss: 66.6448\n",
      "Epoch 31/500, Training Loss: 67.4449\n",
      "Epoch 32/500, Training Loss: 13.9396\n",
      "Epoch 33/500, Training Loss: 64.1305\n",
      "Epoch 34/500, Training Loss: 22.8364\n",
      "Epoch 35/500, Training Loss: 26.2612\n",
      "Epoch 36/500, Training Loss: 20.2019\n",
      "Epoch 37/500, Training Loss: 28.8947\n",
      "Epoch 38/500, Training Loss: 19.1772\n",
      "Epoch 39/500, Training Loss: 46.5090\n",
      "Epoch 40/500, Training Loss: 23.2662\n",
      "Epoch 41/500, Training Loss: 20.2233\n",
      "Epoch 42/500, Training Loss: 45.4906\n",
      "Epoch 43/500, Training Loss: 19.3669\n",
      "Epoch 44/500, Training Loss: 44.6702\n",
      "Epoch 45/500, Training Loss: 19.5368\n",
      "Epoch 46/500, Training Loss: 38.8449\n",
      "Epoch 47/500, Training Loss: 41.0520\n",
      "Epoch 48/500, Training Loss: 15.7973\n",
      "Epoch 49/500, Training Loss: 17.6114\n",
      "Epoch 50/500, Training Loss: 40.1781\n",
      "Epoch 51/500, Training Loss: 21.5920\n",
      "Epoch 52/500, Training Loss: 46.3899\n",
      "Epoch 53/500, Training Loss: 40.5064\n",
      "Epoch 54/500, Training Loss: 14.1652\n",
      "Epoch 55/500, Training Loss: 14.5584\n",
      "Epoch 56/500, Training Loss: 14.4551\n",
      "Epoch 57/500, Training Loss: 9.1195\n",
      "Epoch 58/500, Training Loss: 40.0716\n",
      "Epoch 59/500, Training Loss: 37.9905\n",
      "Epoch 60/500, Training Loss: 28.6342\n",
      "Epoch 61/500, Training Loss: 32.7623\n",
      "Epoch 62/500, Training Loss: 32.9519\n",
      "Epoch 63/500, Training Loss: 28.2504\n",
      "Epoch 64/500, Training Loss: 17.7977\n",
      "Epoch 65/500, Training Loss: 29.1732\n",
      "Epoch 66/500, Training Loss: 31.5528\n",
      "Epoch 67/500, Training Loss: 14.3719\n",
      "Epoch 68/500, Training Loss: 29.6223\n",
      "Epoch 69/500, Training Loss: 31.2180\n",
      "Epoch 70/500, Training Loss: 13.5020\n",
      "Epoch 71/500, Training Loss: 7.7233\n",
      "Epoch 72/500, Training Loss: 4.5152\n",
      "Epoch 73/500, Training Loss: 33.8624\n",
      "Epoch 74/500, Training Loss: 10.4158\n",
      "Epoch 75/500, Training Loss: 6.6850\n",
      "Epoch 76/500, Training Loss: 11.6147\n",
      "Epoch 77/500, Training Loss: 45.3246\n",
      "Epoch 78/500, Training Loss: 6.5438\n",
      "Epoch 79/500, Training Loss: 8.4823\n",
      "Epoch 80/500, Training Loss: 13.6556\n",
      "Epoch 81/500, Training Loss: 11.8774\n",
      "Epoch 82/500, Training Loss: 44.3655\n",
      "Epoch 83/500, Training Loss: 28.8444\n",
      "Epoch 84/500, Training Loss: 14.3454\n",
      "Epoch 85/500, Training Loss: 9.3810\n",
      "Epoch 86/500, Training Loss: 9.1996\n",
      "Epoch 87/500, Training Loss: 24.2512\n",
      "Epoch 88/500, Training Loss: 23.1362\n",
      "Epoch 89/500, Training Loss: 10.6925\n",
      "Epoch 90/500, Training Loss: 6.5571\n",
      "Epoch 91/500, Training Loss: 28.6232\n",
      "Epoch 92/500, Training Loss: 20.1317\n",
      "Epoch 93/500, Training Loss: 40.2516\n",
      "Epoch 94/500, Training Loss: 11.9462\n",
      "Epoch 95/500, Training Loss: 5.9996\n",
      "Epoch 96/500, Training Loss: 11.9739\n",
      "Epoch 97/500, Training Loss: 37.1170\n",
      "Epoch 98/500, Training Loss: 21.8247\n",
      "Epoch 99/500, Training Loss: 12.4906\n",
      "Epoch 100/500, Training Loss: 13.4110\n",
      "Epoch 101/500, Training Loss: 13.7038\n",
      "Epoch 102/500, Training Loss: 10.9762\n",
      "Epoch 103/500, Training Loss: 19.2931\n",
      "Epoch 104/500, Training Loss: 9.5106\n",
      "Epoch 105/500, Training Loss: 32.7330\n",
      "Epoch 106/500, Training Loss: 7.8758\n",
      "Epoch 107/500, Training Loss: 8.2373\n",
      "Epoch 108/500, Training Loss: 7.2688\n",
      "Epoch 109/500, Training Loss: 8.9445\n",
      "Epoch 110/500, Training Loss: 20.4048\n",
      "Epoch 111/500, Training Loss: 20.1906\n",
      "Epoch 112/500, Training Loss: 8.3422\n",
      "Epoch 113/500, Training Loss: 5.4215\n",
      "Epoch 114/500, Training Loss: 19.6046\n",
      "Epoch 115/500, Training Loss: 11.5651\n",
      "Epoch 116/500, Training Loss: 11.0166\n",
      "Epoch 117/500, Training Loss: 16.7533\n",
      "Epoch 118/500, Training Loss: 28.5991\n",
      "Epoch 119/500, Training Loss: 16.7042\n",
      "Epoch 120/500, Training Loss: 22.2566\n",
      "Epoch 121/500, Training Loss: 6.9091\n",
      "Epoch 122/500, Training Loss: 6.5958\n",
      "Epoch 123/500, Training Loss: 17.2185\n",
      "Epoch 124/500, Training Loss: 6.9262\n",
      "Epoch 125/500, Training Loss: 6.9562\n",
      "Epoch 126/500, Training Loss: 17.8067\n",
      "Epoch 127/500, Training Loss: 19.6153\n",
      "Epoch 128/500, Training Loss: 8.5675\n",
      "Epoch 129/500, Training Loss: 5.9093\n",
      "Epoch 130/500, Training Loss: 9.2639\n",
      "Epoch 131/500, Training Loss: 29.5834\n",
      "Epoch 132/500, Training Loss: 6.6141\n",
      "Epoch 133/500, Training Loss: 15.6432\n",
      "Epoch 134/500, Training Loss: 5.2462\n",
      "Epoch 135/500, Training Loss: 14.9095\n",
      "Epoch 136/500, Training Loss: 14.8771\n",
      "Epoch 137/500, Training Loss: 7.8578\n",
      "Epoch 138/500, Training Loss: 22.2489\n",
      "Epoch 139/500, Training Loss: 8.9515\n",
      "Epoch 140/500, Training Loss: 13.6011\n",
      "Epoch 141/500, Training Loss: 6.7303\n",
      "Epoch 142/500, Training Loss: 23.2010\n",
      "Epoch 143/500, Training Loss: 6.9694\n",
      "Epoch 144/500, Training Loss: 15.1434\n",
      "Epoch 145/500, Training Loss: 15.9409\n",
      "Epoch 146/500, Training Loss: 16.6138\n",
      "Epoch 147/500, Training Loss: 8.6379\n",
      "Epoch 148/500, Training Loss: 5.7201\n",
      "Epoch 149/500, Training Loss: 4.1910\n",
      "Epoch 150/500, Training Loss: 16.7463\n",
      "Epoch 151/500, Training Loss: 7.2811\n",
      "Epoch 152/500, Training Loss: 12.6788\n",
      "Epoch 153/500, Training Loss: 21.7055\n",
      "Epoch 154/500, Training Loss: 16.8391\n",
      "Epoch 155/500, Training Loss: 14.4514\n",
      "Epoch 156/500, Training Loss: 15.3318\n",
      "Epoch 157/500, Training Loss: 12.0152\n",
      "Epoch 158/500, Training Loss: 12.1350\n",
      "Epoch 159/500, Training Loss: 3.6492\n",
      "Epoch 160/500, Training Loss: 5.7781\n",
      "Epoch 161/500, Training Loss: 7.3988\n",
      "Epoch 162/500, Training Loss: 13.6415\n",
      "Epoch 163/500, Training Loss: 21.6771\n",
      "Epoch 164/500, Training Loss: 14.1512\n",
      "Epoch 165/500, Training Loss: 5.3606\n",
      "Epoch 166/500, Training Loss: 13.4026\n",
      "Epoch 167/500, Training Loss: 18.4503\n",
      "Epoch 168/500, Training Loss: 21.6706\n",
      "Epoch 169/500, Training Loss: 13.1343\n",
      "Epoch 170/500, Training Loss: 13.2156\n",
      "Epoch 171/500, Training Loss: 7.3492\n",
      "Epoch 172/500, Training Loss: 12.0366\n",
      "Epoch 173/500, Training Loss: 10.4777\n",
      "Epoch 174/500, Training Loss: 20.4079\n",
      "Epoch 175/500, Training Loss: 6.1455\n",
      "Epoch 176/500, Training Loss: 12.8401\n",
      "Epoch 177/500, Training Loss: 16.4097\n",
      "Epoch 178/500, Training Loss: 4.1873\n",
      "Epoch 179/500, Training Loss: 3.7606\n",
      "Epoch 180/500, Training Loss: 4.0913\n",
      "Epoch 181/500, Training Loss: 4.1220\n",
      "Epoch 182/500, Training Loss: 12.4816\n",
      "Epoch 183/500, Training Loss: 11.5141\n",
      "Epoch 184/500, Training Loss: 4.8948\n",
      "Epoch 185/500, Training Loss: 15.8829\n",
      "Epoch 186/500, Training Loss: 8.0722\n",
      "Epoch 187/500, Training Loss: 4.2161\n",
      "Epoch 188/500, Training Loss: 10.7193\n",
      "Epoch 189/500, Training Loss: 12.4070\n",
      "Epoch 190/500, Training Loss: 7.6740\n",
      "Epoch 191/500, Training Loss: 6.3050\n",
      "Epoch 192/500, Training Loss: 7.3518\n",
      "Epoch 193/500, Training Loss: 10.3217\n",
      "Epoch 194/500, Training Loss: 5.5520\n",
      "Epoch 195/500, Training Loss: 17.4465\n",
      "Epoch 196/500, Training Loss: 4.5879\n",
      "Epoch 197/500, Training Loss: 13.2798\n",
      "Epoch 198/500, Training Loss: 10.6650\n",
      "Epoch 199/500, Training Loss: 4.6302\n",
      "Epoch 200/500, Training Loss: 16.0006\n",
      "Epoch 201/500, Training Loss: 4.9634\n",
      "Epoch 202/500, Training Loss: 4.8547\n",
      "Epoch 203/500, Training Loss: 11.0433\n",
      "Epoch 204/500, Training Loss: 10.0005\n",
      "Epoch 205/500, Training Loss: 9.9377\n",
      "Epoch 206/500, Training Loss: 11.2018\n",
      "Epoch 207/500, Training Loss: 9.6771\n",
      "Epoch 208/500, Training Loss: 5.8630\n",
      "Epoch 209/500, Training Loss: 10.1104\n",
      "Epoch 210/500, Training Loss: 4.9577\n",
      "Epoch 211/500, Training Loss: 11.4549\n",
      "Epoch 212/500, Training Loss: 13.3559\n",
      "Epoch 213/500, Training Loss: 18.9265\n",
      "Epoch 214/500, Training Loss: 15.1662\n",
      "Epoch 215/500, Training Loss: 10.3181\n",
      "Epoch 216/500, Training Loss: 8.2951\n",
      "Epoch 217/500, Training Loss: 12.9233\n",
      "Epoch 218/500, Training Loss: 5.6593\n",
      "Epoch 219/500, Training Loss: 6.6817\n",
      "Epoch 220/500, Training Loss: 6.3104\n",
      "Epoch 221/500, Training Loss: 4.8714\n",
      "Epoch 222/500, Training Loss: 4.3877\n",
      "Epoch 223/500, Training Loss: 10.0530\n",
      "Epoch 224/500, Training Loss: 9.1867\n",
      "Epoch 225/500, Training Loss: 6.8517\n",
      "Epoch 226/500, Training Loss: 14.2347\n",
      "Epoch 227/500, Training Loss: 10.2189\n",
      "Epoch 228/500, Training Loss: 9.4541\n",
      "Epoch 229/500, Training Loss: 12.7292\n",
      "Epoch 230/500, Training Loss: 14.4695\n",
      "Epoch 231/500, Training Loss: 12.5521\n",
      "Epoch 232/500, Training Loss: 7.6133\n",
      "Epoch 233/500, Training Loss: 5.0358\n",
      "Epoch 234/500, Training Loss: 9.4067\n",
      "Epoch 235/500, Training Loss: 6.2599\n",
      "Epoch 236/500, Training Loss: 8.1538\n",
      "Epoch 237/500, Training Loss: 4.8028\n",
      "Epoch 238/500, Training Loss: 4.6032\n",
      "Epoch 239/500, Training Loss: 4.2977\n",
      "Epoch 240/500, Training Loss: 9.8348\n",
      "Epoch 241/500, Training Loss: 8.6860\n",
      "Epoch 242/500, Training Loss: 4.6741\n",
      "Epoch 243/500, Training Loss: 5.8338\n",
      "Epoch 244/500, Training Loss: 13.2866\n",
      "Epoch 245/500, Training Loss: 10.8415\n",
      "Epoch 246/500, Training Loss: 8.0449\n",
      "Epoch 247/500, Training Loss: 11.9368\n",
      "Epoch 248/500, Training Loss: 4.5945\n",
      "Epoch 249/500, Training Loss: 8.4423\n",
      "Epoch 250/500, Training Loss: 4.9698\n",
      "Epoch 251/500, Training Loss: 4.6224\n",
      "Epoch 252/500, Training Loss: 11.2079\n",
      "Epoch 253/500, Training Loss: 11.4000\n",
      "Epoch 254/500, Training Loss: 5.4961\n",
      "Epoch 255/500, Training Loss: 15.2102\n",
      "Epoch 256/500, Training Loss: 12.6139\n",
      "Epoch 257/500, Training Loss: 7.7340\n",
      "Epoch 258/500, Training Loss: 6.0946\n",
      "Epoch 259/500, Training Loss: 7.4043\n",
      "Epoch 260/500, Training Loss: 7.1242\n",
      "Epoch 261/500, Training Loss: 9.3852\n",
      "Epoch 262/500, Training Loss: 5.0658\n",
      "Epoch 263/500, Training Loss: 5.5005\n",
      "Epoch 264/500, Training Loss: 5.1073\n",
      "Epoch 265/500, Training Loss: 6.3832\n",
      "Epoch 266/500, Training Loss: 7.2963\n",
      "Epoch 267/500, Training Loss: 7.8774\n",
      "Epoch 268/500, Training Loss: 11.0551\n",
      "Epoch 269/500, Training Loss: 6.6578\n",
      "Epoch 270/500, Training Loss: 8.6337\n",
      "Epoch 271/500, Training Loss: 9.0496\n",
      "Epoch 272/500, Training Loss: 10.3900\n",
      "Epoch 273/500, Training Loss: 10.7530\n",
      "Epoch 274/500, Training Loss: 8.2522\n",
      "Epoch 275/500, Training Loss: 4.5146\n",
      "Epoch 276/500, Training Loss: 7.0630\n",
      "Epoch 277/500, Training Loss: 6.3537\n",
      "Epoch 278/500, Training Loss: 3.6723\n",
      "Epoch 279/500, Training Loss: 9.8841\n",
      "Epoch 280/500, Training Loss: 4.9763\n",
      "Epoch 281/500, Training Loss: 4.8791\n",
      "Epoch 282/500, Training Loss: 6.7746\n",
      "Epoch 283/500, Training Loss: 4.6952\n",
      "Epoch 284/500, Training Loss: 3.9533\n",
      "Epoch 285/500, Training Loss: 6.5347\n",
      "Epoch 286/500, Training Loss: 5.3876\n",
      "Epoch 287/500, Training Loss: 4.1844\n",
      "Epoch 288/500, Training Loss: 4.5943\n",
      "Epoch 289/500, Training Loss: 10.4347\n",
      "Epoch 290/500, Training Loss: 6.6161\n",
      "Epoch 291/500, Training Loss: 7.0689\n",
      "Epoch 292/500, Training Loss: 7.7417\n",
      "Epoch 293/500, Training Loss: 4.5184\n",
      "Epoch 294/500, Training Loss: 6.0375\n",
      "Epoch 295/500, Training Loss: 4.1501\n",
      "Epoch 296/500, Training Loss: 6.7583\n",
      "Epoch 297/500, Training Loss: 4.2924\n",
      "Epoch 298/500, Training Loss: 6.6693\n",
      "Epoch 299/500, Training Loss: 4.3076\n",
      "Epoch 300/500, Training Loss: 4.7757\n",
      "Epoch 301/500, Training Loss: 6.6145\n",
      "Epoch 302/500, Training Loss: 5.1041\n",
      "Epoch 303/500, Training Loss: 4.6428\n",
      "Epoch 304/500, Training Loss: 5.4676\n",
      "Epoch 305/500, Training Loss: 6.0225\n",
      "Epoch 306/500, Training Loss: 6.8387\n",
      "Epoch 307/500, Training Loss: 6.9755\n",
      "Epoch 308/500, Training Loss: 7.6034\n",
      "Epoch 309/500, Training Loss: 6.2336\n",
      "Epoch 310/500, Training Loss: 11.7762\n",
      "Epoch 311/500, Training Loss: 7.0223\n",
      "Epoch 312/500, Training Loss: 8.8309\n",
      "Epoch 313/500, Training Loss: 4.1929\n",
      "Epoch 314/500, Training Loss: 4.3959\n",
      "Epoch 315/500, Training Loss: 8.2755\n",
      "Epoch 316/500, Training Loss: 4.5016\n",
      "Epoch 317/500, Training Loss: 5.2932\n",
      "Epoch 318/500, Training Loss: 6.1941\n",
      "Epoch 319/500, Training Loss: 4.9365\n",
      "Epoch 320/500, Training Loss: 6.6363\n",
      "Epoch 321/500, Training Loss: 3.5888\n",
      "Epoch 322/500, Training Loss: 4.6355\n",
      "Epoch 323/500, Training Loss: 6.3554\n",
      "Epoch 324/500, Training Loss: 4.7025\n",
      "Epoch 325/500, Training Loss: 9.4150\n",
      "Epoch 326/500, Training Loss: 4.5536\n",
      "Epoch 327/500, Training Loss: 6.6433\n",
      "Epoch 328/500, Training Loss: 4.8632\n",
      "Epoch 329/500, Training Loss: 9.8078\n",
      "Epoch 330/500, Training Loss: 4.4464\n",
      "Epoch 331/500, Training Loss: 6.7376\n",
      "Epoch 332/500, Training Loss: 4.4144\n",
      "Epoch 333/500, Training Loss: 3.6376\n",
      "Epoch 334/500, Training Loss: 5.8657\n",
      "Epoch 335/500, Training Loss: 4.9159\n",
      "Epoch 336/500, Training Loss: 4.2798\n",
      "Epoch 337/500, Training Loss: 6.5398\n",
      "Epoch 338/500, Training Loss: 6.4606\n",
      "Epoch 339/500, Training Loss: 3.6914\n",
      "Epoch 340/500, Training Loss: 4.1177\n",
      "Epoch 341/500, Training Loss: 4.7041\n",
      "Epoch 342/500, Training Loss: 4.7314\n",
      "Epoch 343/500, Training Loss: 6.3019\n",
      "Epoch 344/500, Training Loss: 5.0240\n",
      "Epoch 345/500, Training Loss: 4.8819\n",
      "Epoch 346/500, Training Loss: 4.0512\n",
      "Epoch 347/500, Training Loss: 7.2752\n",
      "Epoch 348/500, Training Loss: 4.4643\n",
      "Epoch 349/500, Training Loss: 3.9043\n",
      "Epoch 350/500, Training Loss: 6.7589\n",
      "Epoch 351/500, Training Loss: 5.9196\n",
      "Epoch 352/500, Training Loss: 5.7734\n",
      "Epoch 353/500, Training Loss: 5.8294\n",
      "Epoch 354/500, Training Loss: 4.9063\n",
      "Epoch 355/500, Training Loss: 9.3147\n",
      "Epoch 356/500, Training Loss: 4.6101\n",
      "Epoch 357/500, Training Loss: 5.6894\n",
      "Epoch 358/500, Training Loss: 6.4875\n",
      "Epoch 359/500, Training Loss: 7.7026\n",
      "Epoch 360/500, Training Loss: 8.1961\n",
      "Epoch 361/500, Training Loss: 6.0534\n",
      "Epoch 362/500, Training Loss: 5.4731\n",
      "Epoch 363/500, Training Loss: 3.5904\n",
      "Epoch 364/500, Training Loss: 5.8468\n",
      "Epoch 365/500, Training Loss: 6.0927\n",
      "Epoch 366/500, Training Loss: 6.2037\n",
      "Epoch 367/500, Training Loss: 4.1077\n",
      "Epoch 368/500, Training Loss: 8.0115\n",
      "Epoch 369/500, Training Loss: 5.2002\n",
      "Epoch 370/500, Training Loss: 4.0722\n",
      "Epoch 371/500, Training Loss: 5.7247\n",
      "Epoch 372/500, Training Loss: 5.8703\n",
      "Epoch 373/500, Training Loss: 8.8963\n",
      "Epoch 374/500, Training Loss: 10.3840\n",
      "Epoch 375/500, Training Loss: 6.4419\n",
      "Epoch 376/500, Training Loss: 6.4064\n",
      "Epoch 377/500, Training Loss: 5.1572\n",
      "Epoch 378/500, Training Loss: 6.2816\n",
      "Epoch 379/500, Training Loss: 4.3760\n",
      "Epoch 380/500, Training Loss: 6.4203\n",
      "Epoch 381/500, Training Loss: 7.6621\n",
      "Epoch 382/500, Training Loss: 5.5007\n",
      "Epoch 383/500, Training Loss: 4.5195\n",
      "Epoch 384/500, Training Loss: 6.0921\n",
      "Epoch 385/500, Training Loss: 3.6660\n",
      "Epoch 386/500, Training Loss: 3.5938\n",
      "Epoch 387/500, Training Loss: 7.2971\n",
      "Epoch 388/500, Training Loss: 6.2462\n",
      "Epoch 389/500, Training Loss: 7.5166\n",
      "Epoch 390/500, Training Loss: 6.0261\n",
      "Epoch 391/500, Training Loss: 6.4640\n",
      "Epoch 392/500, Training Loss: 5.6370\n",
      "Epoch 393/500, Training Loss: 4.4157\n",
      "Epoch 394/500, Training Loss: 7.0888\n",
      "Epoch 395/500, Training Loss: 3.5165\n",
      "Epoch 396/500, Training Loss: 5.3392\n",
      "Epoch 397/500, Training Loss: 9.3703\n",
      "Epoch 398/500, Training Loss: 7.4403\n",
      "Epoch 399/500, Training Loss: 4.2653\n",
      "Epoch 400/500, Training Loss: 4.1025\n",
      "Epoch 401/500, Training Loss: 5.1001\n",
      "Epoch 402/500, Training Loss: 5.3730\n",
      "Epoch 403/500, Training Loss: 5.3834\n",
      "Epoch 404/500, Training Loss: 5.6935\n",
      "Epoch 405/500, Training Loss: 5.6453\n",
      "Epoch 406/500, Training Loss: 4.1463\n",
      "Epoch 407/500, Training Loss: 3.7400\n",
      "Epoch 408/500, Training Loss: 5.3540\n",
      "Epoch 409/500, Training Loss: 4.0664\n",
      "Epoch 410/500, Training Loss: 5.6886\n",
      "Epoch 411/500, Training Loss: 5.2846\n",
      "Epoch 412/500, Training Loss: 6.8963\n",
      "Epoch 413/500, Training Loss: 5.7628\n",
      "Epoch 414/500, Training Loss: 4.4017\n",
      "Epoch 415/500, Training Loss: 5.6032\n",
      "Epoch 416/500, Training Loss: 5.0888\n",
      "Epoch 417/500, Training Loss: 4.6542\n",
      "Epoch 418/500, Training Loss: 5.7465\n",
      "Epoch 419/500, Training Loss: 3.6731\n",
      "Epoch 420/500, Training Loss: 5.2572\n",
      "Epoch 421/500, Training Loss: 4.0329\n",
      "Epoch 422/500, Training Loss: 3.7257\n",
      "Epoch 423/500, Training Loss: 4.1334\n",
      "Epoch 424/500, Training Loss: 4.9453\n",
      "Epoch 425/500, Training Loss: 6.0030\n",
      "Epoch 426/500, Training Loss: 5.4578\n",
      "Epoch 427/500, Training Loss: 6.9846\n",
      "Epoch 428/500, Training Loss: 3.9912\n",
      "Epoch 429/500, Training Loss: 4.9053\n",
      "Epoch 430/500, Training Loss: 7.3170\n",
      "Epoch 431/500, Training Loss: 5.6980\n",
      "Epoch 432/500, Training Loss: 7.0242\n",
      "Epoch 433/500, Training Loss: 5.1855\n",
      "Epoch 434/500, Training Loss: 4.1326\n",
      "Epoch 435/500, Training Loss: 5.3063\n",
      "Epoch 436/500, Training Loss: 5.2905\n",
      "Epoch 437/500, Training Loss: 3.7673\n",
      "Epoch 438/500, Training Loss: 6.6729\n",
      "Epoch 439/500, Training Loss: 5.0020\n",
      "Epoch 440/500, Training Loss: 6.7392\n",
      "Epoch 441/500, Training Loss: 4.3449\n",
      "Epoch 442/500, Training Loss: 5.3606\n",
      "Epoch 443/500, Training Loss: 5.7818\n",
      "Epoch 444/500, Training Loss: 3.5208\n",
      "Epoch 445/500, Training Loss: 3.9334\n",
      "Epoch 446/500, Training Loss: 4.0354\n",
      "Epoch 447/500, Training Loss: 6.8048\n",
      "Epoch 448/500, Training Loss: 4.2994\n",
      "Epoch 449/500, Training Loss: 5.1494\n",
      "Epoch 450/500, Training Loss: 3.8460\n",
      "Epoch 451/500, Training Loss: 5.6083\n",
      "Epoch 452/500, Training Loss: 5.6399\n",
      "Epoch 453/500, Training Loss: 3.6739\n",
      "Epoch 454/500, Training Loss: 5.6279\n",
      "Epoch 455/500, Training Loss: 6.4329\n",
      "Epoch 456/500, Training Loss: 3.8844\n",
      "Epoch 457/500, Training Loss: 3.4690\n",
      "Epoch 458/500, Training Loss: 5.3353\n",
      "Epoch 459/500, Training Loss: 5.0862\n",
      "Epoch 460/500, Training Loss: 5.3594\n",
      "Epoch 461/500, Training Loss: 3.9498\n",
      "Epoch 462/500, Training Loss: 6.4509\n",
      "Epoch 463/500, Training Loss: 6.8747\n",
      "Epoch 464/500, Training Loss: 4.2046\n",
      "Epoch 465/500, Training Loss: 3.3290\n",
      "Epoch 466/500, Training Loss: 7.1369\n",
      "Epoch 467/500, Training Loss: 5.5553\n",
      "Epoch 468/500, Training Loss: 3.8942\n",
      "Epoch 469/500, Training Loss: 5.2060\n",
      "Epoch 470/500, Training Loss: 4.1361\n",
      "Epoch 471/500, Training Loss: 3.9234\n",
      "Epoch 472/500, Training Loss: 4.9545\n",
      "Epoch 473/500, Training Loss: 5.2145\n",
      "Epoch 474/500, Training Loss: 4.0247\n",
      "Epoch 475/500, Training Loss: 3.6892\n",
      "Epoch 476/500, Training Loss: 4.0201\n",
      "Epoch 477/500, Training Loss: 4.8752\n",
      "Epoch 478/500, Training Loss: 5.0778\n",
      "Epoch 479/500, Training Loss: 4.2243\n",
      "Epoch 480/500, Training Loss: 5.0623\n",
      "Epoch 481/500, Training Loss: 4.1480\n",
      "Epoch 482/500, Training Loss: 3.7898\n",
      "Epoch 483/500, Training Loss: 4.9708\n",
      "Epoch 484/500, Training Loss: 5.4416\n",
      "Epoch 485/500, Training Loss: 5.1779\n",
      "Epoch 486/500, Training Loss: 6.4309\n",
      "Epoch 487/500, Training Loss: 6.3427\n",
      "Epoch 488/500, Training Loss: 3.8280\n",
      "Epoch 489/500, Training Loss: 3.6473\n",
      "Epoch 490/500, Training Loss: 4.1536\n",
      "Epoch 491/500, Training Loss: 5.3392\n",
      "Epoch 492/500, Training Loss: 5.3545\n",
      "Epoch 493/500, Training Loss: 5.0520\n",
      "Epoch 494/500, Training Loss: 7.3429\n",
      "Epoch 495/500, Training Loss: 4.6966\n",
      "Epoch 496/500, Training Loss: 6.8344\n",
      "Epoch 497/500, Training Loss: 3.4485\n",
      "Epoch 498/500, Training Loss: 4.0982\n",
      "Epoch 499/500, Training Loss: 3.8864\n",
      "Epoch 500/500, Training Loss: 3.8817\n",
      "Validation Loss: 5.0846\n",
      "Model 0, Fold 1: Validation Loss = 5.0846\n",
      "Epoch 1/500, Training Loss: 134.2293\n",
      "Epoch 2/500, Training Loss: 120.0447\n",
      "Epoch 3/500, Training Loss: 195.0259\n",
      "Epoch 4/500, Training Loss: 161.7708\n",
      "Epoch 5/500, Training Loss: 194.5474\n",
      "Epoch 6/500, Training Loss: 66.7098\n",
      "Epoch 7/500, Training Loss: 181.6494\n",
      "Epoch 8/500, Training Loss: 113.0521\n",
      "Epoch 9/500, Training Loss: 79.1822\n",
      "Epoch 10/500, Training Loss: 164.4959\n",
      "Epoch 11/500, Training Loss: 54.8794\n",
      "Epoch 12/500, Training Loss: 98.6671\n",
      "Epoch 13/500, Training Loss: 69.5231\n",
      "Epoch 14/500, Training Loss: 28.6752\n",
      "Epoch 15/500, Training Loss: 105.7108\n",
      "Epoch 16/500, Training Loss: 82.6510\n",
      "Epoch 17/500, Training Loss: 113.1365\n",
      "Epoch 18/500, Training Loss: 75.7209\n",
      "Epoch 19/500, Training Loss: 57.2453\n",
      "Epoch 20/500, Training Loss: 67.0120\n",
      "Epoch 21/500, Training Loss: 92.6636\n",
      "Epoch 22/500, Training Loss: 63.8390\n",
      "Epoch 23/500, Training Loss: 27.0938\n",
      "Epoch 24/500, Training Loss: 125.5402\n",
      "Epoch 25/500, Training Loss: 43.9512\n",
      "Epoch 26/500, Training Loss: 105.6534\n",
      "Epoch 27/500, Training Loss: 73.2048\n",
      "Epoch 28/500, Training Loss: 62.2400\n",
      "Epoch 29/500, Training Loss: 20.1968\n",
      "Epoch 30/500, Training Loss: 59.8728\n",
      "Epoch 31/500, Training Loss: 86.2322\n",
      "Epoch 32/500, Training Loss: 25.5274\n",
      "Epoch 33/500, Training Loss: 47.3500\n",
      "Epoch 34/500, Training Loss: 41.3793\n",
      "Epoch 35/500, Training Loss: 51.3186\n",
      "Epoch 36/500, Training Loss: 33.2031\n",
      "Epoch 37/500, Training Loss: 53.2966\n",
      "Epoch 38/500, Training Loss: 36.6220\n",
      "Epoch 39/500, Training Loss: 36.2642\n",
      "Epoch 40/500, Training Loss: 58.0015\n",
      "Epoch 41/500, Training Loss: 34.3697\n",
      "Epoch 42/500, Training Loss: 46.5853\n",
      "Epoch 43/500, Training Loss: 28.6649\n",
      "Epoch 44/500, Training Loss: 47.1996\n",
      "Epoch 45/500, Training Loss: 24.4444\n",
      "Epoch 46/500, Training Loss: 24.7902\n",
      "Epoch 47/500, Training Loss: 63.1386\n",
      "Epoch 48/500, Training Loss: 37.8140\n",
      "Epoch 49/500, Training Loss: 14.3390\n",
      "Epoch 50/500, Training Loss: 19.5711\n",
      "Epoch 51/500, Training Loss: 32.3214\n",
      "Epoch 52/500, Training Loss: 27.9424\n",
      "Epoch 53/500, Training Loss: 34.8839\n",
      "Epoch 54/500, Training Loss: 31.3841\n",
      "Epoch 55/500, Training Loss: 16.5299\n",
      "Epoch 56/500, Training Loss: 45.3065\n",
      "Epoch 57/500, Training Loss: 27.0346\n",
      "Epoch 58/500, Training Loss: 36.2326\n",
      "Epoch 59/500, Training Loss: 36.8846\n",
      "Epoch 60/500, Training Loss: 17.1168\n",
      "Epoch 61/500, Training Loss: 23.1565\n",
      "Epoch 62/500, Training Loss: 12.8572\n",
      "Epoch 63/500, Training Loss: 21.7766\n",
      "Epoch 64/500, Training Loss: 10.3219\n",
      "Epoch 65/500, Training Loss: 28.5125\n",
      "Epoch 66/500, Training Loss: 39.6632\n",
      "Epoch 67/500, Training Loss: 13.7302\n",
      "Epoch 68/500, Training Loss: 20.3570\n",
      "Epoch 69/500, Training Loss: 26.7594\n",
      "Epoch 70/500, Training Loss: 33.5247\n",
      "Epoch 71/500, Training Loss: 50.1562\n",
      "Epoch 72/500, Training Loss: 12.4721\n",
      "Epoch 73/500, Training Loss: 28.3058\n",
      "Epoch 74/500, Training Loss: 29.3396\n",
      "Epoch 75/500, Training Loss: 32.1673\n",
      "Epoch 76/500, Training Loss: 6.3512\n",
      "Epoch 77/500, Training Loss: 22.2767\n",
      "Epoch 78/500, Training Loss: 31.1084\n",
      "Epoch 79/500, Training Loss: 27.9358\n",
      "Epoch 80/500, Training Loss: 46.0295\n",
      "Epoch 81/500, Training Loss: 18.5647\n",
      "Epoch 82/500, Training Loss: 27.2018\n",
      "Epoch 83/500, Training Loss: 21.3335\n",
      "Epoch 84/500, Training Loss: 15.4971\n",
      "Epoch 85/500, Training Loss: 13.9307\n",
      "Epoch 86/500, Training Loss: 22.0936\n",
      "Epoch 87/500, Training Loss: 18.7711\n",
      "Epoch 88/500, Training Loss: 12.3961\n",
      "Epoch 89/500, Training Loss: 17.9446\n",
      "Epoch 90/500, Training Loss: 7.9560\n",
      "Epoch 91/500, Training Loss: 6.9488\n",
      "Epoch 92/500, Training Loss: 10.7038\n",
      "Epoch 93/500, Training Loss: 21.5526\n",
      "Epoch 94/500, Training Loss: 7.5706\n",
      "Epoch 95/500, Training Loss: 18.8978\n",
      "Epoch 96/500, Training Loss: 30.7721\n",
      "Epoch 97/500, Training Loss: 18.7566\n",
      "Epoch 98/500, Training Loss: 20.7050\n",
      "Epoch 99/500, Training Loss: 28.5292\n",
      "Epoch 100/500, Training Loss: 17.5668\n",
      "Epoch 101/500, Training Loss: 7.5589\n",
      "Epoch 102/500, Training Loss: 16.4960\n",
      "Epoch 103/500, Training Loss: 16.3250\n",
      "Epoch 104/500, Training Loss: 7.4173\n",
      "Epoch 105/500, Training Loss: 13.2514\n",
      "Epoch 106/500, Training Loss: 9.0994\n",
      "Epoch 107/500, Training Loss: 10.5141\n",
      "Epoch 108/500, Training Loss: 12.6899\n",
      "Epoch 109/500, Training Loss: 8.5848\n",
      "Epoch 110/500, Training Loss: 23.1807\n",
      "Epoch 111/500, Training Loss: 13.6182\n",
      "Epoch 112/500, Training Loss: 13.8638\n",
      "Epoch 113/500, Training Loss: 16.6367\n",
      "Epoch 114/500, Training Loss: 18.7914\n",
      "Epoch 115/500, Training Loss: 23.8644\n",
      "Epoch 116/500, Training Loss: 18.7264\n",
      "Epoch 117/500, Training Loss: 17.5488\n",
      "Epoch 118/500, Training Loss: 10.0383\n",
      "Epoch 119/500, Training Loss: 5.4101\n",
      "Epoch 120/500, Training Loss: 14.9240\n",
      "Epoch 121/500, Training Loss: 12.1384\n",
      "Epoch 122/500, Training Loss: 12.4211\n",
      "Epoch 123/500, Training Loss: 9.2896\n",
      "Epoch 124/500, Training Loss: 11.4627\n",
      "Epoch 125/500, Training Loss: 10.7844\n",
      "Epoch 126/500, Training Loss: 14.5947\n",
      "Epoch 127/500, Training Loss: 10.7090\n",
      "Epoch 128/500, Training Loss: 6.8071\n",
      "Epoch 129/500, Training Loss: 8.9616\n",
      "Epoch 130/500, Training Loss: 10.6660\n",
      "Epoch 131/500, Training Loss: 6.8634\n",
      "Epoch 132/500, Training Loss: 10.3189\n",
      "Epoch 133/500, Training Loss: 15.2948\n",
      "Epoch 134/500, Training Loss: 12.8519\n",
      "Epoch 135/500, Training Loss: 12.6865\n",
      "Epoch 136/500, Training Loss: 10.7819\n",
      "Epoch 137/500, Training Loss: 9.4284\n",
      "Epoch 138/500, Training Loss: 16.6154\n",
      "Epoch 139/500, Training Loss: 13.1339\n",
      "Epoch 140/500, Training Loss: 10.8490\n",
      "Epoch 141/500, Training Loss: 11.6881\n",
      "Epoch 142/500, Training Loss: 8.7988\n",
      "Epoch 143/500, Training Loss: 21.8166\n",
      "Epoch 144/500, Training Loss: 15.0198\n",
      "Epoch 145/500, Training Loss: 10.3730\n",
      "Epoch 146/500, Training Loss: 11.1631\n",
      "Epoch 147/500, Training Loss: 13.4476\n",
      "Epoch 148/500, Training Loss: 9.4860\n",
      "Epoch 149/500, Training Loss: 10.1986\n",
      "Epoch 150/500, Training Loss: 17.6499\n",
      "Epoch 151/500, Training Loss: 10.1127\n",
      "Epoch 152/500, Training Loss: 21.2719\n",
      "Epoch 153/500, Training Loss: 6.5641\n",
      "Epoch 154/500, Training Loss: 9.2021\n",
      "Epoch 155/500, Training Loss: 6.6205\n",
      "Epoch 156/500, Training Loss: 11.1441\n",
      "Epoch 157/500, Training Loss: 14.6966\n",
      "Epoch 158/500, Training Loss: 8.2647\n",
      "Epoch 159/500, Training Loss: 13.8167\n",
      "Epoch 160/500, Training Loss: 8.0771\n",
      "Epoch 161/500, Training Loss: 13.6583\n",
      "Epoch 162/500, Training Loss: 6.9814\n",
      "Epoch 163/500, Training Loss: 14.4547\n",
      "Epoch 164/500, Training Loss: 8.5170\n",
      "Epoch 165/500, Training Loss: 8.6755\n",
      "Epoch 166/500, Training Loss: 16.9199\n",
      "Epoch 167/500, Training Loss: 10.0271\n",
      "Epoch 168/500, Training Loss: 6.5902\n",
      "Epoch 169/500, Training Loss: 8.8813\n",
      "Epoch 170/500, Training Loss: 10.0847\n",
      "Epoch 171/500, Training Loss: 8.5598\n",
      "Epoch 172/500, Training Loss: 5.6256\n",
      "Epoch 173/500, Training Loss: 7.7579\n",
      "Epoch 174/500, Training Loss: 6.0716\n",
      "Epoch 175/500, Training Loss: 8.5357\n",
      "Epoch 176/500, Training Loss: 7.6437\n",
      "Epoch 177/500, Training Loss: 16.3983\n",
      "Epoch 178/500, Training Loss: 7.3943\n",
      "Epoch 179/500, Training Loss: 10.6772\n",
      "Epoch 180/500, Training Loss: 6.9882\n",
      "Epoch 181/500, Training Loss: 6.0922\n",
      "Epoch 182/500, Training Loss: 11.1488\n",
      "Epoch 183/500, Training Loss: 8.4955\n",
      "Epoch 184/500, Training Loss: 10.7966\n",
      "Epoch 185/500, Training Loss: 7.1899\n",
      "Epoch 186/500, Training Loss: 5.9584\n",
      "Epoch 187/500, Training Loss: 5.6647\n",
      "Epoch 188/500, Training Loss: 6.7843\n",
      "Epoch 189/500, Training Loss: 12.4815\n",
      "Epoch 190/500, Training Loss: 5.3484\n",
      "Epoch 191/500, Training Loss: 4.5521\n",
      "Epoch 192/500, Training Loss: 8.9872\n",
      "Epoch 193/500, Training Loss: 9.9302\n",
      "Epoch 194/500, Training Loss: 14.3918\n",
      "Epoch 195/500, Training Loss: 6.1647\n",
      "Epoch 196/500, Training Loss: 10.0925\n",
      "Epoch 197/500, Training Loss: 9.2915\n",
      "Epoch 198/500, Training Loss: 5.3223\n",
      "Epoch 199/500, Training Loss: 8.8662\n",
      "Epoch 200/500, Training Loss: 8.9070\n",
      "Epoch 201/500, Training Loss: 10.9033\n",
      "Epoch 202/500, Training Loss: 10.3583\n",
      "Epoch 203/500, Training Loss: 7.7144\n",
      "Epoch 204/500, Training Loss: 10.9347\n",
      "Epoch 205/500, Training Loss: 13.1298\n",
      "Epoch 206/500, Training Loss: 7.2545\n",
      "Epoch 207/500, Training Loss: 8.7339\n",
      "Epoch 208/500, Training Loss: 4.6913\n",
      "Epoch 209/500, Training Loss: 5.2289\n",
      "Epoch 210/500, Training Loss: 10.8134\n",
      "Epoch 211/500, Training Loss: 9.4883\n",
      "Epoch 212/500, Training Loss: 9.4097\n",
      "Epoch 213/500, Training Loss: 8.3355\n",
      "Epoch 214/500, Training Loss: 5.4810\n",
      "Epoch 215/500, Training Loss: 6.7083\n",
      "Epoch 216/500, Training Loss: 10.6556\n",
      "Epoch 217/500, Training Loss: 7.9886\n",
      "Epoch 218/500, Training Loss: 5.8046\n",
      "Epoch 219/500, Training Loss: 6.5268\n",
      "Epoch 220/500, Training Loss: 6.1813\n",
      "Epoch 221/500, Training Loss: 4.7850\n",
      "Epoch 222/500, Training Loss: 6.5807\n",
      "Epoch 223/500, Training Loss: 5.1388\n",
      "Epoch 224/500, Training Loss: 6.7618\n",
      "Epoch 225/500, Training Loss: 9.2658\n",
      "Epoch 226/500, Training Loss: 4.1266\n",
      "Epoch 227/500, Training Loss: 5.5098\n",
      "Epoch 228/500, Training Loss: 5.1946\n",
      "Epoch 229/500, Training Loss: 6.5654\n",
      "Epoch 230/500, Training Loss: 5.2637\n",
      "Epoch 231/500, Training Loss: 4.2628\n",
      "Epoch 232/500, Training Loss: 5.5593\n",
      "Epoch 233/500, Training Loss: 8.5366\n",
      "Epoch 234/500, Training Loss: 5.5437\n",
      "Epoch 235/500, Training Loss: 7.6059\n",
      "Epoch 236/500, Training Loss: 9.4307\n",
      "Epoch 237/500, Training Loss: 8.0545\n",
      "Epoch 238/500, Training Loss: 8.2781\n",
      "Epoch 239/500, Training Loss: 8.1558\n",
      "Epoch 240/500, Training Loss: 8.2316\n",
      "Epoch 241/500, Training Loss: 4.0570\n",
      "Epoch 242/500, Training Loss: 7.3604\n",
      "Epoch 243/500, Training Loss: 7.9922\n",
      "Epoch 244/500, Training Loss: 5.0532\n",
      "Epoch 245/500, Training Loss: 7.7784\n",
      "Epoch 246/500, Training Loss: 4.6785\n",
      "Epoch 247/500, Training Loss: 7.6307\n",
      "Epoch 248/500, Training Loss: 7.8905\n",
      "Epoch 249/500, Training Loss: 5.8300\n",
      "Epoch 250/500, Training Loss: 5.1353\n",
      "Epoch 251/500, Training Loss: 8.0544\n",
      "Epoch 252/500, Training Loss: 7.9009\n",
      "Epoch 253/500, Training Loss: 4.5867\n",
      "Epoch 254/500, Training Loss: 4.3869\n",
      "Epoch 255/500, Training Loss: 9.8112\n",
      "Epoch 256/500, Training Loss: 3.9859\n",
      "Epoch 257/500, Training Loss: 5.0731\n",
      "Epoch 258/500, Training Loss: 6.8211\n",
      "Epoch 259/500, Training Loss: 9.9324\n",
      "Epoch 260/500, Training Loss: 9.6027\n",
      "Epoch 261/500, Training Loss: 5.2665\n",
      "Epoch 262/500, Training Loss: 8.5077\n",
      "Epoch 263/500, Training Loss: 9.4182\n",
      "Epoch 264/500, Training Loss: 8.3074\n",
      "Epoch 265/500, Training Loss: 4.5264\n",
      "Epoch 266/500, Training Loss: 5.5805\n",
      "Epoch 267/500, Training Loss: 6.5782\n",
      "Epoch 268/500, Training Loss: 6.5413\n",
      "Epoch 269/500, Training Loss: 4.3833\n",
      "Epoch 270/500, Training Loss: 6.0241\n",
      "Epoch 271/500, Training Loss: 9.0474\n",
      "Epoch 272/500, Training Loss: 5.5845\n",
      "Epoch 273/500, Training Loss: 3.8409\n",
      "Epoch 274/500, Training Loss: 4.3459\n",
      "Epoch 275/500, Training Loss: 6.6022\n",
      "Epoch 276/500, Training Loss: 8.1010\n",
      "Epoch 277/500, Training Loss: 6.5620\n",
      "Epoch 278/500, Training Loss: 5.4812\n",
      "Epoch 279/500, Training Loss: 8.5157\n",
      "Epoch 280/500, Training Loss: 5.2731\n",
      "Epoch 281/500, Training Loss: 7.2581\n",
      "Epoch 282/500, Training Loss: 4.8861\n",
      "Epoch 283/500, Training Loss: 6.0361\n",
      "Epoch 284/500, Training Loss: 7.4767\n",
      "Epoch 285/500, Training Loss: 7.8019\n",
      "Epoch 286/500, Training Loss: 5.1224\n",
      "Epoch 287/500, Training Loss: 7.1413\n",
      "Epoch 288/500, Training Loss: 8.8086\n",
      "Epoch 289/500, Training Loss: 6.7437\n",
      "Epoch 290/500, Training Loss: 8.0138\n",
      "Epoch 291/500, Training Loss: 5.4004\n",
      "Epoch 292/500, Training Loss: 4.4093\n",
      "Epoch 293/500, Training Loss: 8.1201\n",
      "Epoch 294/500, Training Loss: 9.5882\n",
      "Epoch 295/500, Training Loss: 5.6892\n",
      "Epoch 296/500, Training Loss: 7.1651\n",
      "Epoch 297/500, Training Loss: 5.0128\n",
      "Epoch 298/500, Training Loss: 7.1476\n",
      "Epoch 299/500, Training Loss: 7.6674\n",
      "Epoch 300/500, Training Loss: 6.6164\n",
      "Epoch 301/500, Training Loss: 4.9029\n",
      "Epoch 302/500, Training Loss: 5.9277\n",
      "Epoch 303/500, Training Loss: 6.1591\n",
      "Epoch 304/500, Training Loss: 6.2656\n",
      "Epoch 305/500, Training Loss: 5.5608\n",
      "Epoch 306/500, Training Loss: 5.3596\n",
      "Epoch 307/500, Training Loss: 4.1463\n",
      "Epoch 308/500, Training Loss: 5.0928\n",
      "Epoch 309/500, Training Loss: 4.1701\n",
      "Epoch 310/500, Training Loss: 4.6168\n",
      "Epoch 311/500, Training Loss: 5.8279\n",
      "Epoch 312/500, Training Loss: 5.4981\n",
      "Epoch 313/500, Training Loss: 3.6956\n",
      "Epoch 314/500, Training Loss: 4.3080\n",
      "Epoch 315/500, Training Loss: 6.2948\n",
      "Epoch 316/500, Training Loss: 7.2960\n",
      "Epoch 317/500, Training Loss: 5.9119\n",
      "Epoch 318/500, Training Loss: 5.2984\n",
      "Epoch 319/500, Training Loss: 3.9657\n",
      "Epoch 320/500, Training Loss: 5.2605\n",
      "Epoch 321/500, Training Loss: 5.4122\n",
      "Epoch 322/500, Training Loss: 4.4846\n",
      "Epoch 323/500, Training Loss: 7.0854\n",
      "Epoch 324/500, Training Loss: 5.6127\n",
      "Epoch 325/500, Training Loss: 4.1863\n",
      "Epoch 326/500, Training Loss: 6.0798\n",
      "Epoch 327/500, Training Loss: 6.7455\n",
      "Epoch 328/500, Training Loss: 8.4566\n",
      "Epoch 329/500, Training Loss: 5.6830\n",
      "Epoch 330/500, Training Loss: 7.4083\n",
      "Epoch 331/500, Training Loss: 5.4348\n",
      "Epoch 332/500, Training Loss: 8.4020\n",
      "Epoch 333/500, Training Loss: 4.3766\n",
      "Epoch 334/500, Training Loss: 4.9226\n",
      "Epoch 335/500, Training Loss: 7.0401\n",
      "Epoch 336/500, Training Loss: 6.0144\n",
      "Epoch 337/500, Training Loss: 6.2397\n",
      "Epoch 338/500, Training Loss: 6.4826\n",
      "Epoch 339/500, Training Loss: 6.9473\n",
      "Epoch 340/500, Training Loss: 5.9991\n",
      "Epoch 341/500, Training Loss: 6.0279\n",
      "Epoch 342/500, Training Loss: 7.4457\n",
      "Epoch 343/500, Training Loss: 5.3866\n",
      "Epoch 344/500, Training Loss: 6.5790\n",
      "Epoch 345/500, Training Loss: 5.5547\n",
      "Epoch 346/500, Training Loss: 5.3803\n",
      "Epoch 347/500, Training Loss: 4.8268\n",
      "Epoch 348/500, Training Loss: 6.1045\n",
      "Epoch 349/500, Training Loss: 7.7378\n",
      "Epoch 350/500, Training Loss: 5.8109\n",
      "Epoch 351/500, Training Loss: 5.9140\n",
      "Epoch 352/500, Training Loss: 4.9712\n",
      "Epoch 353/500, Training Loss: 6.0921\n",
      "Epoch 354/500, Training Loss: 4.5833\n",
      "Epoch 355/500, Training Loss: 4.6461\n",
      "Epoch 356/500, Training Loss: 5.1838\n",
      "Epoch 357/500, Training Loss: 3.9472\n",
      "Epoch 358/500, Training Loss: 4.8505\n",
      "Epoch 359/500, Training Loss: 5.2906\n",
      "Epoch 360/500, Training Loss: 5.6939\n",
      "Epoch 361/500, Training Loss: 5.5584\n",
      "Epoch 362/500, Training Loss: 4.0473\n",
      "Epoch 363/500, Training Loss: 3.6826\n",
      "Epoch 364/500, Training Loss: 5.1764\n",
      "Epoch 365/500, Training Loss: 5.7776\n",
      "Epoch 366/500, Training Loss: 3.7408\n",
      "Epoch 367/500, Training Loss: 5.0519\n",
      "Epoch 368/500, Training Loss: 6.3757\n",
      "Epoch 369/500, Training Loss: 4.6882\n",
      "Epoch 370/500, Training Loss: 7.1004\n",
      "Epoch 371/500, Training Loss: 5.2547\n",
      "Epoch 372/500, Training Loss: 5.5107\n",
      "Epoch 373/500, Training Loss: 4.0561\n",
      "Epoch 374/500, Training Loss: 7.1257\n",
      "Epoch 375/500, Training Loss: 5.2406\n",
      "Epoch 376/500, Training Loss: 5.8107\n",
      "Epoch 377/500, Training Loss: 4.7656\n",
      "Epoch 378/500, Training Loss: 6.1527\n",
      "Epoch 379/500, Training Loss: 6.8451\n",
      "Epoch 380/500, Training Loss: 4.2557\n",
      "Epoch 381/500, Training Loss: 5.3315\n",
      "Epoch 382/500, Training Loss: 4.2502\n",
      "Epoch 383/500, Training Loss: 5.0487\n",
      "Epoch 384/500, Training Loss: 5.9302\n",
      "Epoch 385/500, Training Loss: 5.1493\n",
      "Epoch 386/500, Training Loss: 4.5169\n",
      "Epoch 387/500, Training Loss: 5.2415\n",
      "Epoch 388/500, Training Loss: 4.7814\n",
      "Epoch 389/500, Training Loss: 7.2868\n",
      "Epoch 390/500, Training Loss: 3.6827\n",
      "Epoch 391/500, Training Loss: 5.3677\n",
      "Epoch 392/500, Training Loss: 5.5420\n",
      "Epoch 393/500, Training Loss: 5.1095\n",
      "Epoch 394/500, Training Loss: 5.5622\n",
      "Epoch 395/500, Training Loss: 5.7033\n",
      "Epoch 396/500, Training Loss: 5.2179\n",
      "Epoch 397/500, Training Loss: 6.0225\n",
      "Epoch 398/500, Training Loss: 5.3439\n",
      "Epoch 399/500, Training Loss: 5.6742\n",
      "Epoch 400/500, Training Loss: 4.7387\n",
      "Epoch 401/500, Training Loss: 6.3076\n",
      "Epoch 402/500, Training Loss: 4.6996\n",
      "Epoch 403/500, Training Loss: 7.3956\n",
      "Epoch 404/500, Training Loss: 5.5372\n",
      "Epoch 405/500, Training Loss: 3.6829\n",
      "Epoch 406/500, Training Loss: 5.6425\n",
      "Epoch 407/500, Training Loss: 4.5324\n",
      "Epoch 408/500, Training Loss: 5.6651\n",
      "Epoch 409/500, Training Loss: 4.3907\n",
      "Epoch 410/500, Training Loss: 5.5038\n",
      "Epoch 411/500, Training Loss: 5.8270\n",
      "Epoch 412/500, Training Loss: 4.8313\n",
      "Epoch 413/500, Training Loss: 5.8665\n",
      "Epoch 414/500, Training Loss: 5.1376\n",
      "Epoch 415/500, Training Loss: 5.0104\n",
      "Epoch 416/500, Training Loss: 4.4456\n",
      "Epoch 417/500, Training Loss: 5.6797\n",
      "Epoch 418/500, Training Loss: 4.1308\n",
      "Epoch 419/500, Training Loss: 4.2433\n",
      "Epoch 420/500, Training Loss: 4.5529\n",
      "Epoch 421/500, Training Loss: 5.7619\n",
      "Epoch 422/500, Training Loss: 5.1458\n",
      "Epoch 423/500, Training Loss: 6.1810\n",
      "Epoch 424/500, Training Loss: 4.7482\n",
      "Epoch 425/500, Training Loss: 4.1851\n",
      "Epoch 426/500, Training Loss: 5.1456\n",
      "Epoch 427/500, Training Loss: 4.6680\n",
      "Epoch 428/500, Training Loss: 7.1907\n",
      "Epoch 429/500, Training Loss: 5.1041\n",
      "Epoch 430/500, Training Loss: 6.1248\n",
      "Epoch 431/500, Training Loss: 3.9948\n",
      "Epoch 432/500, Training Loss: 4.4178\n",
      "Epoch 433/500, Training Loss: 4.7107\n",
      "Epoch 434/500, Training Loss: 5.1126\n",
      "Epoch 435/500, Training Loss: 5.0199\n",
      "Epoch 436/500, Training Loss: 4.0329\n",
      "Epoch 437/500, Training Loss: 6.2446\n",
      "Epoch 438/500, Training Loss: 5.0313\n",
      "Epoch 439/500, Training Loss: 3.8135\n",
      "Epoch 440/500, Training Loss: 4.9613\n",
      "Epoch 441/500, Training Loss: 5.4386\n",
      "Epoch 442/500, Training Loss: 5.4319\n",
      "Epoch 443/500, Training Loss: 4.4659\n",
      "Epoch 444/500, Training Loss: 4.7862\n",
      "Epoch 445/500, Training Loss: 5.6653\n",
      "Epoch 446/500, Training Loss: 4.8246\n",
      "Epoch 447/500, Training Loss: 5.3770\n",
      "Epoch 448/500, Training Loss: 4.4455\n",
      "Epoch 449/500, Training Loss: 5.2101\n",
      "Epoch 450/500, Training Loss: 5.3130\n",
      "Epoch 451/500, Training Loss: 5.3183\n",
      "Epoch 452/500, Training Loss: 5.1086\n",
      "Epoch 453/500, Training Loss: 5.7846\n",
      "Epoch 454/500, Training Loss: 4.5982\n",
      "Epoch 455/500, Training Loss: 4.6029\n",
      "Epoch 456/500, Training Loss: 4.2836\n",
      "Epoch 457/500, Training Loss: 6.1111\n",
      "Epoch 458/500, Training Loss: 5.0483\n",
      "Epoch 459/500, Training Loss: 4.2835\n",
      "Epoch 460/500, Training Loss: 6.1297\n",
      "Epoch 461/500, Training Loss: 4.5214\n",
      "Epoch 462/500, Training Loss: 4.4622\n",
      "Epoch 463/500, Training Loss: 3.7067\n",
      "Epoch 464/500, Training Loss: 5.1510\n",
      "Epoch 465/500, Training Loss: 3.6517\n",
      "Epoch 466/500, Training Loss: 5.4632\n",
      "Epoch 467/500, Training Loss: 6.4573\n",
      "Epoch 468/500, Training Loss: 5.7200\n",
      "Epoch 469/500, Training Loss: 6.0675\n",
      "Epoch 470/500, Training Loss: 4.4706\n",
      "Epoch 471/500, Training Loss: 4.5120\n",
      "Epoch 472/500, Training Loss: 4.0792\n",
      "Epoch 473/500, Training Loss: 5.3317\n",
      "Epoch 474/500, Training Loss: 4.4820\n",
      "Epoch 475/500, Training Loss: 5.0089\n",
      "Epoch 476/500, Training Loss: 5.7299\n",
      "Epoch 477/500, Training Loss: 5.4609\n",
      "Epoch 478/500, Training Loss: 4.2863\n",
      "Epoch 479/500, Training Loss: 5.2634\n",
      "Epoch 480/500, Training Loss: 4.6176\n",
      "Epoch 481/500, Training Loss: 4.2301\n",
      "Epoch 482/500, Training Loss: 4.5270\n",
      "Epoch 483/500, Training Loss: 4.1845\n",
      "Epoch 484/500, Training Loss: 5.0655\n",
      "Epoch 485/500, Training Loss: 3.8101\n",
      "Epoch 486/500, Training Loss: 5.0363\n",
      "Epoch 487/500, Training Loss: 5.6902\n",
      "Epoch 488/500, Training Loss: 4.8523\n",
      "Epoch 489/500, Training Loss: 5.1971\n",
      "Epoch 490/500, Training Loss: 4.4846\n",
      "Epoch 491/500, Training Loss: 3.6505\n",
      "Epoch 492/500, Training Loss: 3.5313\n",
      "Epoch 493/500, Training Loss: 4.1675\n",
      "Epoch 494/500, Training Loss: 4.8135\n",
      "Epoch 495/500, Training Loss: 5.3287\n",
      "Epoch 496/500, Training Loss: 4.7839\n",
      "Epoch 497/500, Training Loss: 7.5261\n",
      "Epoch 498/500, Training Loss: 4.5078\n",
      "Epoch 499/500, Training Loss: 4.6996\n",
      "Epoch 500/500, Training Loss: 4.4975\n",
      "Validation Loss: 4.7274\n",
      "Model 1, Fold 0: Validation Loss = 4.7274\n",
      "Epoch 1/500, Training Loss: 69.0006\n",
      "Epoch 2/500, Training Loss: 88.9610\n",
      "Epoch 3/500, Training Loss: 199.2277\n",
      "Epoch 4/500, Training Loss: 38.0913\n",
      "Epoch 5/500, Training Loss: 50.4077\n",
      "Epoch 6/500, Training Loss: 131.4788\n",
      "Epoch 7/500, Training Loss: 247.6003\n",
      "Epoch 8/500, Training Loss: 128.8117\n",
      "Epoch 9/500, Training Loss: 62.6689\n",
      "Epoch 10/500, Training Loss: 27.4912\n",
      "Epoch 11/500, Training Loss: 47.2993\n",
      "Epoch 12/500, Training Loss: 30.1392\n",
      "Epoch 13/500, Training Loss: 23.6550\n",
      "Epoch 14/500, Training Loss: 46.4155\n",
      "Epoch 15/500, Training Loss: 50.2896\n",
      "Epoch 16/500, Training Loss: 109.6702\n",
      "Epoch 17/500, Training Loss: 83.6985\n",
      "Epoch 18/500, Training Loss: 96.5855\n",
      "Epoch 19/500, Training Loss: 113.0093\n",
      "Epoch 20/500, Training Loss: 24.7626\n",
      "Epoch 21/500, Training Loss: 140.1073\n",
      "Epoch 22/500, Training Loss: 132.6626\n",
      "Epoch 23/500, Training Loss: 73.8759\n",
      "Epoch 24/500, Training Loss: 77.3775\n",
      "Epoch 25/500, Training Loss: 27.3544\n",
      "Epoch 26/500, Training Loss: 104.5151\n",
      "Epoch 27/500, Training Loss: 52.5707\n",
      "Epoch 28/500, Training Loss: 58.7327\n",
      "Epoch 29/500, Training Loss: 55.9707\n",
      "Epoch 30/500, Training Loss: 12.6772\n",
      "Epoch 31/500, Training Loss: 24.4928\n",
      "Epoch 32/500, Training Loss: 98.7748\n",
      "Epoch 33/500, Training Loss: 128.3950\n",
      "Epoch 34/500, Training Loss: 83.4946\n",
      "Epoch 35/500, Training Loss: 86.1347\n",
      "Epoch 36/500, Training Loss: 42.7156\n",
      "Epoch 37/500, Training Loss: 13.6965\n",
      "Epoch 38/500, Training Loss: 19.8923\n",
      "Epoch 39/500, Training Loss: 50.8986\n",
      "Epoch 40/500, Training Loss: 14.1399\n",
      "Epoch 41/500, Training Loss: 66.0697\n",
      "Epoch 42/500, Training Loss: 49.7356\n",
      "Epoch 43/500, Training Loss: 60.2116\n",
      "Epoch 44/500, Training Loss: 12.3670\n",
      "Epoch 45/500, Training Loss: 38.8118\n",
      "Epoch 46/500, Training Loss: 31.9832\n",
      "Epoch 47/500, Training Loss: 14.6455\n",
      "Epoch 48/500, Training Loss: 47.8724\n",
      "Epoch 49/500, Training Loss: 14.2163\n",
      "Epoch 50/500, Training Loss: 7.6585\n",
      "Epoch 51/500, Training Loss: 32.2869\n",
      "Epoch 52/500, Training Loss: 14.6997\n",
      "Epoch 53/500, Training Loss: 35.8092\n",
      "Epoch 54/500, Training Loss: 53.1394\n",
      "Epoch 55/500, Training Loss: 19.2029\n",
      "Epoch 56/500, Training Loss: 12.6279\n",
      "Epoch 57/500, Training Loss: 16.0402\n",
      "Epoch 58/500, Training Loss: 46.1870\n",
      "Epoch 59/500, Training Loss: 13.3782\n",
      "Epoch 60/500, Training Loss: 11.1712\n",
      "Epoch 61/500, Training Loss: 33.2780\n",
      "Epoch 62/500, Training Loss: 25.3723\n",
      "Epoch 63/500, Training Loss: 7.7467\n",
      "Epoch 64/500, Training Loss: 44.8943\n",
      "Epoch 65/500, Training Loss: 24.8268\n",
      "Epoch 66/500, Training Loss: 16.8528\n",
      "Epoch 67/500, Training Loss: 32.4926\n",
      "Epoch 68/500, Training Loss: 5.5751\n",
      "Epoch 69/500, Training Loss: 46.6512\n",
      "Epoch 70/500, Training Loss: 6.3130\n",
      "Epoch 71/500, Training Loss: 27.5083\n",
      "Epoch 72/500, Training Loss: 7.9281\n",
      "Epoch 73/500, Training Loss: 10.5831\n",
      "Epoch 74/500, Training Loss: 9.4679\n",
      "Epoch 75/500, Training Loss: 7.8821\n",
      "Epoch 76/500, Training Loss: 19.9760\n",
      "Epoch 77/500, Training Loss: 22.3254\n",
      "Epoch 78/500, Training Loss: 26.6677\n",
      "Epoch 79/500, Training Loss: 8.4894\n",
      "Epoch 80/500, Training Loss: 18.0010\n",
      "Epoch 81/500, Training Loss: 7.4884\n",
      "Epoch 82/500, Training Loss: 37.3300\n",
      "Epoch 83/500, Training Loss: 37.7275\n",
      "Epoch 84/500, Training Loss: 7.1816\n",
      "Epoch 85/500, Training Loss: 21.9154\n",
      "Epoch 86/500, Training Loss: 10.0457\n",
      "Epoch 87/500, Training Loss: 32.7568\n",
      "Epoch 88/500, Training Loss: 25.2239\n",
      "Epoch 89/500, Training Loss: 8.9465\n",
      "Epoch 90/500, Training Loss: 19.3143\n",
      "Epoch 91/500, Training Loss: 7.6188\n",
      "Epoch 92/500, Training Loss: 23.7449\n",
      "Epoch 93/500, Training Loss: 31.6944\n",
      "Epoch 94/500, Training Loss: 16.9101\n",
      "Epoch 95/500, Training Loss: 8.8876\n",
      "Epoch 96/500, Training Loss: 9.7898\n",
      "Epoch 97/500, Training Loss: 30.8825\n",
      "Epoch 98/500, Training Loss: 10.6308\n",
      "Epoch 99/500, Training Loss: 14.7182\n",
      "Epoch 100/500, Training Loss: 7.7947\n",
      "Epoch 101/500, Training Loss: 6.3591\n",
      "Epoch 102/500, Training Loss: 7.8605\n",
      "Epoch 103/500, Training Loss: 10.5384\n",
      "Epoch 104/500, Training Loss: 8.5682\n",
      "Epoch 105/500, Training Loss: 5.2441\n",
      "Epoch 106/500, Training Loss: 6.8966\n",
      "Epoch 107/500, Training Loss: 18.6793\n",
      "Epoch 108/500, Training Loss: 5.9769\n",
      "Epoch 109/500, Training Loss: 7.3368\n",
      "Epoch 110/500, Training Loss: 18.7310\n",
      "Epoch 111/500, Training Loss: 19.1121\n",
      "Epoch 112/500, Training Loss: 10.5846\n",
      "Epoch 113/500, Training Loss: 16.2581\n",
      "Epoch 114/500, Training Loss: 6.5334\n",
      "Epoch 115/500, Training Loss: 5.3817\n",
      "Epoch 116/500, Training Loss: 15.6281\n",
      "Epoch 117/500, Training Loss: 7.6173\n",
      "Epoch 118/500, Training Loss: 8.9496\n",
      "Epoch 119/500, Training Loss: 28.1980\n",
      "Epoch 120/500, Training Loss: 22.8208\n",
      "Epoch 121/500, Training Loss: 4.9826\n",
      "Epoch 122/500, Training Loss: 16.8825\n",
      "Epoch 123/500, Training Loss: 27.5797\n",
      "Epoch 124/500, Training Loss: 11.5606\n",
      "Epoch 125/500, Training Loss: 13.4565\n",
      "Epoch 126/500, Training Loss: 15.5150\n",
      "Epoch 127/500, Training Loss: 7.4990\n",
      "Epoch 128/500, Training Loss: 24.1326\n",
      "Epoch 129/500, Training Loss: 16.3611\n",
      "Epoch 130/500, Training Loss: 12.5054\n",
      "Epoch 131/500, Training Loss: 14.8887\n",
      "Epoch 132/500, Training Loss: 6.9518\n",
      "Epoch 133/500, Training Loss: 22.8373\n",
      "Epoch 134/500, Training Loss: 5.5889\n",
      "Epoch 135/500, Training Loss: 29.5062\n",
      "Epoch 136/500, Training Loss: 19.4695\n",
      "Epoch 137/500, Training Loss: 7.2711\n",
      "Epoch 138/500, Training Loss: 4.8540\n",
      "Epoch 139/500, Training Loss: 10.6617\n",
      "Epoch 140/500, Training Loss: 6.1772\n",
      "Epoch 141/500, Training Loss: 6.0347\n",
      "Epoch 142/500, Training Loss: 16.0928\n",
      "Epoch 143/500, Training Loss: 22.2589\n",
      "Epoch 144/500, Training Loss: 5.8807\n",
      "Epoch 145/500, Training Loss: 20.0856\n",
      "Epoch 146/500, Training Loss: 7.4135\n",
      "Epoch 147/500, Training Loss: 6.7088\n",
      "Epoch 148/500, Training Loss: 5.3967\n",
      "Epoch 149/500, Training Loss: 11.5936\n",
      "Epoch 150/500, Training Loss: 6.1180\n",
      "Epoch 151/500, Training Loss: 19.0467\n",
      "Epoch 152/500, Training Loss: 7.5637\n",
      "Epoch 153/500, Training Loss: 11.4232\n",
      "Epoch 154/500, Training Loss: 6.1217\n",
      "Epoch 155/500, Training Loss: 23.7149\n",
      "Epoch 156/500, Training Loss: 12.0268\n",
      "Epoch 157/500, Training Loss: 11.4748\n",
      "Epoch 158/500, Training Loss: 11.8601\n",
      "Epoch 159/500, Training Loss: 5.1859\n",
      "Epoch 160/500, Training Loss: 15.1932\n",
      "Epoch 161/500, Training Loss: 18.6127\n",
      "Epoch 162/500, Training Loss: 12.1997\n",
      "Epoch 163/500, Training Loss: 4.3757\n",
      "Epoch 164/500, Training Loss: 4.2584\n",
      "Epoch 165/500, Training Loss: 10.8141\n",
      "Epoch 166/500, Training Loss: 16.5527\n",
      "Epoch 167/500, Training Loss: 5.5175\n",
      "Epoch 168/500, Training Loss: 5.4124\n",
      "Epoch 169/500, Training Loss: 5.3450\n",
      "Epoch 170/500, Training Loss: 12.7439\n",
      "Epoch 171/500, Training Loss: 7.2272\n",
      "Epoch 172/500, Training Loss: 14.9670\n",
      "Epoch 173/500, Training Loss: 10.1258\n",
      "Epoch 174/500, Training Loss: 5.2224\n",
      "Epoch 175/500, Training Loss: 6.7420\n",
      "Epoch 176/500, Training Loss: 16.0581\n",
      "Epoch 177/500, Training Loss: 10.6210\n",
      "Epoch 178/500, Training Loss: 9.8619\n",
      "Epoch 179/500, Training Loss: 9.4864\n",
      "Epoch 180/500, Training Loss: 5.6189\n",
      "Epoch 181/500, Training Loss: 5.8084\n",
      "Epoch 182/500, Training Loss: 4.1704\n",
      "Epoch 183/500, Training Loss: 8.9316\n",
      "Epoch 184/500, Training Loss: 13.1608\n",
      "Epoch 185/500, Training Loss: 9.7693\n",
      "Epoch 186/500, Training Loss: 9.1356\n",
      "Epoch 187/500, Training Loss: 4.0779\n",
      "Epoch 188/500, Training Loss: 9.5633\n",
      "Epoch 189/500, Training Loss: 5.5869\n",
      "Epoch 190/500, Training Loss: 13.4382\n",
      "Epoch 191/500, Training Loss: 5.2984\n",
      "Epoch 192/500, Training Loss: 5.0476\n",
      "Epoch 193/500, Training Loss: 6.8806\n",
      "Epoch 194/500, Training Loss: 16.4508\n",
      "Epoch 195/500, Training Loss: 10.9446\n",
      "Epoch 196/500, Training Loss: 9.4982\n",
      "Epoch 197/500, Training Loss: 8.6287\n",
      "Epoch 198/500, Training Loss: 9.9446\n",
      "Epoch 199/500, Training Loss: 4.4187\n",
      "Epoch 200/500, Training Loss: 10.5689\n",
      "Epoch 201/500, Training Loss: 9.5797\n",
      "Epoch 202/500, Training Loss: 13.4174\n",
      "Epoch 203/500, Training Loss: 8.9978\n",
      "Epoch 204/500, Training Loss: 9.0917\n",
      "Epoch 205/500, Training Loss: 12.7823\n",
      "Epoch 206/500, Training Loss: 5.6757\n",
      "Epoch 207/500, Training Loss: 9.9358\n",
      "Epoch 208/500, Training Loss: 4.6409\n",
      "Epoch 209/500, Training Loss: 3.7010\n",
      "Epoch 210/500, Training Loss: 9.0541\n",
      "Epoch 211/500, Training Loss: 5.3523\n",
      "Epoch 212/500, Training Loss: 9.0869\n",
      "Epoch 213/500, Training Loss: 4.6331\n",
      "Epoch 214/500, Training Loss: 13.8942\n",
      "Epoch 215/500, Training Loss: 9.1670\n",
      "Epoch 216/500, Training Loss: 12.5435\n",
      "Epoch 217/500, Training Loss: 5.6938\n",
      "Epoch 218/500, Training Loss: 11.8323\n",
      "Epoch 219/500, Training Loss: 3.1874\n",
      "Epoch 220/500, Training Loss: 3.4176\n",
      "Epoch 221/500, Training Loss: 11.3916\n",
      "Epoch 222/500, Training Loss: 8.0766\n",
      "Epoch 223/500, Training Loss: 13.7004\n",
      "Epoch 224/500, Training Loss: 7.6172\n",
      "Epoch 225/500, Training Loss: 3.8915\n",
      "Epoch 226/500, Training Loss: 8.8277\n",
      "Epoch 227/500, Training Loss: 7.5784\n",
      "Epoch 228/500, Training Loss: 12.7692\n",
      "Epoch 229/500, Training Loss: 4.3726\n",
      "Epoch 230/500, Training Loss: 7.9535\n",
      "Epoch 231/500, Training Loss: 8.3122\n",
      "Epoch 232/500, Training Loss: 4.7976\n",
      "Epoch 233/500, Training Loss: 6.9177\n",
      "Epoch 234/500, Training Loss: 4.4424\n",
      "Epoch 235/500, Training Loss: 10.3947\n",
      "Epoch 236/500, Training Loss: 7.4624\n",
      "Epoch 237/500, Training Loss: 3.4959\n",
      "Epoch 238/500, Training Loss: 7.5146\n",
      "Epoch 239/500, Training Loss: 7.6106\n",
      "Epoch 240/500, Training Loss: 8.0683\n",
      "Epoch 241/500, Training Loss: 3.9671\n",
      "Epoch 242/500, Training Loss: 7.7099\n",
      "Epoch 243/500, Training Loss: 4.0617\n",
      "Epoch 244/500, Training Loss: 3.8596\n",
      "Epoch 245/500, Training Loss: 11.5708\n",
      "Epoch 246/500, Training Loss: 10.3415\n",
      "Epoch 247/500, Training Loss: 9.7357\n",
      "Epoch 248/500, Training Loss: 6.6825\n",
      "Epoch 249/500, Training Loss: 7.2521\n",
      "Epoch 250/500, Training Loss: 3.9666\n",
      "Epoch 251/500, Training Loss: 4.9958\n",
      "Epoch 252/500, Training Loss: 6.8655\n",
      "Epoch 253/500, Training Loss: 7.4997\n",
      "Epoch 254/500, Training Loss: 3.8932\n",
      "Epoch 255/500, Training Loss: 11.3567\n",
      "Epoch 256/500, Training Loss: 7.1107\n",
      "Epoch 257/500, Training Loss: 3.8407\n",
      "Epoch 258/500, Training Loss: 5.4877\n",
      "Epoch 259/500, Training Loss: 7.2899\n",
      "Epoch 260/500, Training Loss: 6.3642\n",
      "Epoch 261/500, Training Loss: 3.7880\n",
      "Epoch 262/500, Training Loss: 13.2859\n",
      "Epoch 263/500, Training Loss: 7.3020\n",
      "Epoch 264/500, Training Loss: 3.2609\n",
      "Epoch 265/500, Training Loss: 3.4348\n",
      "Epoch 266/500, Training Loss: 4.7974\n",
      "Epoch 267/500, Training Loss: 6.4219\n",
      "Epoch 268/500, Training Loss: 6.3037\n",
      "Epoch 269/500, Training Loss: 4.0484\n",
      "Epoch 270/500, Training Loss: 3.2596\n",
      "Epoch 271/500, Training Loss: 10.0546\n",
      "Epoch 272/500, Training Loss: 3.9351\n",
      "Epoch 273/500, Training Loss: 7.4716\n",
      "Epoch 274/500, Training Loss: 10.6097\n",
      "Epoch 275/500, Training Loss: 3.5444\n",
      "Epoch 276/500, Training Loss: 11.2118\n",
      "Epoch 277/500, Training Loss: 8.6323\n",
      "Epoch 278/500, Training Loss: 4.9033\n",
      "Epoch 279/500, Training Loss: 6.7922\n",
      "Epoch 280/500, Training Loss: 6.6484\n",
      "Epoch 281/500, Training Loss: 6.3085\n",
      "Epoch 282/500, Training Loss: 9.1388\n",
      "Epoch 283/500, Training Loss: 5.7139\n",
      "Epoch 284/500, Training Loss: 6.1220\n",
      "Epoch 285/500, Training Loss: 4.5927\n",
      "Epoch 286/500, Training Loss: 5.3462\n",
      "Epoch 287/500, Training Loss: 6.2954\n",
      "Epoch 288/500, Training Loss: 8.3161\n",
      "Epoch 289/500, Training Loss: 4.3027\n",
      "Epoch 290/500, Training Loss: 8.2432\n",
      "Epoch 291/500, Training Loss: 4.2762\n",
      "Epoch 292/500, Training Loss: 3.8185\n",
      "Epoch 293/500, Training Loss: 5.7410\n",
      "Epoch 294/500, Training Loss: 4.1429\n",
      "Epoch 295/500, Training Loss: 3.8817\n",
      "Epoch 296/500, Training Loss: 6.6903\n",
      "Epoch 297/500, Training Loss: 4.1743\n",
      "Epoch 298/500, Training Loss: 4.2685\n",
      "Epoch 299/500, Training Loss: 4.4888\n",
      "Epoch 300/500, Training Loss: 8.8265\n",
      "Epoch 301/500, Training Loss: 6.8627\n",
      "Epoch 302/500, Training Loss: 6.0173\n",
      "Epoch 303/500, Training Loss: 7.6826\n",
      "Epoch 304/500, Training Loss: 3.2231\n",
      "Epoch 305/500, Training Loss: 6.7038\n",
      "Epoch 306/500, Training Loss: 6.5727\n",
      "Epoch 307/500, Training Loss: 8.9318\n",
      "Epoch 308/500, Training Loss: 7.9969\n",
      "Epoch 309/500, Training Loss: 8.2334\n",
      "Epoch 310/500, Training Loss: 8.3957\n",
      "Epoch 311/500, Training Loss: 8.5632\n",
      "Epoch 312/500, Training Loss: 5.6423\n",
      "Epoch 313/500, Training Loss: 8.1766\n",
      "Epoch 314/500, Training Loss: 4.0213\n",
      "Epoch 315/500, Training Loss: 5.7279\n",
      "Epoch 316/500, Training Loss: 4.2007\n",
      "Epoch 317/500, Training Loss: 6.7039\n",
      "Epoch 318/500, Training Loss: 7.8564\n",
      "Epoch 319/500, Training Loss: 5.6941\n",
      "Epoch 320/500, Training Loss: 4.2651\n",
      "Epoch 321/500, Training Loss: 3.6035\n",
      "Epoch 322/500, Training Loss: 5.7329\n",
      "Epoch 323/500, Training Loss: 6.4522\n",
      "Epoch 324/500, Training Loss: 10.3548\n",
      "Epoch 325/500, Training Loss: 5.9136\n",
      "Epoch 326/500, Training Loss: 8.6489\n",
      "Epoch 327/500, Training Loss: 4.2071\n",
      "Epoch 328/500, Training Loss: 6.5641\n",
      "Epoch 329/500, Training Loss: 4.3779\n",
      "Epoch 330/500, Training Loss: 4.9125\n",
      "Epoch 331/500, Training Loss: 6.2582\n",
      "Epoch 332/500, Training Loss: 3.7794\n",
      "Epoch 333/500, Training Loss: 6.2318\n",
      "Epoch 334/500, Training Loss: 5.6165\n",
      "Epoch 335/500, Training Loss: 3.5957\n",
      "Epoch 336/500, Training Loss: 3.5503\n",
      "Epoch 337/500, Training Loss: 3.6547\n",
      "Epoch 338/500, Training Loss: 4.9447\n",
      "Epoch 339/500, Training Loss: 3.6957\n",
      "Epoch 340/500, Training Loss: 7.4437\n",
      "Epoch 341/500, Training Loss: 7.8107\n",
      "Epoch 342/500, Training Loss: 6.4135\n",
      "Epoch 343/500, Training Loss: 5.9717\n",
      "Epoch 344/500, Training Loss: 4.4971\n",
      "Epoch 345/500, Training Loss: 6.7353\n",
      "Epoch 346/500, Training Loss: 5.5454\n",
      "Epoch 347/500, Training Loss: 3.8397\n",
      "Epoch 348/500, Training Loss: 5.9450\n",
      "Epoch 349/500, Training Loss: 5.5764\n",
      "Epoch 350/500, Training Loss: 4.0528\n",
      "Epoch 351/500, Training Loss: 7.3103\n",
      "Epoch 352/500, Training Loss: 6.3366\n",
      "Epoch 353/500, Training Loss: 7.8933\n",
      "Epoch 354/500, Training Loss: 6.1531\n",
      "Epoch 355/500, Training Loss: 3.9406\n",
      "Epoch 356/500, Training Loss: 4.0293\n",
      "Epoch 357/500, Training Loss: 7.0188\n",
      "Epoch 358/500, Training Loss: 3.8667\n",
      "Epoch 359/500, Training Loss: 5.8470\n",
      "Epoch 360/500, Training Loss: 3.4757\n",
      "Epoch 361/500, Training Loss: 4.8093\n",
      "Epoch 362/500, Training Loss: 3.8178\n",
      "Epoch 363/500, Training Loss: 5.8441\n",
      "Epoch 364/500, Training Loss: 4.1215\n",
      "Epoch 365/500, Training Loss: 3.8903\n",
      "Epoch 366/500, Training Loss: 3.8914\n",
      "Epoch 367/500, Training Loss: 3.8429\n",
      "Epoch 368/500, Training Loss: 5.6076\n",
      "Epoch 369/500, Training Loss: 3.6352\n",
      "Epoch 370/500, Training Loss: 4.9606\n",
      "Epoch 371/500, Training Loss: 5.7270\n",
      "Epoch 372/500, Training Loss: 3.7877\n",
      "Epoch 373/500, Training Loss: 3.8915\n",
      "Epoch 374/500, Training Loss: 4.0478\n",
      "Epoch 375/500, Training Loss: 6.0245\n",
      "Epoch 376/500, Training Loss: 7.1692\n",
      "Epoch 377/500, Training Loss: 3.4647\n",
      "Epoch 378/500, Training Loss: 5.7043\n",
      "Epoch 379/500, Training Loss: 5.6388\n",
      "Epoch 380/500, Training Loss: 6.1915\n",
      "Epoch 381/500, Training Loss: 6.5925\n",
      "Epoch 382/500, Training Loss: 3.7374\n",
      "Epoch 383/500, Training Loss: 4.0557\n",
      "Epoch 384/500, Training Loss: 5.4169\n",
      "Epoch 385/500, Training Loss: 5.0623\n",
      "Epoch 386/500, Training Loss: 3.8989\n",
      "Epoch 387/500, Training Loss: 5.4252\n",
      "Epoch 388/500, Training Loss: 3.6341\n",
      "Epoch 389/500, Training Loss: 5.4340\n",
      "Epoch 390/500, Training Loss: 6.0393\n",
      "Epoch 391/500, Training Loss: 3.5897\n",
      "Epoch 392/500, Training Loss: 5.4326\n",
      "Epoch 393/500, Training Loss: 3.4358\n",
      "Epoch 394/500, Training Loss: 5.7117\n",
      "Epoch 395/500, Training Loss: 3.9819\n",
      "Epoch 396/500, Training Loss: 3.7148\n",
      "Epoch 397/500, Training Loss: 5.2632\n",
      "Epoch 398/500, Training Loss: 3.4989\n",
      "Epoch 399/500, Training Loss: 3.7706\n",
      "Epoch 400/500, Training Loss: 5.4231\n",
      "Epoch 401/500, Training Loss: 5.8227\n",
      "Epoch 402/500, Training Loss: 4.4857\n",
      "Epoch 403/500, Training Loss: 7.1007\n",
      "Epoch 404/500, Training Loss: 6.7017\n",
      "Epoch 405/500, Training Loss: 5.5603\n",
      "Epoch 406/500, Training Loss: 4.1515\n",
      "Epoch 407/500, Training Loss: 3.7653\n",
      "Epoch 408/500, Training Loss: 5.3819\n",
      "Epoch 409/500, Training Loss: 5.5085\n",
      "Epoch 410/500, Training Loss: 3.7520\n",
      "Epoch 411/500, Training Loss: 3.4875\n",
      "Epoch 412/500, Training Loss: 3.8268\n",
      "Epoch 413/500, Training Loss: 7.2248\n",
      "Epoch 414/500, Training Loss: 4.8706\n",
      "Epoch 415/500, Training Loss: 5.4077\n",
      "Epoch 416/500, Training Loss: 3.7494\n",
      "Epoch 417/500, Training Loss: 5.9475\n",
      "Epoch 418/500, Training Loss: 3.7099\n",
      "Epoch 419/500, Training Loss: 5.4530\n",
      "Epoch 420/500, Training Loss: 5.6717\n",
      "Epoch 421/500, Training Loss: 4.1147\n",
      "Epoch 422/500, Training Loss: 5.4927\n",
      "Epoch 423/500, Training Loss: 5.7169\n",
      "Epoch 424/500, Training Loss: 6.7152\n",
      "Epoch 425/500, Training Loss: 5.1206\n",
      "Epoch 426/500, Training Loss: 3.7542\n",
      "Epoch 427/500, Training Loss: 5.6825\n",
      "Epoch 428/500, Training Loss: 8.7787\n",
      "Epoch 429/500, Training Loss: 4.4361\n",
      "Epoch 430/500, Training Loss: 6.8050\n",
      "Epoch 431/500, Training Loss: 4.8212\n",
      "Epoch 432/500, Training Loss: 5.6389\n",
      "Epoch 433/500, Training Loss: 3.4580\n",
      "Epoch 434/500, Training Loss: 3.5327\n",
      "Epoch 435/500, Training Loss: 3.9905\n",
      "Epoch 436/500, Training Loss: 5.2491\n",
      "Epoch 437/500, Training Loss: 5.2276\n",
      "Epoch 438/500, Training Loss: 4.3063\n",
      "Epoch 439/500, Training Loss: 6.5912\n",
      "Epoch 440/500, Training Loss: 5.4789\n",
      "Epoch 441/500, Training Loss: 4.1069\n",
      "Epoch 442/500, Training Loss: 3.5816\n",
      "Epoch 443/500, Training Loss: 4.1145\n",
      "Epoch 444/500, Training Loss: 7.6007\n",
      "Epoch 445/500, Training Loss: 4.0032\n",
      "Epoch 446/500, Training Loss: 5.0811\n",
      "Epoch 447/500, Training Loss: 5.2450\n",
      "Epoch 448/500, Training Loss: 5.2344\n",
      "Epoch 449/500, Training Loss: 6.9168\n",
      "Epoch 450/500, Training Loss: 6.0252\n",
      "Epoch 451/500, Training Loss: 8.0569\n",
      "Epoch 452/500, Training Loss: 3.9152\n",
      "Epoch 453/500, Training Loss: 3.6080\n",
      "Epoch 454/500, Training Loss: 3.9507\n",
      "Epoch 455/500, Training Loss: 8.4703\n",
      "Epoch 456/500, Training Loss: 4.0421\n",
      "Epoch 457/500, Training Loss: 5.2683\n",
      "Epoch 458/500, Training Loss: 5.6564\n",
      "Epoch 459/500, Training Loss: 4.9402\n",
      "Epoch 460/500, Training Loss: 3.6550\n",
      "Epoch 461/500, Training Loss: 4.1881\n",
      "Epoch 462/500, Training Loss: 3.9573\n",
      "Epoch 463/500, Training Loss: 5.4038\n",
      "Epoch 464/500, Training Loss: 4.8279\n",
      "Epoch 465/500, Training Loss: 5.4348\n",
      "Epoch 466/500, Training Loss: 6.3498\n",
      "Epoch 467/500, Training Loss: 5.5920\n",
      "Epoch 468/500, Training Loss: 3.8462\n",
      "Epoch 469/500, Training Loss: 4.8222\n",
      "Epoch 470/500, Training Loss: 5.4524\n",
      "Epoch 471/500, Training Loss: 5.2430\n",
      "Epoch 472/500, Training Loss: 6.4849\n",
      "Epoch 473/500, Training Loss: 4.2636\n",
      "Epoch 474/500, Training Loss: 4.8577\n",
      "Epoch 475/500, Training Loss: 6.1972\n",
      "Epoch 476/500, Training Loss: 6.5697\n",
      "Epoch 477/500, Training Loss: 4.9540\n",
      "Epoch 478/500, Training Loss: 5.1987\n",
      "Epoch 479/500, Training Loss: 6.3491\n",
      "Epoch 480/500, Training Loss: 5.1067\n",
      "Epoch 481/500, Training Loss: 4.7363\n",
      "Epoch 482/500, Training Loss: 4.8669\n",
      "Epoch 483/500, Training Loss: 3.5600\n",
      "Epoch 484/500, Training Loss: 4.9429\n",
      "Epoch 485/500, Training Loss: 4.9093\n",
      "Epoch 486/500, Training Loss: 4.9928\n",
      "Epoch 487/500, Training Loss: 6.0692\n",
      "Epoch 488/500, Training Loss: 6.0006\n",
      "Epoch 489/500, Training Loss: 6.7210\n",
      "Epoch 490/500, Training Loss: 4.7278\n",
      "Epoch 491/500, Training Loss: 4.7807\n",
      "Epoch 492/500, Training Loss: 5.0903\n",
      "Epoch 493/500, Training Loss: 3.5444\n",
      "Epoch 494/500, Training Loss: 5.3721\n",
      "Epoch 495/500, Training Loss: 6.0165\n",
      "Epoch 496/500, Training Loss: 3.6835\n",
      "Epoch 497/500, Training Loss: 3.9067\n",
      "Epoch 498/500, Training Loss: 5.1026\n",
      "Epoch 499/500, Training Loss: 5.0442\n",
      "Epoch 500/500, Training Loss: 5.8281\n",
      "Validation Loss: 4.9823\n",
      "Model 1, Fold 1: Validation Loss = 4.9823\n",
      "Epoch 1/500, Training Loss: 186.2228\n",
      "Epoch 2/500, Training Loss: 34.6440\n",
      "Epoch 3/500, Training Loss: 15.9056\n",
      "Epoch 4/500, Training Loss: 6.2925\n",
      "Epoch 5/500, Training Loss: 6.3191\n",
      "Epoch 6/500, Training Loss: 5.6801\n",
      "Epoch 7/500, Training Loss: 4.5276\n",
      "Epoch 8/500, Training Loss: 4.1905\n",
      "Epoch 9/500, Training Loss: 3.9806\n",
      "Epoch 10/500, Training Loss: 3.9071\n",
      "Epoch 11/500, Training Loss: 4.1107\n",
      "Epoch 12/500, Training Loss: 4.1383\n",
      "Epoch 13/500, Training Loss: 4.2148\n",
      "Epoch 14/500, Training Loss: 4.0794\n",
      "Epoch 15/500, Training Loss: 4.4167\n",
      "Epoch 16/500, Training Loss: 4.3083\n",
      "Epoch 17/500, Training Loss: 4.9192\n",
      "Epoch 18/500, Training Loss: 4.1483\n",
      "Epoch 19/500, Training Loss: 3.9940\n",
      "Epoch 20/500, Training Loss: 4.1916\n",
      "Epoch 21/500, Training Loss: 4.0899\n",
      "Epoch 22/500, Training Loss: 4.6405\n",
      "Epoch 23/500, Training Loss: 4.2022\n",
      "Epoch 24/500, Training Loss: 4.1676\n",
      "Epoch 25/500, Training Loss: 3.9748\n",
      "Epoch 26/500, Training Loss: 4.2725\n",
      "Epoch 27/500, Training Loss: 4.3879\n",
      "Epoch 28/500, Training Loss: 4.2593\n",
      "Epoch 29/500, Training Loss: 4.1512\n",
      "Epoch 30/500, Training Loss: 4.0563\n",
      "Epoch 31/500, Training Loss: 4.3777\n",
      "Epoch 32/500, Training Loss: 4.4408\n",
      "Epoch 33/500, Training Loss: 4.4280\n",
      "Epoch 34/500, Training Loss: 4.0720\n",
      "Epoch 35/500, Training Loss: 4.0540\n",
      "Epoch 36/500, Training Loss: 4.2971\n",
      "Epoch 37/500, Training Loss: 4.1822\n",
      "Epoch 38/500, Training Loss: 4.2921\n",
      "Epoch 39/500, Training Loss: 3.9769\n",
      "Epoch 40/500, Training Loss: 4.2616\n",
      "Epoch 41/500, Training Loss: 4.2937\n",
      "Epoch 42/500, Training Loss: 4.4699\n",
      "Epoch 43/500, Training Loss: 4.1654\n",
      "Epoch 44/500, Training Loss: 4.4241\n",
      "Epoch 45/500, Training Loss: 4.4009\n",
      "Epoch 46/500, Training Loss: 3.9673\n",
      "Epoch 47/500, Training Loss: 4.0611\n",
      "Epoch 48/500, Training Loss: 3.9795\n",
      "Epoch 49/500, Training Loss: 4.3234\n",
      "Epoch 50/500, Training Loss: 4.4497\n",
      "Epoch 51/500, Training Loss: 4.5458\n",
      "Epoch 52/500, Training Loss: 4.3916\n",
      "Epoch 53/500, Training Loss: 4.1654\n",
      "Epoch 54/500, Training Loss: 3.9493\n",
      "Epoch 55/500, Training Loss: 4.2000\n",
      "Epoch 56/500, Training Loss: 4.1410\n",
      "Epoch 57/500, Training Loss: 3.9528\n",
      "Epoch 58/500, Training Loss: 3.9815\n",
      "Epoch 59/500, Training Loss: 4.1676\n",
      "Epoch 60/500, Training Loss: 4.2085\n",
      "Epoch 61/500, Training Loss: 3.7916\n",
      "Epoch 62/500, Training Loss: 4.2896\n",
      "Epoch 63/500, Training Loss: 4.2428\n",
      "Epoch 64/500, Training Loss: 4.2140\n",
      "Epoch 65/500, Training Loss: 4.1579\n",
      "Epoch 66/500, Training Loss: 4.0313\n",
      "Epoch 67/500, Training Loss: 4.0636\n",
      "Epoch 68/500, Training Loss: 4.3722\n",
      "Epoch 69/500, Training Loss: 4.1552\n",
      "Epoch 70/500, Training Loss: 4.1405\n",
      "Epoch 71/500, Training Loss: 4.3595\n",
      "Epoch 72/500, Training Loss: 4.2082\n",
      "Epoch 73/500, Training Loss: 4.2186\n",
      "Epoch 74/500, Training Loss: 4.2347\n",
      "Epoch 75/500, Training Loss: 3.9719\n",
      "Epoch 76/500, Training Loss: 4.1681\n",
      "Epoch 77/500, Training Loss: 4.2437\n",
      "Epoch 78/500, Training Loss: 4.4456\n",
      "Epoch 79/500, Training Loss: 4.2187\n",
      "Epoch 80/500, Training Loss: 4.0155\n",
      "Epoch 81/500, Training Loss: 4.3130\n",
      "Epoch 82/500, Training Loss: 4.0962\n",
      "Epoch 83/500, Training Loss: 4.2123\n",
      "Epoch 84/500, Training Loss: 4.2207\n",
      "Epoch 85/500, Training Loss: 4.2484\n",
      "Epoch 86/500, Training Loss: 3.9970\n",
      "Epoch 87/500, Training Loss: 3.8519\n",
      "Epoch 88/500, Training Loss: 4.2724\n",
      "Epoch 89/500, Training Loss: 4.1231\n",
      "Epoch 90/500, Training Loss: 4.2802\n",
      "Epoch 91/500, Training Loss: 4.1432\n",
      "Epoch 92/500, Training Loss: 4.0329\n",
      "Epoch 93/500, Training Loss: 4.2337\n",
      "Epoch 94/500, Training Loss: 3.7985\n",
      "Epoch 95/500, Training Loss: 4.1995\n",
      "Epoch 96/500, Training Loss: 4.1558\n",
      "Epoch 97/500, Training Loss: 3.9240\n",
      "Epoch 98/500, Training Loss: 4.3214\n",
      "Epoch 99/500, Training Loss: 4.2613\n",
      "Epoch 100/500, Training Loss: 4.0235\n",
      "Epoch 101/500, Training Loss: 4.3909\n",
      "Epoch 102/500, Training Loss: 4.4121\n",
      "Epoch 103/500, Training Loss: 4.1754\n",
      "Epoch 104/500, Training Loss: 4.3221\n",
      "Epoch 105/500, Training Loss: 4.1489\n",
      "Epoch 106/500, Training Loss: 4.0674\n",
      "Epoch 107/500, Training Loss: 4.0408\n",
      "Epoch 108/500, Training Loss: 4.2701\n",
      "Epoch 109/500, Training Loss: 3.8802\n",
      "Epoch 110/500, Training Loss: 4.1348\n",
      "Epoch 111/500, Training Loss: 4.1871\n",
      "Epoch 112/500, Training Loss: 4.5069\n",
      "Epoch 113/500, Training Loss: 4.1132\n",
      "Epoch 114/500, Training Loss: 3.9620\n",
      "Epoch 115/500, Training Loss: 4.1543\n",
      "Epoch 116/500, Training Loss: 4.2462\n",
      "Epoch 117/500, Training Loss: 4.2480\n",
      "Epoch 118/500, Training Loss: 4.2105\n",
      "Epoch 119/500, Training Loss: 4.3510\n",
      "Epoch 120/500, Training Loss: 4.0521\n",
      "Epoch 121/500, Training Loss: 4.0980\n",
      "Epoch 122/500, Training Loss: 4.2197\n",
      "Epoch 123/500, Training Loss: 4.2710\n",
      "Epoch 124/500, Training Loss: 4.3343\n",
      "Epoch 125/500, Training Loss: 4.2391\n",
      "Epoch 126/500, Training Loss: 4.1462\n",
      "Epoch 127/500, Training Loss: 4.0462\n",
      "Epoch 128/500, Training Loss: 4.1178\n",
      "Epoch 129/500, Training Loss: 4.2302\n",
      "Epoch 130/500, Training Loss: 4.4383\n",
      "Epoch 131/500, Training Loss: 3.9741\n",
      "Epoch 132/500, Training Loss: 4.1653\n",
      "Epoch 133/500, Training Loss: 4.1384\n",
      "Epoch 134/500, Training Loss: 4.3457\n",
      "Epoch 135/500, Training Loss: 4.3086\n",
      "Epoch 136/500, Training Loss: 4.3944\n",
      "Epoch 137/500, Training Loss: 4.1645\n",
      "Epoch 138/500, Training Loss: 3.9777\n",
      "Epoch 139/500, Training Loss: 4.1434\n",
      "Epoch 140/500, Training Loss: 4.0324\n",
      "Epoch 141/500, Training Loss: 4.1033\n",
      "Epoch 142/500, Training Loss: 4.0912\n",
      "Epoch 143/500, Training Loss: 4.2151\n",
      "Epoch 144/500, Training Loss: 4.2646\n",
      "Epoch 145/500, Training Loss: 4.0576\n",
      "Epoch 146/500, Training Loss: 4.1634\n",
      "Epoch 147/500, Training Loss: 4.4323\n",
      "Epoch 148/500, Training Loss: 4.2026\n",
      "Epoch 149/500, Training Loss: 3.8351\n",
      "Epoch 150/500, Training Loss: 4.4677\n",
      "Epoch 151/500, Training Loss: 4.2693\n",
      "Epoch 152/500, Training Loss: 3.9236\n",
      "Epoch 153/500, Training Loss: 4.2041\n",
      "Epoch 154/500, Training Loss: 4.1453\n",
      "Epoch 155/500, Training Loss: 4.4595\n",
      "Epoch 156/500, Training Loss: 3.9421\n",
      "Epoch 157/500, Training Loss: 4.3733\n",
      "Epoch 158/500, Training Loss: 4.3731\n",
      "Epoch 159/500, Training Loss: 4.0837\n",
      "Epoch 160/500, Training Loss: 4.1692\n",
      "Epoch 161/500, Training Loss: 4.3239\n",
      "Epoch 162/500, Training Loss: 3.9920\n",
      "Epoch 163/500, Training Loss: 4.2318\n",
      "Epoch 164/500, Training Loss: 4.1583\n",
      "Epoch 165/500, Training Loss: 4.0830\n",
      "Epoch 166/500, Training Loss: 3.9507\n",
      "Epoch 167/500, Training Loss: 4.4270\n",
      "Epoch 168/500, Training Loss: 4.2702\n",
      "Epoch 169/500, Training Loss: 4.1842\n",
      "Epoch 170/500, Training Loss: 3.9166\n",
      "Epoch 171/500, Training Loss: 4.3367\n",
      "Epoch 172/500, Training Loss: 4.2153\n",
      "Epoch 173/500, Training Loss: 4.1634\n",
      "Epoch 174/500, Training Loss: 4.1698\n",
      "Epoch 175/500, Training Loss: 4.2965\n",
      "Epoch 176/500, Training Loss: 4.0476\n",
      "Epoch 177/500, Training Loss: 4.3303\n",
      "Epoch 178/500, Training Loss: 4.2385\n",
      "Epoch 179/500, Training Loss: 4.0127\n",
      "Epoch 180/500, Training Loss: 4.0747\n",
      "Epoch 181/500, Training Loss: 4.1097\n",
      "Epoch 182/500, Training Loss: 3.9718\n",
      "Epoch 183/500, Training Loss: 4.0858\n",
      "Epoch 184/500, Training Loss: 4.0126\n",
      "Epoch 185/500, Training Loss: 4.4661\n",
      "Epoch 186/500, Training Loss: 4.1644\n",
      "Epoch 187/500, Training Loss: 4.1570\n",
      "Epoch 188/500, Training Loss: 4.5439\n",
      "Epoch 189/500, Training Loss: 4.2143\n",
      "Epoch 190/500, Training Loss: 4.3909\n",
      "Epoch 191/500, Training Loss: 4.1993\n",
      "Epoch 192/500, Training Loss: 4.2534\n",
      "Epoch 193/500, Training Loss: 4.0723\n",
      "Epoch 194/500, Training Loss: 4.3270\n",
      "Epoch 195/500, Training Loss: 4.2498\n",
      "Epoch 196/500, Training Loss: 4.1019\n",
      "Epoch 197/500, Training Loss: 4.1887\n",
      "Epoch 198/500, Training Loss: 4.1231\n",
      "Epoch 199/500, Training Loss: 4.1437\n",
      "Epoch 200/500, Training Loss: 4.3865\n",
      "Epoch 201/500, Training Loss: 4.1867\n",
      "Epoch 202/500, Training Loss: 4.3822\n",
      "Epoch 203/500, Training Loss: 4.2960\n",
      "Epoch 204/500, Training Loss: 3.9226\n",
      "Epoch 205/500, Training Loss: 4.3399\n",
      "Epoch 206/500, Training Loss: 4.0680\n",
      "Epoch 207/500, Training Loss: 4.2653\n",
      "Epoch 208/500, Training Loss: 4.1724\n",
      "Epoch 209/500, Training Loss: 4.5509\n",
      "Epoch 210/500, Training Loss: 4.2939\n",
      "Epoch 211/500, Training Loss: 4.3238\n",
      "Epoch 212/500, Training Loss: 4.1724\n",
      "Epoch 213/500, Training Loss: 4.1385\n",
      "Epoch 214/500, Training Loss: 4.2045\n",
      "Epoch 215/500, Training Loss: 4.2262\n",
      "Epoch 216/500, Training Loss: 4.0875\n",
      "Epoch 217/500, Training Loss: 4.2915\n",
      "Epoch 218/500, Training Loss: 4.1710\n",
      "Epoch 219/500, Training Loss: 4.0657\n",
      "Epoch 220/500, Training Loss: 4.4523\n",
      "Epoch 221/500, Training Loss: 4.3352\n",
      "Epoch 222/500, Training Loss: 4.2950\n",
      "Epoch 223/500, Training Loss: 4.1224\n",
      "Epoch 224/500, Training Loss: 4.3204\n",
      "Epoch 225/500, Training Loss: 4.2285\n",
      "Epoch 226/500, Training Loss: 4.3072\n",
      "Epoch 227/500, Training Loss: 4.4213\n",
      "Epoch 228/500, Training Loss: 4.1193\n",
      "Epoch 229/500, Training Loss: 4.2127\n",
      "Epoch 230/500, Training Loss: 4.1852\n",
      "Epoch 231/500, Training Loss: 4.2574\n",
      "Epoch 232/500, Training Loss: 4.3178\n",
      "Epoch 233/500, Training Loss: 4.1458\n",
      "Epoch 234/500, Training Loss: 3.9933\n",
      "Epoch 235/500, Training Loss: 4.5437\n",
      "Epoch 236/500, Training Loss: 3.9608\n",
      "Epoch 237/500, Training Loss: 3.9899\n",
      "Epoch 238/500, Training Loss: 4.1458\n",
      "Epoch 239/500, Training Loss: 4.3859\n",
      "Epoch 240/500, Training Loss: 4.1790\n",
      "Epoch 241/500, Training Loss: 4.2666\n",
      "Epoch 242/500, Training Loss: 4.4802\n",
      "Epoch 243/500, Training Loss: 4.2065\n",
      "Epoch 244/500, Training Loss: 3.9189\n",
      "Epoch 245/500, Training Loss: 4.0624\n",
      "Epoch 246/500, Training Loss: 3.8927\n",
      "Epoch 247/500, Training Loss: 4.1096\n",
      "Epoch 248/500, Training Loss: 4.3164\n",
      "Epoch 249/500, Training Loss: 4.1991\n",
      "Epoch 250/500, Training Loss: 4.4307\n",
      "Epoch 251/500, Training Loss: 4.3914\n",
      "Epoch 252/500, Training Loss: 4.4188\n",
      "Epoch 253/500, Training Loss: 4.0708\n",
      "Epoch 254/500, Training Loss: 4.3037\n",
      "Epoch 255/500, Training Loss: 4.2247\n",
      "Epoch 256/500, Training Loss: 4.3207\n",
      "Epoch 257/500, Training Loss: 4.2144\n",
      "Epoch 258/500, Training Loss: 4.1425\n",
      "Epoch 259/500, Training Loss: 4.2474\n",
      "Epoch 260/500, Training Loss: 4.1361\n",
      "Epoch 261/500, Training Loss: 3.9466\n",
      "Epoch 262/500, Training Loss: 4.2686\n",
      "Epoch 263/500, Training Loss: 4.2126\n",
      "Epoch 264/500, Training Loss: 4.1827\n",
      "Epoch 265/500, Training Loss: 3.9644\n",
      "Epoch 266/500, Training Loss: 4.2909\n",
      "Epoch 267/500, Training Loss: 3.9646\n",
      "Epoch 268/500, Training Loss: 3.9826\n",
      "Epoch 269/500, Training Loss: 4.0525\n",
      "Epoch 270/500, Training Loss: 3.9582\n",
      "Epoch 271/500, Training Loss: 3.8464\n",
      "Epoch 272/500, Training Loss: 4.2109\n",
      "Epoch 273/500, Training Loss: 4.2903\n",
      "Epoch 274/500, Training Loss: 3.8827\n",
      "Epoch 275/500, Training Loss: 3.9450\n",
      "Epoch 276/500, Training Loss: 4.0358\n",
      "Epoch 277/500, Training Loss: 4.3031\n",
      "Epoch 278/500, Training Loss: 3.8760\n",
      "Epoch 279/500, Training Loss: 4.4059\n",
      "Epoch 280/500, Training Loss: 4.0635\n",
      "Epoch 281/500, Training Loss: 4.2366\n",
      "Epoch 282/500, Training Loss: 4.0242\n",
      "Epoch 283/500, Training Loss: 4.3003\n",
      "Epoch 284/500, Training Loss: 4.3985\n",
      "Epoch 285/500, Training Loss: 4.0959\n",
      "Epoch 286/500, Training Loss: 4.1147\n",
      "Epoch 287/500, Training Loss: 4.1988\n",
      "Epoch 288/500, Training Loss: 4.2359\n",
      "Epoch 289/500, Training Loss: 4.2991\n",
      "Epoch 290/500, Training Loss: 4.2805\n",
      "Epoch 291/500, Training Loss: 4.4066\n",
      "Epoch 292/500, Training Loss: 4.2148\n",
      "Epoch 293/500, Training Loss: 4.2420\n",
      "Epoch 294/500, Training Loss: 4.1770\n",
      "Epoch 295/500, Training Loss: 4.0221\n",
      "Epoch 296/500, Training Loss: 4.0658\n",
      "Epoch 297/500, Training Loss: 4.2309\n",
      "Epoch 298/500, Training Loss: 4.1186\n",
      "Epoch 299/500, Training Loss: 3.8581\n",
      "Epoch 300/500, Training Loss: 4.3208\n",
      "Epoch 301/500, Training Loss: 4.0007\n",
      "Epoch 302/500, Training Loss: 4.0347\n",
      "Epoch 303/500, Training Loss: 4.2271\n",
      "Epoch 304/500, Training Loss: 4.3392\n",
      "Epoch 305/500, Training Loss: 4.1002\n",
      "Epoch 306/500, Training Loss: 4.0542\n",
      "Epoch 307/500, Training Loss: 4.3969\n",
      "Epoch 308/500, Training Loss: 4.0882\n",
      "Epoch 309/500, Training Loss: 4.3921\n",
      "Epoch 310/500, Training Loss: 4.1770\n",
      "Epoch 311/500, Training Loss: 4.0936\n",
      "Epoch 312/500, Training Loss: 4.1010\n",
      "Epoch 313/500, Training Loss: 4.1421\n",
      "Epoch 314/500, Training Loss: 4.1843\n",
      "Epoch 315/500, Training Loss: 4.0560\n",
      "Epoch 316/500, Training Loss: 4.1118\n",
      "Epoch 317/500, Training Loss: 4.1700\n",
      "Epoch 318/500, Training Loss: 4.2695\n",
      "Epoch 319/500, Training Loss: 4.2346\n",
      "Epoch 320/500, Training Loss: 3.9797\n",
      "Epoch 321/500, Training Loss: 3.8676\n",
      "Epoch 322/500, Training Loss: 4.0335\n",
      "Epoch 323/500, Training Loss: 4.0404\n",
      "Epoch 324/500, Training Loss: 4.1624\n",
      "Epoch 325/500, Training Loss: 4.2451\n",
      "Epoch 326/500, Training Loss: 4.0480\n",
      "Epoch 327/500, Training Loss: 4.2927\n",
      "Epoch 328/500, Training Loss: 4.2456\n",
      "Epoch 329/500, Training Loss: 4.1882\n",
      "Epoch 330/500, Training Loss: 4.2525\n",
      "Epoch 331/500, Training Loss: 4.4163\n",
      "Epoch 332/500, Training Loss: 4.2506\n",
      "Epoch 333/500, Training Loss: 4.2084\n",
      "Epoch 334/500, Training Loss: 4.2145\n",
      "Epoch 335/500, Training Loss: 4.2089\n",
      "Epoch 336/500, Training Loss: 4.1303\n",
      "Epoch 337/500, Training Loss: 4.0594\n",
      "Epoch 338/500, Training Loss: 4.1685\n",
      "Epoch 339/500, Training Loss: 4.1517\n",
      "Epoch 340/500, Training Loss: 4.2645\n",
      "Epoch 341/500, Training Loss: 3.9519\n",
      "Epoch 342/500, Training Loss: 4.1960\n",
      "Epoch 343/500, Training Loss: 4.4244\n",
      "Epoch 344/500, Training Loss: 3.9266\n",
      "Epoch 345/500, Training Loss: 3.9055\n",
      "Epoch 346/500, Training Loss: 3.8223\n",
      "Epoch 347/500, Training Loss: 3.9256\n",
      "Epoch 348/500, Training Loss: 3.8447\n",
      "Epoch 349/500, Training Loss: 4.0813\n",
      "Epoch 350/500, Training Loss: 4.4267\n",
      "Epoch 351/500, Training Loss: 4.1901\n",
      "Epoch 352/500, Training Loss: 4.2038\n",
      "Epoch 353/500, Training Loss: 3.9583\n",
      "Epoch 354/500, Training Loss: 4.1048\n",
      "Epoch 355/500, Training Loss: 4.0735\n",
      "Epoch 356/500, Training Loss: 4.0666\n",
      "Epoch 357/500, Training Loss: 4.2252\n",
      "Epoch 358/500, Training Loss: 3.8706\n",
      "Epoch 359/500, Training Loss: 4.4401\n",
      "Epoch 360/500, Training Loss: 4.3164\n",
      "Epoch 361/500, Training Loss: 4.2124\n",
      "Epoch 362/500, Training Loss: 4.2273\n",
      "Epoch 363/500, Training Loss: 4.0622\n",
      "Epoch 364/500, Training Loss: 3.9705\n",
      "Epoch 365/500, Training Loss: 4.1163\n",
      "Epoch 366/500, Training Loss: 4.0716\n",
      "Epoch 367/500, Training Loss: 4.2694\n",
      "Epoch 368/500, Training Loss: 4.2324\n",
      "Epoch 369/500, Training Loss: 4.1484\n",
      "Epoch 370/500, Training Loss: 4.1520\n",
      "Epoch 371/500, Training Loss: 4.0633\n",
      "Epoch 372/500, Training Loss: 4.2959\n",
      "Epoch 373/500, Training Loss: 4.2734\n",
      "Epoch 374/500, Training Loss: 3.9951\n",
      "Epoch 375/500, Training Loss: 4.1183\n",
      "Epoch 376/500, Training Loss: 4.2611\n",
      "Epoch 377/500, Training Loss: 4.1923\n",
      "Epoch 378/500, Training Loss: 4.2269\n",
      "Epoch 379/500, Training Loss: 4.1715\n",
      "Epoch 380/500, Training Loss: 4.2955\n",
      "Epoch 381/500, Training Loss: 4.1774\n",
      "Epoch 382/500, Training Loss: 4.1150\n",
      "Epoch 383/500, Training Loss: 4.1501\n",
      "Epoch 384/500, Training Loss: 4.1883\n",
      "Epoch 385/500, Training Loss: 4.1343\n",
      "Epoch 386/500, Training Loss: 3.9685\n",
      "Epoch 387/500, Training Loss: 3.9281\n",
      "Epoch 388/500, Training Loss: 3.9471\n",
      "Epoch 389/500, Training Loss: 4.3659\n",
      "Epoch 390/500, Training Loss: 3.9288\n",
      "Epoch 391/500, Training Loss: 4.2030\n",
      "Epoch 392/500, Training Loss: 4.1436\n",
      "Epoch 393/500, Training Loss: 3.7363\n",
      "Epoch 394/500, Training Loss: 3.8529\n",
      "Epoch 395/500, Training Loss: 4.0996\n",
      "Epoch 396/500, Training Loss: 4.0112\n",
      "Epoch 397/500, Training Loss: 3.9666\n",
      "Epoch 398/500, Training Loss: 4.4440\n",
      "Epoch 399/500, Training Loss: 4.1600\n",
      "Epoch 400/500, Training Loss: 4.1008\n",
      "Epoch 401/500, Training Loss: 3.9935\n",
      "Epoch 402/500, Training Loss: 4.0051\n",
      "Epoch 403/500, Training Loss: 3.9579\n",
      "Epoch 404/500, Training Loss: 4.2161\n",
      "Epoch 405/500, Training Loss: 4.1238\n",
      "Epoch 406/500, Training Loss: 4.1420\n",
      "Epoch 407/500, Training Loss: 4.0621\n",
      "Epoch 408/500, Training Loss: 3.9987\n",
      "Epoch 409/500, Training Loss: 4.3539\n",
      "Epoch 410/500, Training Loss: 3.8905\n",
      "Epoch 411/500, Training Loss: 4.1732\n",
      "Epoch 412/500, Training Loss: 3.9298\n",
      "Epoch 413/500, Training Loss: 4.1300\n",
      "Epoch 414/500, Training Loss: 4.0391\n",
      "Epoch 415/500, Training Loss: 4.0829\n",
      "Epoch 416/500, Training Loss: 4.0187\n",
      "Epoch 417/500, Training Loss: 4.2126\n",
      "Epoch 418/500, Training Loss: 3.8917\n",
      "Epoch 419/500, Training Loss: 3.7410\n",
      "Epoch 420/500, Training Loss: 4.0738\n",
      "Epoch 421/500, Training Loss: 3.7019\n",
      "Epoch 422/500, Training Loss: 3.8216\n",
      "Epoch 423/500, Training Loss: 3.8854\n",
      "Epoch 424/500, Training Loss: 4.1501\n",
      "Epoch 425/500, Training Loss: 4.1516\n",
      "Epoch 426/500, Training Loss: 3.9978\n",
      "Epoch 427/500, Training Loss: 3.9217\n",
      "Epoch 428/500, Training Loss: 4.0787\n",
      "Epoch 429/500, Training Loss: 3.9812\n",
      "Epoch 430/500, Training Loss: 3.9369\n",
      "Epoch 431/500, Training Loss: 3.8681\n",
      "Epoch 432/500, Training Loss: 4.1714\n",
      "Epoch 433/500, Training Loss: 4.2382\n",
      "Epoch 434/500, Training Loss: 3.8006\n",
      "Epoch 435/500, Training Loss: 3.9616\n",
      "Epoch 436/500, Training Loss: 3.9718\n",
      "Epoch 437/500, Training Loss: 4.0844\n",
      "Epoch 438/500, Training Loss: 4.0724\n",
      "Epoch 439/500, Training Loss: 3.9448\n",
      "Epoch 440/500, Training Loss: 4.0454\n",
      "Epoch 441/500, Training Loss: 4.0159\n",
      "Epoch 442/500, Training Loss: 3.9251\n",
      "Epoch 443/500, Training Loss: 3.9508\n",
      "Epoch 444/500, Training Loss: 3.7966\n",
      "Epoch 445/500, Training Loss: 4.1611\n",
      "Epoch 446/500, Training Loss: 3.7268\n",
      "Epoch 447/500, Training Loss: 3.8875\n",
      "Epoch 448/500, Training Loss: 3.8754\n",
      "Epoch 449/500, Training Loss: 3.8777\n",
      "Epoch 450/500, Training Loss: 4.2354\n",
      "Epoch 451/500, Training Loss: 4.2254\n",
      "Epoch 452/500, Training Loss: 3.9688\n",
      "Epoch 453/500, Training Loss: 3.9924\n",
      "Epoch 454/500, Training Loss: 4.0016\n",
      "Epoch 455/500, Training Loss: 3.9969\n",
      "Epoch 456/500, Training Loss: 3.8925\n",
      "Epoch 457/500, Training Loss: 4.1907\n",
      "Epoch 458/500, Training Loss: 3.9514\n",
      "Epoch 459/500, Training Loss: 4.1571\n",
      "Epoch 460/500, Training Loss: 3.7493\n",
      "Epoch 461/500, Training Loss: 3.9591\n",
      "Epoch 462/500, Training Loss: 3.9589\n",
      "Epoch 463/500, Training Loss: 3.8269\n",
      "Epoch 464/500, Training Loss: 3.9925\n",
      "Epoch 465/500, Training Loss: 3.9532\n",
      "Epoch 466/500, Training Loss: 4.0664\n",
      "Epoch 467/500, Training Loss: 4.0842\n",
      "Epoch 468/500, Training Loss: 4.0940\n",
      "Epoch 469/500, Training Loss: 3.9730\n",
      "Epoch 470/500, Training Loss: 3.8630\n",
      "Epoch 471/500, Training Loss: 3.8492\n",
      "Epoch 472/500, Training Loss: 3.9860\n",
      "Epoch 473/500, Training Loss: 3.9053\n",
      "Epoch 474/500, Training Loss: 3.9789\n",
      "Epoch 475/500, Training Loss: 3.8579\n",
      "Epoch 476/500, Training Loss: 4.0897\n",
      "Epoch 477/500, Training Loss: 3.9416\n",
      "Epoch 478/500, Training Loss: 3.9839\n",
      "Epoch 479/500, Training Loss: 3.8797\n",
      "Epoch 480/500, Training Loss: 4.0348\n",
      "Epoch 481/500, Training Loss: 3.9315\n",
      "Epoch 482/500, Training Loss: 4.0793\n",
      "Epoch 483/500, Training Loss: 3.9485\n",
      "Epoch 484/500, Training Loss: 3.7515\n",
      "Epoch 485/500, Training Loss: 4.0119\n",
      "Epoch 486/500, Training Loss: 3.8122\n",
      "Epoch 487/500, Training Loss: 4.3992\n",
      "Epoch 488/500, Training Loss: 3.9718\n",
      "Epoch 489/500, Training Loss: 3.9257\n",
      "Epoch 490/500, Training Loss: 3.9855\n",
      "Epoch 491/500, Training Loss: 3.9836\n",
      "Epoch 492/500, Training Loss: 4.0563\n",
      "Epoch 493/500, Training Loss: 3.9495\n",
      "Epoch 494/500, Training Loss: 3.8289\n",
      "Epoch 495/500, Training Loss: 3.9307\n",
      "Epoch 496/500, Training Loss: 3.8188\n",
      "Epoch 497/500, Training Loss: 3.7891\n",
      "Epoch 498/500, Training Loss: 3.9857\n",
      "Epoch 499/500, Training Loss: 3.7848\n",
      "Epoch 500/500, Training Loss: 4.1934\n",
      "Validation Loss: 4.7023\n",
      "Model 2, Fold 0: Validation Loss = 4.7023\n",
      "Epoch 1/500, Training Loss: 212.7251\n",
      "Epoch 2/500, Training Loss: 16.3638\n",
      "Epoch 3/500, Training Loss: 16.2333\n",
      "Epoch 4/500, Training Loss: 6.4991\n",
      "Epoch 5/500, Training Loss: 12.0374\n",
      "Epoch 6/500, Training Loss: 4.0142\n",
      "Epoch 7/500, Training Loss: 6.2056\n",
      "Epoch 8/500, Training Loss: 7.7013\n",
      "Epoch 9/500, Training Loss: 4.0443\n",
      "Epoch 10/500, Training Loss: 4.8255\n",
      "Epoch 11/500, Training Loss: 4.7013\n",
      "Epoch 12/500, Training Loss: 4.5537\n",
      "Epoch 13/500, Training Loss: 4.4503\n",
      "Epoch 14/500, Training Loss: 4.5849\n",
      "Epoch 15/500, Training Loss: 4.7501\n",
      "Epoch 16/500, Training Loss: 4.0545\n",
      "Epoch 17/500, Training Loss: 4.0491\n",
      "Epoch 18/500, Training Loss: 4.3004\n",
      "Epoch 19/500, Training Loss: 4.0464\n",
      "Epoch 20/500, Training Loss: 4.0020\n",
      "Epoch 21/500, Training Loss: 4.0181\n",
      "Epoch 22/500, Training Loss: 4.4067\n",
      "Epoch 23/500, Training Loss: 4.3741\n",
      "Epoch 24/500, Training Loss: 4.4419\n",
      "Epoch 25/500, Training Loss: 4.5949\n",
      "Epoch 26/500, Training Loss: 4.7563\n",
      "Epoch 27/500, Training Loss: 4.3140\n",
      "Epoch 28/500, Training Loss: 4.0197\n",
      "Epoch 29/500, Training Loss: 3.9141\n",
      "Epoch 30/500, Training Loss: 4.4234\n",
      "Epoch 31/500, Training Loss: 3.9012\n",
      "Epoch 32/500, Training Loss: 3.9618\n",
      "Epoch 33/500, Training Loss: 3.9407\n",
      "Epoch 34/500, Training Loss: 3.9336\n",
      "Epoch 35/500, Training Loss: 4.5504\n",
      "Epoch 36/500, Training Loss: 3.8813\n",
      "Epoch 37/500, Training Loss: 5.2191\n",
      "Epoch 38/500, Training Loss: 3.8189\n",
      "Epoch 39/500, Training Loss: 5.4444\n",
      "Epoch 40/500, Training Loss: 4.5056\n",
      "Epoch 41/500, Training Loss: 4.7044\n",
      "Epoch 42/500, Training Loss: 4.2607\n",
      "Epoch 43/500, Training Loss: 3.8400\n",
      "Epoch 44/500, Training Loss: 3.8889\n",
      "Epoch 45/500, Training Loss: 4.2511\n",
      "Epoch 46/500, Training Loss: 4.4330\n",
      "Epoch 47/500, Training Loss: 4.4325\n",
      "Epoch 48/500, Training Loss: 4.1667\n",
      "Epoch 49/500, Training Loss: 4.5492\n",
      "Epoch 50/500, Training Loss: 4.2761\n",
      "Epoch 51/500, Training Loss: 3.8710\n",
      "Epoch 52/500, Training Loss: 4.3695\n",
      "Epoch 53/500, Training Loss: 4.2081\n",
      "Epoch 54/500, Training Loss: 3.9529\n",
      "Epoch 55/500, Training Loss: 3.8754\n",
      "Epoch 56/500, Training Loss: 3.8063\n",
      "Epoch 57/500, Training Loss: 4.4134\n",
      "Epoch 58/500, Training Loss: 4.1335\n",
      "Epoch 59/500, Training Loss: 3.9964\n",
      "Epoch 60/500, Training Loss: 4.5061\n",
      "Epoch 61/500, Training Loss: 3.9487\n",
      "Epoch 62/500, Training Loss: 4.6844\n",
      "Epoch 63/500, Training Loss: 3.7944\n",
      "Epoch 64/500, Training Loss: 4.2695\n",
      "Epoch 65/500, Training Loss: 4.0223\n",
      "Epoch 66/500, Training Loss: 4.3084\n",
      "Epoch 67/500, Training Loss: 4.0288\n",
      "Epoch 68/500, Training Loss: 4.3142\n",
      "Epoch 69/500, Training Loss: 4.4419\n",
      "Epoch 70/500, Training Loss: 4.1862\n",
      "Epoch 71/500, Training Loss: 3.9076\n",
      "Epoch 72/500, Training Loss: 4.0164\n",
      "Epoch 73/500, Training Loss: 4.2305\n",
      "Epoch 74/500, Training Loss: 4.3892\n",
      "Epoch 75/500, Training Loss: 4.0019\n",
      "Epoch 76/500, Training Loss: 4.2868\n",
      "Epoch 77/500, Training Loss: 3.9296\n",
      "Epoch 78/500, Training Loss: 4.2276\n",
      "Epoch 79/500, Training Loss: 4.1108\n",
      "Epoch 80/500, Training Loss: 4.3833\n",
      "Epoch 81/500, Training Loss: 4.2816\n",
      "Epoch 82/500, Training Loss: 4.1100\n",
      "Epoch 83/500, Training Loss: 3.8790\n",
      "Epoch 84/500, Training Loss: 3.8459\n",
      "Epoch 85/500, Training Loss: 4.5423\n",
      "Epoch 86/500, Training Loss: 4.2544\n",
      "Epoch 87/500, Training Loss: 4.2673\n",
      "Epoch 88/500, Training Loss: 3.7795\n",
      "Epoch 89/500, Training Loss: 4.4635\n",
      "Epoch 90/500, Training Loss: 3.9220\n",
      "Epoch 91/500, Training Loss: 4.2378\n",
      "Epoch 92/500, Training Loss: 4.1203\n",
      "Epoch 93/500, Training Loss: 4.0518\n",
      "Epoch 94/500, Training Loss: 3.8979\n",
      "Epoch 95/500, Training Loss: 4.1431\n",
      "Epoch 96/500, Training Loss: 4.3917\n",
      "Epoch 97/500, Training Loss: 4.5903\n",
      "Epoch 98/500, Training Loss: 4.2689\n",
      "Epoch 99/500, Training Loss: 4.3414\n",
      "Epoch 100/500, Training Loss: 3.9088\n",
      "Epoch 101/500, Training Loss: 4.2276\n",
      "Epoch 102/500, Training Loss: 4.2929\n",
      "Epoch 103/500, Training Loss: 4.0720\n",
      "Epoch 104/500, Training Loss: 4.2858\n",
      "Epoch 105/500, Training Loss: 4.1923\n",
      "Epoch 106/500, Training Loss: 4.2957\n",
      "Epoch 107/500, Training Loss: 4.2733\n",
      "Epoch 108/500, Training Loss: 4.3155\n",
      "Epoch 109/500, Training Loss: 3.9491\n",
      "Epoch 110/500, Training Loss: 4.5133\n",
      "Epoch 111/500, Training Loss: 3.8949\n",
      "Epoch 112/500, Training Loss: 4.0298\n",
      "Epoch 113/500, Training Loss: 4.0193\n",
      "Epoch 114/500, Training Loss: 4.2653\n",
      "Epoch 115/500, Training Loss: 4.2863\n",
      "Epoch 116/500, Training Loss: 4.4821\n",
      "Epoch 117/500, Training Loss: 4.1096\n",
      "Epoch 118/500, Training Loss: 4.2736\n",
      "Epoch 119/500, Training Loss: 4.0800\n",
      "Epoch 120/500, Training Loss: 4.2402\n",
      "Epoch 121/500, Training Loss: 4.0360\n",
      "Epoch 122/500, Training Loss: 4.4397\n",
      "Epoch 123/500, Training Loss: 3.9775\n",
      "Epoch 124/500, Training Loss: 3.8918\n",
      "Epoch 125/500, Training Loss: 4.0582\n",
      "Epoch 126/500, Training Loss: 4.2953\n",
      "Epoch 127/500, Training Loss: 3.8431\n",
      "Epoch 128/500, Training Loss: 3.8637\n",
      "Epoch 129/500, Training Loss: 4.1044\n",
      "Epoch 130/500, Training Loss: 3.9516\n",
      "Epoch 131/500, Training Loss: 3.8045\n",
      "Epoch 132/500, Training Loss: 4.8820\n",
      "Epoch 133/500, Training Loss: 4.5366\n",
      "Epoch 134/500, Training Loss: 4.0101\n",
      "Epoch 135/500, Training Loss: 3.7811\n",
      "Epoch 136/500, Training Loss: 4.4449\n",
      "Epoch 137/500, Training Loss: 4.5405\n",
      "Epoch 138/500, Training Loss: 4.2533\n",
      "Epoch 139/500, Training Loss: 4.2180\n",
      "Epoch 140/500, Training Loss: 4.2600\n",
      "Epoch 141/500, Training Loss: 3.8353\n",
      "Epoch 142/500, Training Loss: 4.3521\n",
      "Epoch 143/500, Training Loss: 4.2839\n",
      "Epoch 144/500, Training Loss: 4.1154\n",
      "Epoch 145/500, Training Loss: 4.1735\n",
      "Epoch 146/500, Training Loss: 4.4823\n",
      "Epoch 147/500, Training Loss: 3.9984\n",
      "Epoch 148/500, Training Loss: 4.1134\n",
      "Epoch 149/500, Training Loss: 4.2681\n",
      "Epoch 150/500, Training Loss: 4.2561\n",
      "Epoch 151/500, Training Loss: 4.1341\n",
      "Epoch 152/500, Training Loss: 3.9896\n",
      "Epoch 153/500, Training Loss: 4.1581\n",
      "Epoch 154/500, Training Loss: 3.8144\n",
      "Epoch 155/500, Training Loss: 3.9080\n",
      "Epoch 156/500, Training Loss: 3.8484\n",
      "Epoch 157/500, Training Loss: 4.2346\n",
      "Epoch 158/500, Training Loss: 4.3279\n",
      "Epoch 159/500, Training Loss: 3.6365\n",
      "Epoch 160/500, Training Loss: 4.1676\n",
      "Epoch 161/500, Training Loss: 3.8267\n",
      "Epoch 162/500, Training Loss: 3.7464\n",
      "Epoch 163/500, Training Loss: 4.4353\n",
      "Epoch 164/500, Training Loss: 4.4146\n",
      "Epoch 165/500, Training Loss: 4.2114\n",
      "Epoch 166/500, Training Loss: 3.9873\n",
      "Epoch 167/500, Training Loss: 3.8866\n",
      "Epoch 168/500, Training Loss: 4.1450\n",
      "Epoch 169/500, Training Loss: 3.9453\n",
      "Epoch 170/500, Training Loss: 4.0329\n",
      "Epoch 171/500, Training Loss: 4.2645\n",
      "Epoch 172/500, Training Loss: 3.9140\n",
      "Epoch 173/500, Training Loss: 4.1094\n",
      "Epoch 174/500, Training Loss: 3.9212\n",
      "Epoch 175/500, Training Loss: 4.4093\n",
      "Epoch 176/500, Training Loss: 3.8652\n",
      "Epoch 177/500, Training Loss: 4.0758\n",
      "Epoch 178/500, Training Loss: 3.9698\n",
      "Epoch 179/500, Training Loss: 4.2176\n",
      "Epoch 180/500, Training Loss: 4.3889\n",
      "Epoch 181/500, Training Loss: 3.7584\n",
      "Epoch 182/500, Training Loss: 4.0926\n",
      "Epoch 183/500, Training Loss: 3.8924\n",
      "Epoch 184/500, Training Loss: 4.0756\n",
      "Epoch 185/500, Training Loss: 4.0730\n",
      "Epoch 186/500, Training Loss: 3.7917\n",
      "Epoch 187/500, Training Loss: 3.9197\n",
      "Epoch 188/500, Training Loss: 3.8563\n",
      "Epoch 189/500, Training Loss: 3.9457\n",
      "Epoch 190/500, Training Loss: 3.8183\n",
      "Epoch 191/500, Training Loss: 3.7742\n",
      "Epoch 192/500, Training Loss: 4.1142\n",
      "Epoch 193/500, Training Loss: 4.2152\n",
      "Epoch 194/500, Training Loss: 3.9692\n",
      "Epoch 195/500, Training Loss: 3.8044\n",
      "Epoch 196/500, Training Loss: 4.0051\n",
      "Epoch 197/500, Training Loss: 3.8017\n",
      "Epoch 198/500, Training Loss: 3.8559\n",
      "Epoch 199/500, Training Loss: 3.9269\n",
      "Epoch 200/500, Training Loss: 3.8694\n",
      "Epoch 201/500, Training Loss: 4.0433\n",
      "Epoch 202/500, Training Loss: 4.2257\n",
      "Epoch 203/500, Training Loss: 4.0997\n",
      "Epoch 204/500, Training Loss: 3.8126\n",
      "Epoch 205/500, Training Loss: 3.7460\n",
      "Epoch 206/500, Training Loss: 3.7999\n",
      "Epoch 207/500, Training Loss: 4.0443\n",
      "Epoch 208/500, Training Loss: 3.7534\n",
      "Epoch 209/500, Training Loss: 3.9096\n",
      "Epoch 210/500, Training Loss: 4.1754\n",
      "Epoch 211/500, Training Loss: 4.2551\n",
      "Epoch 212/500, Training Loss: 3.9461\n",
      "Epoch 213/500, Training Loss: 4.2038\n",
      "Epoch 214/500, Training Loss: 4.0354\n",
      "Epoch 215/500, Training Loss: 3.9620\n",
      "Epoch 216/500, Training Loss: 4.0084\n",
      "Epoch 217/500, Training Loss: 4.0639\n",
      "Epoch 218/500, Training Loss: 3.7169\n",
      "Epoch 219/500, Training Loss: 4.3292\n",
      "Epoch 220/500, Training Loss: 4.2826\n",
      "Epoch 221/500, Training Loss: 3.9898\n",
      "Epoch 222/500, Training Loss: 3.9184\n",
      "Epoch 223/500, Training Loss: 4.0006\n",
      "Epoch 224/500, Training Loss: 4.2954\n",
      "Epoch 225/500, Training Loss: 3.8138\n",
      "Epoch 226/500, Training Loss: 3.9266\n",
      "Epoch 227/500, Training Loss: 3.7868\n",
      "Epoch 228/500, Training Loss: 4.1352\n",
      "Epoch 229/500, Training Loss: 3.7018\n",
      "Epoch 230/500, Training Loss: 3.7628\n",
      "Epoch 231/500, Training Loss: 3.7442\n",
      "Epoch 232/500, Training Loss: 3.8795\n",
      "Epoch 233/500, Training Loss: 4.2802\n",
      "Epoch 234/500, Training Loss: 3.8128\n",
      "Epoch 235/500, Training Loss: 4.0897\n",
      "Epoch 236/500, Training Loss: 4.3759\n",
      "Epoch 237/500, Training Loss: 4.1652\n",
      "Epoch 238/500, Training Loss: 3.9888\n",
      "Epoch 239/500, Training Loss: 3.8599\n",
      "Epoch 240/500, Training Loss: 4.3206\n",
      "Epoch 241/500, Training Loss: 3.8014\n",
      "Epoch 242/500, Training Loss: 4.0522\n",
      "Epoch 243/500, Training Loss: 4.0629\n",
      "Epoch 244/500, Training Loss: 3.8884\n",
      "Epoch 245/500, Training Loss: 3.7421\n",
      "Epoch 246/500, Training Loss: 3.7271\n",
      "Epoch 247/500, Training Loss: 3.8076\n",
      "Epoch 248/500, Training Loss: 4.0067\n",
      "Epoch 249/500, Training Loss: 3.7211\n",
      "Epoch 250/500, Training Loss: 4.1515\n",
      "Epoch 251/500, Training Loss: 3.9279\n",
      "Epoch 252/500, Training Loss: 3.9735\n",
      "Epoch 253/500, Training Loss: 3.7889\n",
      "Epoch 254/500, Training Loss: 3.8167\n",
      "Epoch 255/500, Training Loss: 4.1248\n",
      "Epoch 256/500, Training Loss: 4.1897\n",
      "Epoch 257/500, Training Loss: 4.2231\n",
      "Epoch 258/500, Training Loss: 4.0771\n",
      "Epoch 259/500, Training Loss: 3.8384\n",
      "Epoch 260/500, Training Loss: 4.0747\n",
      "Epoch 261/500, Training Loss: 3.7425\n",
      "Epoch 262/500, Training Loss: 4.0025\n",
      "Epoch 263/500, Training Loss: 3.8396\n",
      "Epoch 264/500, Training Loss: 4.0906\n",
      "Epoch 265/500, Training Loss: 4.1669\n",
      "Epoch 266/500, Training Loss: 3.9799\n",
      "Epoch 267/500, Training Loss: 3.8620\n",
      "Epoch 268/500, Training Loss: 4.3417\n",
      "Epoch 269/500, Training Loss: 3.8329\n",
      "Epoch 270/500, Training Loss: 3.9795\n",
      "Epoch 271/500, Training Loss: 3.9760\n",
      "Epoch 272/500, Training Loss: 3.8981\n",
      "Epoch 273/500, Training Loss: 3.9482\n",
      "Epoch 274/500, Training Loss: 3.8093\n",
      "Epoch 275/500, Training Loss: 3.9327\n",
      "Epoch 276/500, Training Loss: 4.0119\n",
      "Epoch 277/500, Training Loss: 3.6791\n",
      "Epoch 278/500, Training Loss: 3.8532\n",
      "Epoch 279/500, Training Loss: 4.0764\n",
      "Epoch 280/500, Training Loss: 3.5852\n",
      "Epoch 281/500, Training Loss: 3.9541\n",
      "Epoch 282/500, Training Loss: 3.8301\n",
      "Epoch 283/500, Training Loss: 3.7343\n",
      "Epoch 284/500, Training Loss: 4.2353\n",
      "Epoch 285/500, Training Loss: 4.1732\n",
      "Epoch 286/500, Training Loss: 3.8277\n",
      "Epoch 287/500, Training Loss: 4.0105\n",
      "Epoch 288/500, Training Loss: 4.0007\n",
      "Epoch 289/500, Training Loss: 3.8066\n",
      "Epoch 290/500, Training Loss: 3.8045\n",
      "Epoch 291/500, Training Loss: 3.9038\n",
      "Epoch 292/500, Training Loss: 4.2238\n",
      "Epoch 293/500, Training Loss: 3.7472\n",
      "Epoch 294/500, Training Loss: 3.9051\n",
      "Epoch 295/500, Training Loss: 3.9774\n",
      "Epoch 296/500, Training Loss: 3.8212\n",
      "Epoch 297/500, Training Loss: 3.6563\n",
      "Epoch 298/500, Training Loss: 4.4403\n",
      "Epoch 299/500, Training Loss: 3.9421\n",
      "Epoch 300/500, Training Loss: 4.0666\n",
      "Epoch 301/500, Training Loss: 3.7892\n",
      "Epoch 302/500, Training Loss: 4.0748\n",
      "Epoch 303/500, Training Loss: 4.1506\n",
      "Epoch 304/500, Training Loss: 4.0249\n",
      "Epoch 305/500, Training Loss: 3.6902\n",
      "Epoch 306/500, Training Loss: 3.9664\n",
      "Epoch 307/500, Training Loss: 3.8618\n",
      "Epoch 308/500, Training Loss: 4.1069\n",
      "Epoch 309/500, Training Loss: 4.1239\n",
      "Epoch 310/500, Training Loss: 3.5840\n",
      "Epoch 311/500, Training Loss: 3.7961\n",
      "Epoch 312/500, Training Loss: 3.8503\n",
      "Epoch 313/500, Training Loss: 3.6955\n",
      "Epoch 314/500, Training Loss: 4.1209\n",
      "Epoch 315/500, Training Loss: 4.0499\n",
      "Epoch 316/500, Training Loss: 3.9069\n",
      "Epoch 317/500, Training Loss: 4.0922\n",
      "Epoch 318/500, Training Loss: 4.0868\n",
      "Epoch 319/500, Training Loss: 3.8545\n",
      "Epoch 320/500, Training Loss: 3.6237\n",
      "Epoch 321/500, Training Loss: 3.7780\n",
      "Epoch 322/500, Training Loss: 4.0191\n",
      "Epoch 323/500, Training Loss: 4.0924\n",
      "Epoch 324/500, Training Loss: 3.6550\n",
      "Epoch 325/500, Training Loss: 3.8819\n",
      "Epoch 326/500, Training Loss: 3.9495\n",
      "Epoch 327/500, Training Loss: 3.8294\n",
      "Epoch 328/500, Training Loss: 3.9091\n",
      "Epoch 329/500, Training Loss: 3.7191\n",
      "Epoch 330/500, Training Loss: 3.8465\n",
      "Epoch 331/500, Training Loss: 4.1957\n",
      "Epoch 332/500, Training Loss: 3.9683\n",
      "Epoch 333/500, Training Loss: 3.8579\n",
      "Epoch 334/500, Training Loss: 3.9705\n",
      "Epoch 335/500, Training Loss: 4.2276\n",
      "Epoch 336/500, Training Loss: 3.7008\n",
      "Epoch 337/500, Training Loss: 4.0188\n",
      "Epoch 338/500, Training Loss: 3.6916\n",
      "Epoch 339/500, Training Loss: 3.6375\n",
      "Epoch 340/500, Training Loss: 3.8808\n",
      "Epoch 341/500, Training Loss: 3.5637\n",
      "Epoch 342/500, Training Loss: 3.7054\n",
      "Epoch 343/500, Training Loss: 3.5270\n",
      "Epoch 344/500, Training Loss: 3.7794\n",
      "Epoch 345/500, Training Loss: 4.1014\n",
      "Epoch 346/500, Training Loss: 4.0306\n",
      "Epoch 347/500, Training Loss: 3.8653\n",
      "Epoch 348/500, Training Loss: 3.8216\n",
      "Epoch 349/500, Training Loss: 3.6601\n",
      "Epoch 350/500, Training Loss: 3.7696\n",
      "Epoch 351/500, Training Loss: 3.8140\n",
      "Epoch 352/500, Training Loss: 4.1820\n",
      "Epoch 353/500, Training Loss: 3.9914\n",
      "Epoch 354/500, Training Loss: 3.8083\n",
      "Epoch 355/500, Training Loss: 3.6642\n",
      "Epoch 356/500, Training Loss: 4.1773\n",
      "Epoch 357/500, Training Loss: 3.6118\n",
      "Epoch 358/500, Training Loss: 3.5852\n",
      "Epoch 359/500, Training Loss: 4.2065\n",
      "Epoch 360/500, Training Loss: 3.6824\n",
      "Epoch 361/500, Training Loss: 3.7894\n",
      "Epoch 362/500, Training Loss: 3.9729\n",
      "Epoch 363/500, Training Loss: 4.0699\n",
      "Epoch 364/500, Training Loss: 3.8981\n",
      "Epoch 365/500, Training Loss: 3.9288\n",
      "Epoch 366/500, Training Loss: 4.1850\n",
      "Epoch 367/500, Training Loss: 4.1875\n",
      "Epoch 368/500, Training Loss: 3.9368\n",
      "Epoch 369/500, Training Loss: 3.8374\n",
      "Epoch 370/500, Training Loss: 3.9077\n",
      "Epoch 371/500, Training Loss: 3.8034\n",
      "Epoch 372/500, Training Loss: 4.0553\n",
      "Epoch 373/500, Training Loss: 3.8186\n",
      "Epoch 374/500, Training Loss: 4.2766\n",
      "Epoch 375/500, Training Loss: 3.8600\n",
      "Epoch 376/500, Training Loss: 3.7986\n",
      "Epoch 377/500, Training Loss: 4.1350\n",
      "Epoch 378/500, Training Loss: 3.9647\n",
      "Epoch 379/500, Training Loss: 4.0056\n",
      "Epoch 380/500, Training Loss: 3.8018\n",
      "Epoch 381/500, Training Loss: 3.9703\n",
      "Epoch 382/500, Training Loss: 3.6675\n",
      "Epoch 383/500, Training Loss: 3.7181\n",
      "Epoch 384/500, Training Loss: 3.8974\n",
      "Epoch 385/500, Training Loss: 3.8038\n",
      "Epoch 386/500, Training Loss: 4.0249\n",
      "Epoch 387/500, Training Loss: 3.8186\n",
      "Epoch 388/500, Training Loss: 3.6146\n",
      "Epoch 389/500, Training Loss: 3.7335\n",
      "Epoch 390/500, Training Loss: 3.8017\n",
      "Epoch 391/500, Training Loss: 4.0518\n",
      "Epoch 392/500, Training Loss: 3.8189\n",
      "Epoch 393/500, Training Loss: 3.8250\n",
      "Epoch 394/500, Training Loss: 3.8398\n",
      "Epoch 395/500, Training Loss: 4.1295\n",
      "Epoch 396/500, Training Loss: 4.0717\n",
      "Epoch 397/500, Training Loss: 3.9949\n",
      "Epoch 398/500, Training Loss: 3.7884\n",
      "Epoch 399/500, Training Loss: 4.0894\n",
      "Epoch 400/500, Training Loss: 3.7600\n",
      "Epoch 401/500, Training Loss: 3.7841\n",
      "Epoch 402/500, Training Loss: 3.9152\n",
      "Epoch 403/500, Training Loss: 3.9948\n",
      "Epoch 404/500, Training Loss: 3.5475\n",
      "Epoch 405/500, Training Loss: 3.9927\n",
      "Epoch 406/500, Training Loss: 3.7320\n",
      "Epoch 407/500, Training Loss: 3.6493\n",
      "Epoch 408/500, Training Loss: 4.0860\n",
      "Epoch 409/500, Training Loss: 3.8488\n",
      "Epoch 410/500, Training Loss: 3.7898\n",
      "Epoch 411/500, Training Loss: 4.0334\n",
      "Epoch 412/500, Training Loss: 3.8338\n",
      "Epoch 413/500, Training Loss: 3.7563\n",
      "Epoch 414/500, Training Loss: 3.8243\n",
      "Epoch 415/500, Training Loss: 3.8935\n",
      "Epoch 416/500, Training Loss: 4.1555\n",
      "Epoch 417/500, Training Loss: 4.0711\n",
      "Epoch 418/500, Training Loss: 3.9193\n",
      "Epoch 419/500, Training Loss: 3.8176\n",
      "Epoch 420/500, Training Loss: 3.6841\n",
      "Epoch 421/500, Training Loss: 3.9711\n",
      "Epoch 422/500, Training Loss: 3.9757\n",
      "Epoch 423/500, Training Loss: 4.0400\n",
      "Epoch 424/500, Training Loss: 3.7641\n",
      "Epoch 425/500, Training Loss: 3.9956\n",
      "Epoch 426/500, Training Loss: 4.0507\n",
      "Epoch 427/500, Training Loss: 3.7536\n",
      "Epoch 428/500, Training Loss: 4.0449\n",
      "Epoch 429/500, Training Loss: 4.1892\n",
      "Epoch 430/500, Training Loss: 3.6467\n",
      "Epoch 431/500, Training Loss: 3.7964\n",
      "Epoch 432/500, Training Loss: 3.9828\n",
      "Epoch 433/500, Training Loss: 4.0817\n",
      "Epoch 434/500, Training Loss: 3.8326\n",
      "Epoch 435/500, Training Loss: 3.9424\n",
      "Epoch 436/500, Training Loss: 3.8575\n",
      "Epoch 437/500, Training Loss: 4.0118\n",
      "Epoch 438/500, Training Loss: 3.8323\n",
      "Epoch 439/500, Training Loss: 3.9641\n",
      "Epoch 440/500, Training Loss: 4.0733\n",
      "Epoch 441/500, Training Loss: 3.8304\n",
      "Epoch 442/500, Training Loss: 3.9803\n",
      "Epoch 443/500, Training Loss: 3.8586\n",
      "Epoch 444/500, Training Loss: 3.8543\n",
      "Epoch 445/500, Training Loss: 3.8725\n",
      "Epoch 446/500, Training Loss: 3.9392\n",
      "Epoch 447/500, Training Loss: 3.9163\n",
      "Epoch 448/500, Training Loss: 3.9248\n",
      "Epoch 449/500, Training Loss: 4.1250\n",
      "Epoch 450/500, Training Loss: 3.7961\n",
      "Epoch 451/500, Training Loss: 3.8551\n",
      "Epoch 452/500, Training Loss: 3.9817\n",
      "Epoch 453/500, Training Loss: 3.9608\n",
      "Epoch 454/500, Training Loss: 3.9027\n",
      "Epoch 455/500, Training Loss: 3.7309\n",
      "Epoch 456/500, Training Loss: 3.9249\n",
      "Epoch 457/500, Training Loss: 3.8917\n",
      "Epoch 458/500, Training Loss: 3.9861\n",
      "Epoch 459/500, Training Loss: 3.6021\n",
      "Epoch 460/500, Training Loss: 4.1444\n",
      "Epoch 461/500, Training Loss: 3.7130\n",
      "Epoch 462/500, Training Loss: 4.1752\n",
      "Epoch 463/500, Training Loss: 4.0392\n",
      "Epoch 464/500, Training Loss: 3.7843\n",
      "Epoch 465/500, Training Loss: 3.8455\n",
      "Epoch 466/500, Training Loss: 3.8852\n",
      "Epoch 467/500, Training Loss: 3.5526\n",
      "Epoch 468/500, Training Loss: 3.7044\n",
      "Epoch 469/500, Training Loss: 3.9076\n",
      "Epoch 470/500, Training Loss: 3.7642\n",
      "Epoch 471/500, Training Loss: 4.2193\n",
      "Epoch 472/500, Training Loss: 3.6449\n",
      "Epoch 473/500, Training Loss: 3.8892\n",
      "Epoch 474/500, Training Loss: 3.7436\n",
      "Epoch 475/500, Training Loss: 3.6140\n",
      "Epoch 476/500, Training Loss: 3.8443\n",
      "Epoch 477/500, Training Loss: 3.8156\n",
      "Epoch 478/500, Training Loss: 3.5303\n",
      "Epoch 479/500, Training Loss: 3.9136\n",
      "Epoch 480/500, Training Loss: 4.1453\n",
      "Epoch 481/500, Training Loss: 3.8558\n",
      "Epoch 482/500, Training Loss: 3.5990\n",
      "Epoch 483/500, Training Loss: 3.9876\n",
      "Epoch 484/500, Training Loss: 3.7242\n",
      "Epoch 485/500, Training Loss: 3.5743\n",
      "Epoch 486/500, Training Loss: 3.7879\n",
      "Epoch 487/500, Training Loss: 4.0328\n",
      "Epoch 488/500, Training Loss: 3.9426\n",
      "Epoch 489/500, Training Loss: 3.9694\n",
      "Epoch 490/500, Training Loss: 4.0226\n",
      "Epoch 491/500, Training Loss: 3.7914\n",
      "Epoch 492/500, Training Loss: 4.0782\n",
      "Epoch 493/500, Training Loss: 3.7761\n",
      "Epoch 494/500, Training Loss: 3.7195\n",
      "Epoch 495/500, Training Loss: 4.1194\n",
      "Epoch 496/500, Training Loss: 4.1291\n",
      "Epoch 497/500, Training Loss: 3.8346\n",
      "Epoch 498/500, Training Loss: 3.9785\n",
      "Epoch 499/500, Training Loss: 3.8314\n",
      "Epoch 500/500, Training Loss: 3.6219\n",
      "Validation Loss: 4.8996\n",
      "Model 2, Fold 1: Validation Loss = 4.8996\n",
      "Epoch 1/500, Training Loss: 135.2182\n",
      "Epoch 2/500, Training Loss: 30.3914\n",
      "Epoch 3/500, Training Loss: 10.5271\n",
      "Epoch 4/500, Training Loss: 6.4370\n",
      "Epoch 5/500, Training Loss: 5.2077\n",
      "Epoch 6/500, Training Loss: 3.2783\n",
      "Epoch 7/500, Training Loss: 3.6640\n",
      "Epoch 8/500, Training Loss: 3.4735\n",
      "Epoch 9/500, Training Loss: 3.4038\n",
      "Epoch 10/500, Training Loss: 3.2379\n",
      "Epoch 11/500, Training Loss: 3.1209\n",
      "Epoch 12/500, Training Loss: 3.0449\n",
      "Epoch 13/500, Training Loss: 3.0494\n",
      "Epoch 14/500, Training Loss: 2.7453\n",
      "Epoch 15/500, Training Loss: 2.9483\n",
      "Epoch 16/500, Training Loss: 3.0055\n",
      "Epoch 17/500, Training Loss: 3.0888\n",
      "Epoch 18/500, Training Loss: 3.0515\n",
      "Epoch 19/500, Training Loss: 2.7690\n",
      "Epoch 20/500, Training Loss: 2.8583\n",
      "Epoch 21/500, Training Loss: 3.2337\n",
      "Epoch 22/500, Training Loss: 2.7673\n",
      "Epoch 23/500, Training Loss: 2.8425\n",
      "Epoch 24/500, Training Loss: 2.7975\n",
      "Epoch 25/500, Training Loss: 3.0054\n",
      "Epoch 26/500, Training Loss: 3.1474\n",
      "Epoch 27/500, Training Loss: 2.9768\n",
      "Epoch 28/500, Training Loss: 3.0013\n",
      "Epoch 29/500, Training Loss: 2.7717\n",
      "Epoch 30/500, Training Loss: 2.7613\n",
      "Epoch 31/500, Training Loss: 2.7936\n",
      "Epoch 32/500, Training Loss: 2.9453\n",
      "Epoch 33/500, Training Loss: 2.6153\n",
      "Epoch 34/500, Training Loss: 2.8012\n",
      "Epoch 35/500, Training Loss: 2.9366\n",
      "Epoch 36/500, Training Loss: 2.6758\n",
      "Epoch 37/500, Training Loss: 2.9988\n",
      "Epoch 38/500, Training Loss: 2.8986\n",
      "Epoch 39/500, Training Loss: 2.9101\n",
      "Epoch 40/500, Training Loss: 2.7899\n",
      "Epoch 41/500, Training Loss: 2.8050\n",
      "Epoch 42/500, Training Loss: 2.7865\n",
      "Epoch 43/500, Training Loss: 2.8408\n",
      "Epoch 44/500, Training Loss: 2.8591\n",
      "Epoch 45/500, Training Loss: 2.7499\n",
      "Epoch 46/500, Training Loss: 2.7246\n",
      "Epoch 47/500, Training Loss: 2.8064\n",
      "Epoch 48/500, Training Loss: 2.9180\n",
      "Epoch 49/500, Training Loss: 2.6405\n",
      "Epoch 50/500, Training Loss: 2.8036\n",
      "Epoch 51/500, Training Loss: 2.7523\n",
      "Epoch 52/500, Training Loss: 2.6823\n",
      "Epoch 53/500, Training Loss: 2.8759\n",
      "Epoch 54/500, Training Loss: 2.7735\n",
      "Epoch 55/500, Training Loss: 2.8342\n",
      "Epoch 56/500, Training Loss: 2.9562\n",
      "Epoch 57/500, Training Loss: 2.7289\n",
      "Epoch 58/500, Training Loss: 2.8151\n",
      "Epoch 59/500, Training Loss: 2.7024\n",
      "Epoch 60/500, Training Loss: 2.7550\n",
      "Epoch 61/500, Training Loss: 2.9855\n",
      "Epoch 62/500, Training Loss: 2.8010\n",
      "Epoch 63/500, Training Loss: 2.6533\n",
      "Epoch 64/500, Training Loss: 2.5372\n",
      "Epoch 65/500, Training Loss: 2.7029\n",
      "Epoch 66/500, Training Loss: 2.7994\n",
      "Epoch 67/500, Training Loss: 2.6733\n",
      "Epoch 68/500, Training Loss: 2.6888\n",
      "Epoch 69/500, Training Loss: 2.7217\n",
      "Epoch 70/500, Training Loss: 2.8263\n",
      "Epoch 71/500, Training Loss: 2.7350\n",
      "Epoch 72/500, Training Loss: 2.8525\n",
      "Epoch 73/500, Training Loss: 2.6164\n",
      "Epoch 74/500, Training Loss: 2.7978\n",
      "Epoch 75/500, Training Loss: 2.6950\n",
      "Epoch 76/500, Training Loss: 2.9132\n",
      "Epoch 77/500, Training Loss: 2.9583\n",
      "Epoch 78/500, Training Loss: 2.8635\n",
      "Epoch 79/500, Training Loss: 2.9447\n",
      "Epoch 80/500, Training Loss: 2.9567\n",
      "Epoch 81/500, Training Loss: 2.8454\n",
      "Epoch 82/500, Training Loss: 2.7305\n",
      "Epoch 83/500, Training Loss: 2.5820\n",
      "Epoch 84/500, Training Loss: 2.8017\n",
      "Epoch 85/500, Training Loss: 2.8471\n",
      "Epoch 86/500, Training Loss: 2.7281\n",
      "Epoch 87/500, Training Loss: 2.7040\n",
      "Epoch 88/500, Training Loss: 2.7645\n",
      "Epoch 89/500, Training Loss: 2.5954\n",
      "Epoch 90/500, Training Loss: 2.8144\n",
      "Epoch 91/500, Training Loss: 2.7189\n",
      "Epoch 92/500, Training Loss: 2.4617\n",
      "Epoch 93/500, Training Loss: 2.7674\n",
      "Epoch 94/500, Training Loss: 2.7176\n",
      "Epoch 95/500, Training Loss: 2.5967\n",
      "Epoch 96/500, Training Loss: 2.6286\n",
      "Epoch 97/500, Training Loss: 2.7485\n",
      "Epoch 98/500, Training Loss: 2.6593\n",
      "Epoch 99/500, Training Loss: 2.7365\n",
      "Epoch 100/500, Training Loss: 2.7576\n",
      "Epoch 101/500, Training Loss: 2.5472\n",
      "Epoch 102/500, Training Loss: 2.6130\n",
      "Epoch 103/500, Training Loss: 2.6636\n",
      "Epoch 104/500, Training Loss: 2.5985\n",
      "Epoch 105/500, Training Loss: 2.7482\n",
      "Epoch 106/500, Training Loss: 2.7466\n",
      "Epoch 107/500, Training Loss: 2.9451\n",
      "Epoch 108/500, Training Loss: 2.6273\n",
      "Epoch 109/500, Training Loss: 2.7768\n",
      "Epoch 110/500, Training Loss: 2.7751\n",
      "Epoch 111/500, Training Loss: 2.9013\n",
      "Epoch 112/500, Training Loss: 2.7217\n",
      "Epoch 113/500, Training Loss: 2.6605\n",
      "Epoch 114/500, Training Loss: 2.7228\n",
      "Epoch 115/500, Training Loss: 2.5778\n",
      "Epoch 116/500, Training Loss: 2.7287\n",
      "Epoch 117/500, Training Loss: 2.8388\n",
      "Epoch 118/500, Training Loss: 2.7012\n",
      "Epoch 119/500, Training Loss: 2.7575\n",
      "Epoch 120/500, Training Loss: 2.8408\n",
      "Epoch 121/500, Training Loss: 2.6144\n",
      "Epoch 122/500, Training Loss: 2.6284\n",
      "Epoch 123/500, Training Loss: 2.9529\n",
      "Epoch 124/500, Training Loss: 2.9066\n",
      "Epoch 125/500, Training Loss: 2.7993\n",
      "Epoch 126/500, Training Loss: 2.6515\n",
      "Epoch 127/500, Training Loss: 2.6819\n",
      "Epoch 128/500, Training Loss: 2.8261\n",
      "Epoch 129/500, Training Loss: 2.6715\n",
      "Epoch 130/500, Training Loss: 2.6869\n",
      "Epoch 131/500, Training Loss: 2.6461\n",
      "Epoch 132/500, Training Loss: 2.7943\n",
      "Epoch 133/500, Training Loss: 2.7975\n",
      "Epoch 134/500, Training Loss: 2.5957\n",
      "Epoch 135/500, Training Loss: 2.8897\n",
      "Epoch 136/500, Training Loss: 2.5791\n",
      "Epoch 137/500, Training Loss: 2.8534\n",
      "Epoch 138/500, Training Loss: 2.5762\n",
      "Epoch 139/500, Training Loss: 2.7845\n",
      "Epoch 140/500, Training Loss: 2.7339\n",
      "Epoch 141/500, Training Loss: 2.6648\n",
      "Epoch 142/500, Training Loss: 2.6471\n",
      "Epoch 143/500, Training Loss: 2.5826\n",
      "Epoch 144/500, Training Loss: 2.7374\n",
      "Epoch 145/500, Training Loss: 2.4809\n",
      "Epoch 146/500, Training Loss: 2.8256\n",
      "Epoch 147/500, Training Loss: 2.6841\n",
      "Epoch 148/500, Training Loss: 2.5562\n",
      "Epoch 149/500, Training Loss: 2.6510\n",
      "Epoch 150/500, Training Loss: 2.8205\n",
      "Epoch 151/500, Training Loss: 2.5956\n",
      "Epoch 152/500, Training Loss: 2.6703\n",
      "Epoch 153/500, Training Loss: 2.6291\n",
      "Epoch 154/500, Training Loss: 2.6359\n",
      "Epoch 155/500, Training Loss: 2.6336\n",
      "Epoch 156/500, Training Loss: 2.6093\n",
      "Epoch 157/500, Training Loss: 2.7842\n",
      "Epoch 158/500, Training Loss: 2.8064\n",
      "Epoch 159/500, Training Loss: 2.6708\n",
      "Epoch 160/500, Training Loss: 2.7298\n",
      "Epoch 161/500, Training Loss: 2.7814\n",
      "Epoch 162/500, Training Loss: 2.6716\n",
      "Epoch 163/500, Training Loss: 2.6636\n",
      "Epoch 164/500, Training Loss: 2.4720\n",
      "Epoch 165/500, Training Loss: 2.7647\n",
      "Epoch 166/500, Training Loss: 2.5430\n",
      "Epoch 167/500, Training Loss: 2.6601\n",
      "Epoch 168/500, Training Loss: 2.7602\n",
      "Epoch 169/500, Training Loss: 2.7181\n",
      "Epoch 170/500, Training Loss: 2.6094\n",
      "Epoch 171/500, Training Loss: 2.6693\n",
      "Epoch 172/500, Training Loss: 2.8138\n",
      "Epoch 173/500, Training Loss: 2.7543\n",
      "Epoch 174/500, Training Loss: 2.7522\n",
      "Epoch 175/500, Training Loss: 2.7782\n",
      "Epoch 176/500, Training Loss: 2.6242\n",
      "Epoch 177/500, Training Loss: 2.8993\n",
      "Epoch 178/500, Training Loss: 2.7017\n",
      "Epoch 179/500, Training Loss: 2.7270\n",
      "Epoch 180/500, Training Loss: 2.5205\n",
      "Epoch 181/500, Training Loss: 2.6652\n",
      "Epoch 182/500, Training Loss: 2.7887\n",
      "Epoch 183/500, Training Loss: 2.6998\n",
      "Epoch 184/500, Training Loss: 2.7501\n",
      "Epoch 185/500, Training Loss: 2.6569\n",
      "Epoch 186/500, Training Loss: 2.6159\n",
      "Epoch 187/500, Training Loss: 2.7815\n",
      "Epoch 188/500, Training Loss: 2.6640\n",
      "Epoch 189/500, Training Loss: 2.7131\n",
      "Epoch 190/500, Training Loss: 2.6456\n",
      "Epoch 191/500, Training Loss: 2.5556\n",
      "Epoch 192/500, Training Loss: 2.5461\n",
      "Epoch 193/500, Training Loss: 2.5607\n",
      "Epoch 194/500, Training Loss: 2.8682\n",
      "Epoch 195/500, Training Loss: 2.7109\n",
      "Epoch 196/500, Training Loss: 2.6345\n",
      "Epoch 197/500, Training Loss: 2.6367\n",
      "Epoch 198/500, Training Loss: 2.7642\n",
      "Epoch 199/500, Training Loss: 2.5897\n",
      "Epoch 200/500, Training Loss: 2.6384\n",
      "Epoch 201/500, Training Loss: 2.5083\n",
      "Epoch 202/500, Training Loss: 2.5476\n",
      "Epoch 203/500, Training Loss: 2.7297\n",
      "Epoch 204/500, Training Loss: 2.8502\n",
      "Epoch 205/500, Training Loss: 2.6528\n",
      "Epoch 206/500, Training Loss: 2.7682\n",
      "Epoch 207/500, Training Loss: 2.7094\n",
      "Epoch 208/500, Training Loss: 2.7945\n",
      "Epoch 209/500, Training Loss: 2.5689\n",
      "Epoch 210/500, Training Loss: 2.6354\n",
      "Epoch 211/500, Training Loss: 2.5951\n",
      "Epoch 212/500, Training Loss: 2.6811\n",
      "Epoch 213/500, Training Loss: 2.7134\n",
      "Epoch 214/500, Training Loss: 2.7591\n",
      "Epoch 215/500, Training Loss: 2.8417\n",
      "Epoch 216/500, Training Loss: 2.8691\n",
      "Epoch 217/500, Training Loss: 2.7119\n",
      "Epoch 218/500, Training Loss: 2.8414\n",
      "Epoch 219/500, Training Loss: 2.7146\n",
      "Epoch 220/500, Training Loss: 2.5962\n",
      "Epoch 221/500, Training Loss: 2.7503\n",
      "Epoch 222/500, Training Loss: 2.7136\n",
      "Epoch 223/500, Training Loss: 2.7994\n",
      "Epoch 224/500, Training Loss: 2.8421\n",
      "Epoch 225/500, Training Loss: 2.9187\n",
      "Epoch 226/500, Training Loss: 2.8198\n",
      "Epoch 227/500, Training Loss: 2.9234\n",
      "Epoch 228/500, Training Loss: 2.7533\n",
      "Epoch 229/500, Training Loss: 2.7390\n",
      "Epoch 230/500, Training Loss: 2.9578\n",
      "Epoch 231/500, Training Loss: 2.9215\n",
      "Epoch 232/500, Training Loss: 2.6411\n",
      "Epoch 233/500, Training Loss: 2.8365\n",
      "Epoch 234/500, Training Loss: 2.8744\n",
      "Epoch 235/500, Training Loss: 2.6842\n",
      "Epoch 236/500, Training Loss: 2.7688\n",
      "Epoch 237/500, Training Loss: 2.7044\n",
      "Epoch 238/500, Training Loss: 2.6219\n",
      "Epoch 239/500, Training Loss: 2.6968\n",
      "Epoch 240/500, Training Loss: 2.4962\n",
      "Epoch 241/500, Training Loss: 2.6462\n",
      "Epoch 242/500, Training Loss: 2.7144\n",
      "Epoch 243/500, Training Loss: 2.6290\n",
      "Epoch 244/500, Training Loss: 2.6233\n",
      "Epoch 245/500, Training Loss: 2.5667\n",
      "Epoch 246/500, Training Loss: 2.5565\n",
      "Epoch 247/500, Training Loss: 2.5943\n",
      "Epoch 248/500, Training Loss: 2.7113\n",
      "Epoch 249/500, Training Loss: 2.7806\n",
      "Epoch 250/500, Training Loss: 2.6921\n",
      "Epoch 251/500, Training Loss: 2.6584\n",
      "Epoch 252/500, Training Loss: 2.6712\n",
      "Epoch 253/500, Training Loss: 2.5819\n",
      "Epoch 254/500, Training Loss: 2.4308\n",
      "Epoch 255/500, Training Loss: 2.5276\n",
      "Epoch 256/500, Training Loss: 2.6731\n",
      "Epoch 257/500, Training Loss: 2.7726\n",
      "Epoch 258/500, Training Loss: 2.5885\n",
      "Epoch 259/500, Training Loss: 2.7744\n",
      "Epoch 260/500, Training Loss: 2.8074\n",
      "Epoch 261/500, Training Loss: 2.6133\n",
      "Epoch 262/500, Training Loss: 2.8387\n",
      "Epoch 263/500, Training Loss: 2.6865\n",
      "Epoch 264/500, Training Loss: 2.6308\n",
      "Epoch 265/500, Training Loss: 2.7242\n",
      "Epoch 266/500, Training Loss: 2.8206\n",
      "Epoch 267/500, Training Loss: 2.6539\n",
      "Epoch 268/500, Training Loss: 2.6613\n",
      "Epoch 269/500, Training Loss: 2.5463\n",
      "Epoch 270/500, Training Loss: 2.7749\n",
      "Epoch 271/500, Training Loss: 2.7654\n",
      "Epoch 272/500, Training Loss: 2.5785\n",
      "Epoch 273/500, Training Loss: 2.7235\n",
      "Epoch 274/500, Training Loss: 2.6877\n",
      "Epoch 275/500, Training Loss: 2.7239\n",
      "Epoch 276/500, Training Loss: 2.8793\n",
      "Epoch 277/500, Training Loss: 2.5916\n",
      "Epoch 278/500, Training Loss: 2.6642\n",
      "Epoch 279/500, Training Loss: 2.7973\n",
      "Epoch 280/500, Training Loss: 2.6577\n",
      "Epoch 281/500, Training Loss: 2.6861\n",
      "Epoch 282/500, Training Loss: 2.6641\n",
      "Epoch 283/500, Training Loss: 2.6935\n",
      "Epoch 284/500, Training Loss: 2.7225\n",
      "Epoch 285/500, Training Loss: 2.7125\n",
      "Epoch 286/500, Training Loss: 2.6104\n",
      "Epoch 287/500, Training Loss: 2.7171\n",
      "Epoch 288/500, Training Loss: 2.6484\n",
      "Epoch 289/500, Training Loss: 2.6495\n",
      "Epoch 290/500, Training Loss: 2.8690\n",
      "Epoch 291/500, Training Loss: 2.5595\n",
      "Epoch 292/500, Training Loss: 2.6697\n",
      "Epoch 293/500, Training Loss: 2.7004\n",
      "Epoch 294/500, Training Loss: 2.7627\n",
      "Epoch 295/500, Training Loss: 2.6960\n",
      "Epoch 296/500, Training Loss: 2.6096\n",
      "Epoch 297/500, Training Loss: 2.7291\n",
      "Epoch 298/500, Training Loss: 2.6705\n",
      "Epoch 299/500, Training Loss: 2.5031\n",
      "Epoch 300/500, Training Loss: 2.6233\n",
      "Epoch 301/500, Training Loss: 2.7866\n",
      "Epoch 302/500, Training Loss: 2.6619\n",
      "Epoch 303/500, Training Loss: 2.6790\n",
      "Epoch 304/500, Training Loss: 2.7620\n",
      "Epoch 305/500, Training Loss: 2.6106\n",
      "Epoch 306/500, Training Loss: 2.8025\n",
      "Epoch 307/500, Training Loss: 2.5930\n",
      "Epoch 308/500, Training Loss: 2.5521\n",
      "Epoch 309/500, Training Loss: 2.6956\n",
      "Epoch 310/500, Training Loss: 2.8053\n",
      "Epoch 311/500, Training Loss: 2.6303\n",
      "Epoch 312/500, Training Loss: 2.6414\n",
      "Epoch 313/500, Training Loss: 2.6472\n",
      "Epoch 314/500, Training Loss: 2.6129\n",
      "Epoch 315/500, Training Loss: 2.5984\n",
      "Epoch 316/500, Training Loss: 2.6027\n",
      "Epoch 317/500, Training Loss: 2.5665\n",
      "Epoch 318/500, Training Loss: 2.6487\n",
      "Epoch 319/500, Training Loss: 2.6105\n",
      "Epoch 320/500, Training Loss: 2.7411\n",
      "Epoch 321/500, Training Loss: 2.7343\n",
      "Epoch 322/500, Training Loss: 2.6527\n",
      "Epoch 323/500, Training Loss: 2.6708\n",
      "Epoch 324/500, Training Loss: 2.6598\n",
      "Epoch 325/500, Training Loss: 2.6640\n",
      "Epoch 326/500, Training Loss: 2.6965\n",
      "Epoch 327/500, Training Loss: 2.6061\n",
      "Epoch 328/500, Training Loss: 2.6495\n",
      "Epoch 329/500, Training Loss: 2.7546\n",
      "Epoch 330/500, Training Loss: 2.7932\n",
      "Epoch 331/500, Training Loss: 2.7044\n",
      "Epoch 332/500, Training Loss: 2.6894\n",
      "Epoch 333/500, Training Loss: 2.7026\n",
      "Epoch 334/500, Training Loss: 2.6918\n",
      "Epoch 335/500, Training Loss: 2.5512\n",
      "Epoch 336/500, Training Loss: 2.6007\n",
      "Epoch 337/500, Training Loss: 2.6311\n",
      "Epoch 338/500, Training Loss: 2.6006\n",
      "Epoch 339/500, Training Loss: 2.7315\n",
      "Epoch 340/500, Training Loss: 2.6661\n",
      "Epoch 341/500, Training Loss: 2.5554\n",
      "Epoch 342/500, Training Loss: 2.6566\n",
      "Epoch 343/500, Training Loss: 2.5750\n",
      "Epoch 344/500, Training Loss: 2.6183\n",
      "Epoch 345/500, Training Loss: 2.7488\n",
      "Epoch 346/500, Training Loss: 2.5160\n",
      "Epoch 347/500, Training Loss: 2.5840\n",
      "Epoch 348/500, Training Loss: 2.8303\n",
      "Epoch 349/500, Training Loss: 2.6914\n",
      "Epoch 350/500, Training Loss: 2.6990\n",
      "Epoch 351/500, Training Loss: 2.6617\n",
      "Epoch 352/500, Training Loss: 2.7281\n",
      "Epoch 353/500, Training Loss: 2.5352\n",
      "Epoch 354/500, Training Loss: 2.5927\n",
      "Epoch 355/500, Training Loss: 2.6118\n",
      "Epoch 356/500, Training Loss: 2.7037\n",
      "Epoch 357/500, Training Loss: 2.6521\n",
      "Epoch 358/500, Training Loss: 2.6542\n",
      "Epoch 359/500, Training Loss: 2.8142\n",
      "Epoch 360/500, Training Loss: 2.5562\n",
      "Epoch 361/500, Training Loss: 2.7429\n",
      "Epoch 362/500, Training Loss: 2.6972\n",
      "Epoch 363/500, Training Loss: 2.7065\n",
      "Epoch 364/500, Training Loss: 2.7836\n",
      "Epoch 365/500, Training Loss: 2.7451\n",
      "Epoch 366/500, Training Loss: 2.7905\n",
      "Epoch 367/500, Training Loss: 2.5609\n",
      "Epoch 368/500, Training Loss: 2.6156\n",
      "Epoch 369/500, Training Loss: 2.5548\n",
      "Epoch 370/500, Training Loss: 2.8415\n",
      "Epoch 371/500, Training Loss: 2.7408\n",
      "Epoch 372/500, Training Loss: 2.7143\n",
      "Epoch 373/500, Training Loss: 2.5987\n",
      "Epoch 374/500, Training Loss: 2.6694\n",
      "Epoch 375/500, Training Loss: 2.6402\n",
      "Epoch 376/500, Training Loss: 2.6088\n",
      "Epoch 377/500, Training Loss: 2.7321\n",
      "Epoch 378/500, Training Loss: 2.5827\n",
      "Epoch 379/500, Training Loss: 2.5323\n",
      "Epoch 380/500, Training Loss: 2.6580\n",
      "Epoch 381/500, Training Loss: 2.6074\n",
      "Epoch 382/500, Training Loss: 2.6640\n",
      "Epoch 383/500, Training Loss: 2.6136\n",
      "Epoch 384/500, Training Loss: 2.6746\n",
      "Epoch 385/500, Training Loss: 2.6974\n",
      "Epoch 386/500, Training Loss: 2.7292\n",
      "Epoch 387/500, Training Loss: 2.6491\n",
      "Epoch 388/500, Training Loss: 2.7363\n",
      "Epoch 389/500, Training Loss: 2.7961\n",
      "Epoch 390/500, Training Loss: 2.8495\n",
      "Epoch 391/500, Training Loss: 2.5639\n",
      "Epoch 392/500, Training Loss: 2.5670\n",
      "Epoch 393/500, Training Loss: 2.7280\n",
      "Epoch 394/500, Training Loss: 2.7410\n",
      "Epoch 395/500, Training Loss: 2.6483\n",
      "Epoch 396/500, Training Loss: 2.7545\n",
      "Epoch 397/500, Training Loss: 2.8653\n",
      "Epoch 398/500, Training Loss: 2.7166\n",
      "Epoch 399/500, Training Loss: 2.7136\n",
      "Epoch 400/500, Training Loss: 2.8206\n",
      "Epoch 401/500, Training Loss: 2.7722\n",
      "Epoch 402/500, Training Loss: 2.6859\n",
      "Epoch 403/500, Training Loss: 2.6815\n",
      "Epoch 404/500, Training Loss: 2.7255\n",
      "Epoch 405/500, Training Loss: 2.4798\n",
      "Epoch 406/500, Training Loss: 2.6662\n",
      "Epoch 407/500, Training Loss: 2.6702\n",
      "Epoch 408/500, Training Loss: 2.6748\n",
      "Epoch 409/500, Training Loss: 2.5561\n",
      "Epoch 410/500, Training Loss: 2.5480\n",
      "Epoch 411/500, Training Loss: 2.6089\n",
      "Epoch 412/500, Training Loss: 2.5164\n",
      "Epoch 413/500, Training Loss: 2.7932\n",
      "Epoch 414/500, Training Loss: 2.7991\n",
      "Epoch 415/500, Training Loss: 2.6935\n",
      "Epoch 416/500, Training Loss: 2.5535\n",
      "Epoch 417/500, Training Loss: 2.6557\n",
      "Epoch 418/500, Training Loss: 2.6220\n",
      "Epoch 419/500, Training Loss: 2.8809\n",
      "Epoch 420/500, Training Loss: 2.6981\n",
      "Epoch 421/500, Training Loss: 2.6154\n",
      "Epoch 422/500, Training Loss: 2.6391\n",
      "Epoch 423/500, Training Loss: 2.6953\n",
      "Epoch 424/500, Training Loss: 2.6015\n",
      "Epoch 425/500, Training Loss: 2.6703\n",
      "Epoch 426/500, Training Loss: 2.9272\n",
      "Epoch 427/500, Training Loss: 2.7191\n",
      "Epoch 428/500, Training Loss: 2.8844\n",
      "Epoch 429/500, Training Loss: 2.8349\n",
      "Epoch 430/500, Training Loss: 2.6898\n",
      "Epoch 431/500, Training Loss: 2.4826\n",
      "Epoch 432/500, Training Loss: 2.6421\n",
      "Epoch 433/500, Training Loss: 2.5559\n",
      "Epoch 434/500, Training Loss: 2.4884\n",
      "Epoch 435/500, Training Loss: 2.4342\n",
      "Epoch 436/500, Training Loss: 2.7393\n",
      "Epoch 437/500, Training Loss: 2.5726\n",
      "Epoch 438/500, Training Loss: 2.5743\n",
      "Epoch 439/500, Training Loss: 2.6615\n",
      "Epoch 440/500, Training Loss: 2.7518\n",
      "Epoch 441/500, Training Loss: 2.5473\n",
      "Epoch 442/500, Training Loss: 2.5568\n",
      "Epoch 443/500, Training Loss: 2.6958\n",
      "Epoch 444/500, Training Loss: 2.7349\n",
      "Epoch 445/500, Training Loss: 2.7514\n",
      "Epoch 446/500, Training Loss: 2.5691\n",
      "Epoch 447/500, Training Loss: 2.6579\n",
      "Epoch 448/500, Training Loss: 2.6161\n",
      "Epoch 449/500, Training Loss: 2.5857\n",
      "Epoch 450/500, Training Loss: 2.6974\n",
      "Epoch 451/500, Training Loss: 2.7581\n",
      "Epoch 452/500, Training Loss: 2.7232\n",
      "Epoch 453/500, Training Loss: 2.6712\n",
      "Epoch 454/500, Training Loss: 2.6162\n",
      "Epoch 455/500, Training Loss: 2.6351\n",
      "Epoch 456/500, Training Loss: 2.6915\n",
      "Epoch 457/500, Training Loss: 2.7050\n",
      "Epoch 458/500, Training Loss: 2.5454\n",
      "Epoch 459/500, Training Loss: 2.8879\n",
      "Epoch 460/500, Training Loss: 2.7926\n",
      "Epoch 461/500, Training Loss: 2.6752\n",
      "Epoch 462/500, Training Loss: 2.6472\n",
      "Epoch 463/500, Training Loss: 2.5797\n",
      "Epoch 464/500, Training Loss: 2.5279\n",
      "Epoch 465/500, Training Loss: 2.6513\n",
      "Epoch 466/500, Training Loss: 2.7437\n",
      "Epoch 467/500, Training Loss: 2.4633\n",
      "Epoch 468/500, Training Loss: 2.5974\n",
      "Epoch 469/500, Training Loss: 2.5586\n",
      "Epoch 470/500, Training Loss: 2.6673\n",
      "Epoch 471/500, Training Loss: 2.5719\n",
      "Epoch 472/500, Training Loss: 2.8720\n",
      "Epoch 473/500, Training Loss: 2.9330\n",
      "Epoch 474/500, Training Loss: 2.8786\n",
      "Epoch 475/500, Training Loss: 2.9082\n",
      "Epoch 476/500, Training Loss: 2.8813\n",
      "Epoch 477/500, Training Loss: 2.7248\n",
      "Epoch 478/500, Training Loss: 2.7430\n",
      "Epoch 479/500, Training Loss: 2.5997\n",
      "Epoch 480/500, Training Loss: 2.6439\n",
      "Epoch 481/500, Training Loss: 2.5600\n",
      "Epoch 482/500, Training Loss: 2.5704\n",
      "Epoch 483/500, Training Loss: 2.8318\n",
      "Epoch 484/500, Training Loss: 2.5857\n",
      "Epoch 485/500, Training Loss: 2.7646\n",
      "Epoch 486/500, Training Loss: 2.8189\n",
      "Epoch 487/500, Training Loss: 2.6631\n",
      "Epoch 488/500, Training Loss: 2.5271\n",
      "Epoch 489/500, Training Loss: 2.5538\n",
      "Epoch 490/500, Training Loss: 2.6723\n",
      "Epoch 491/500, Training Loss: 2.6190\n",
      "Epoch 492/500, Training Loss: 2.5328\n",
      "Epoch 493/500, Training Loss: 2.6160\n",
      "Epoch 494/500, Training Loss: 2.6335\n",
      "Epoch 495/500, Training Loss: 2.5470\n",
      "Epoch 496/500, Training Loss: 2.5535\n",
      "Epoch 497/500, Training Loss: 2.5490\n",
      "Epoch 498/500, Training Loss: 2.5245\n",
      "Epoch 499/500, Training Loss: 2.7198\n",
      "Epoch 500/500, Training Loss: 2.6879\n",
      "Validation Loss: 2.8501\n",
      "Model 3, Fold 0: Validation Loss = 2.8501\n",
      "Epoch 1/500, Training Loss: 401.5478\n",
      "Epoch 2/500, Training Loss: 23.1380\n",
      "Epoch 3/500, Training Loss: 12.6251\n",
      "Epoch 4/500, Training Loss: 39.2147\n",
      "Epoch 5/500, Training Loss: 4.4582\n",
      "Epoch 6/500, Training Loss: 3.9544\n",
      "Epoch 7/500, Training Loss: 3.1343\n",
      "Epoch 8/500, Training Loss: 3.6679\n",
      "Epoch 9/500, Training Loss: 3.3223\n",
      "Epoch 10/500, Training Loss: 3.2053\n",
      "Epoch 11/500, Training Loss: 4.4549\n",
      "Epoch 12/500, Training Loss: 6.9511\n",
      "Epoch 13/500, Training Loss: 2.5934\n",
      "Epoch 14/500, Training Loss: 4.0892\n",
      "Epoch 15/500, Training Loss: 2.9690\n",
      "Epoch 16/500, Training Loss: 3.9620\n",
      "Epoch 17/500, Training Loss: 5.3743\n",
      "Epoch 18/500, Training Loss: 2.7157\n",
      "Epoch 19/500, Training Loss: 3.4501\n",
      "Epoch 20/500, Training Loss: 2.8272\n",
      "Epoch 21/500, Training Loss: 3.7066\n",
      "Epoch 22/500, Training Loss: 4.1637\n",
      "Epoch 23/500, Training Loss: 2.5496\n",
      "Epoch 24/500, Training Loss: 2.9840\n",
      "Epoch 25/500, Training Loss: 4.2840\n",
      "Epoch 26/500, Training Loss: 4.1368\n",
      "Epoch 27/500, Training Loss: 3.7185\n",
      "Epoch 28/500, Training Loss: 2.6097\n",
      "Epoch 29/500, Training Loss: 2.9991\n",
      "Epoch 30/500, Training Loss: 3.4269\n",
      "Epoch 31/500, Training Loss: 3.9494\n",
      "Epoch 32/500, Training Loss: 3.0846\n",
      "Epoch 33/500, Training Loss: 3.0374\n",
      "Epoch 34/500, Training Loss: 3.0315\n",
      "Epoch 35/500, Training Loss: 3.0924\n",
      "Epoch 36/500, Training Loss: 3.2340\n",
      "Epoch 37/500, Training Loss: 3.6594\n",
      "Epoch 38/500, Training Loss: 2.5060\n",
      "Epoch 39/500, Training Loss: 2.5130\n",
      "Epoch 40/500, Training Loss: 3.1111\n",
      "Epoch 41/500, Training Loss: 3.3235\n",
      "Epoch 42/500, Training Loss: 2.6849\n",
      "Epoch 43/500, Training Loss: 2.8940\n",
      "Epoch 44/500, Training Loss: 2.5080\n",
      "Epoch 45/500, Training Loss: 3.2568\n",
      "Epoch 46/500, Training Loss: 2.6964\n",
      "Epoch 47/500, Training Loss: 2.5210\n",
      "Epoch 48/500, Training Loss: 3.0556\n",
      "Epoch 49/500, Training Loss: 2.8034\n",
      "Epoch 50/500, Training Loss: 2.6063\n",
      "Epoch 51/500, Training Loss: 2.6626\n",
      "Epoch 52/500, Training Loss: 2.9606\n",
      "Epoch 53/500, Training Loss: 2.5312\n",
      "Epoch 54/500, Training Loss: 2.7132\n",
      "Epoch 55/500, Training Loss: 3.0136\n",
      "Epoch 56/500, Training Loss: 2.8826\n",
      "Epoch 57/500, Training Loss: 2.6103\n",
      "Epoch 58/500, Training Loss: 2.9112\n",
      "Epoch 59/500, Training Loss: 2.9161\n",
      "Epoch 60/500, Training Loss: 3.1071\n",
      "Epoch 61/500, Training Loss: 2.7379\n",
      "Epoch 62/500, Training Loss: 2.6746\n",
      "Epoch 63/500, Training Loss: 2.4557\n",
      "Epoch 64/500, Training Loss: 2.7095\n",
      "Epoch 65/500, Training Loss: 2.5736\n",
      "Epoch 66/500, Training Loss: 2.5134\n",
      "Epoch 67/500, Training Loss: 2.5569\n",
      "Epoch 68/500, Training Loss: 2.8416\n",
      "Epoch 69/500, Training Loss: 2.9045\n",
      "Epoch 70/500, Training Loss: 2.4881\n",
      "Epoch 71/500, Training Loss: 2.9926\n",
      "Epoch 72/500, Training Loss: 3.1380\n",
      "Epoch 73/500, Training Loss: 2.6809\n",
      "Epoch 74/500, Training Loss: 3.0002\n",
      "Epoch 75/500, Training Loss: 3.2669\n",
      "Epoch 76/500, Training Loss: 2.7608\n",
      "Epoch 77/500, Training Loss: 3.2307\n",
      "Epoch 78/500, Training Loss: 3.2918\n",
      "Epoch 79/500, Training Loss: 2.8693\n",
      "Epoch 80/500, Training Loss: 3.0259\n",
      "Epoch 81/500, Training Loss: 2.9091\n",
      "Epoch 82/500, Training Loss: 3.0791\n",
      "Epoch 83/500, Training Loss: 2.9685\n",
      "Epoch 84/500, Training Loss: 3.1215\n",
      "Epoch 85/500, Training Loss: 2.9279\n",
      "Epoch 86/500, Training Loss: 3.0509\n",
      "Epoch 87/500, Training Loss: 2.5860\n",
      "Epoch 88/500, Training Loss: 2.8371\n",
      "Epoch 89/500, Training Loss: 2.7896\n",
      "Epoch 90/500, Training Loss: 2.6285\n",
      "Epoch 91/500, Training Loss: 2.8439\n",
      "Epoch 92/500, Training Loss: 2.7192\n",
      "Epoch 93/500, Training Loss: 2.6721\n",
      "Epoch 94/500, Training Loss: 2.9117\n",
      "Epoch 95/500, Training Loss: 2.7592\n",
      "Epoch 96/500, Training Loss: 2.5207\n",
      "Epoch 97/500, Training Loss: 2.9270\n",
      "Epoch 98/500, Training Loss: 2.6095\n",
      "Epoch 99/500, Training Loss: 2.8201\n",
      "Epoch 100/500, Training Loss: 2.9168\n",
      "Epoch 101/500, Training Loss: 2.5958\n",
      "Epoch 102/500, Training Loss: 2.9388\n",
      "Epoch 103/500, Training Loss: 2.5900\n",
      "Epoch 104/500, Training Loss: 3.0760\n",
      "Epoch 105/500, Training Loss: 2.9172\n",
      "Epoch 106/500, Training Loss: 2.6136\n",
      "Epoch 107/500, Training Loss: 2.8532\n",
      "Epoch 108/500, Training Loss: 2.6503\n",
      "Epoch 109/500, Training Loss: 2.5276\n",
      "Epoch 110/500, Training Loss: 2.7452\n",
      "Epoch 111/500, Training Loss: 2.7861\n",
      "Epoch 112/500, Training Loss: 2.9524\n",
      "Epoch 113/500, Training Loss: 2.6608\n",
      "Epoch 114/500, Training Loss: 2.5890\n",
      "Epoch 115/500, Training Loss: 2.4538\n",
      "Epoch 116/500, Training Loss: 2.5733\n",
      "Epoch 117/500, Training Loss: 3.0053\n",
      "Epoch 118/500, Training Loss: 2.8923\n",
      "Epoch 119/500, Training Loss: 2.5513\n",
      "Epoch 120/500, Training Loss: 2.7876\n",
      "Epoch 121/500, Training Loss: 2.4713\n",
      "Epoch 122/500, Training Loss: 2.5758\n",
      "Epoch 123/500, Training Loss: 2.6037\n",
      "Epoch 124/500, Training Loss: 2.6579\n",
      "Epoch 125/500, Training Loss: 2.5936\n",
      "Epoch 126/500, Training Loss: 2.6794\n",
      "Epoch 127/500, Training Loss: 2.8310\n",
      "Epoch 128/500, Training Loss: 2.6696\n",
      "Epoch 129/500, Training Loss: 2.7008\n",
      "Epoch 130/500, Training Loss: 2.6930\n",
      "Epoch 131/500, Training Loss: 2.5841\n",
      "Epoch 132/500, Training Loss: 2.5604\n",
      "Epoch 133/500, Training Loss: 3.0446\n",
      "Epoch 134/500, Training Loss: 2.8449\n",
      "Epoch 135/500, Training Loss: 2.8874\n",
      "Epoch 136/500, Training Loss: 2.4484\n",
      "Epoch 137/500, Training Loss: 2.7076\n",
      "Epoch 138/500, Training Loss: 2.9493\n",
      "Epoch 139/500, Training Loss: 2.7634\n",
      "Epoch 140/500, Training Loss: 2.8634\n",
      "Epoch 141/500, Training Loss: 3.0417\n",
      "Epoch 142/500, Training Loss: 2.6045\n",
      "Epoch 143/500, Training Loss: 2.7297\n",
      "Epoch 144/500, Training Loss: 2.6709\n",
      "Epoch 145/500, Training Loss: 2.8779\n",
      "Epoch 146/500, Training Loss: 2.9482\n",
      "Epoch 147/500, Training Loss: 2.4729\n",
      "Epoch 148/500, Training Loss: 2.5243\n",
      "Epoch 149/500, Training Loss: 2.5421\n",
      "Epoch 150/500, Training Loss: 2.7471\n",
      "Epoch 151/500, Training Loss: 2.4604\n",
      "Epoch 152/500, Training Loss: 2.7984\n",
      "Epoch 153/500, Training Loss: 2.7621\n",
      "Epoch 154/500, Training Loss: 2.5906\n",
      "Epoch 155/500, Training Loss: 2.6746\n",
      "Epoch 156/500, Training Loss: 2.9451\n",
      "Epoch 157/500, Training Loss: 2.4844\n",
      "Epoch 158/500, Training Loss: 2.7501\n",
      "Epoch 159/500, Training Loss: 2.6863\n",
      "Epoch 160/500, Training Loss: 2.6591\n",
      "Epoch 161/500, Training Loss: 2.5576\n",
      "Epoch 162/500, Training Loss: 2.6907\n",
      "Epoch 163/500, Training Loss: 2.5516\n",
      "Epoch 164/500, Training Loss: 2.5863\n",
      "Epoch 165/500, Training Loss: 2.5995\n",
      "Epoch 166/500, Training Loss: 2.6800\n",
      "Epoch 167/500, Training Loss: 2.8903\n",
      "Epoch 168/500, Training Loss: 2.7406\n",
      "Epoch 169/500, Training Loss: 2.6382\n",
      "Epoch 170/500, Training Loss: 2.6429\n",
      "Epoch 171/500, Training Loss: 2.7318\n",
      "Epoch 172/500, Training Loss: 2.4282\n",
      "Epoch 173/500, Training Loss: 2.8366\n",
      "Epoch 174/500, Training Loss: 2.8750\n",
      "Epoch 175/500, Training Loss: 2.6834\n",
      "Epoch 176/500, Training Loss: 2.5591\n",
      "Epoch 177/500, Training Loss: 2.5936\n",
      "Epoch 178/500, Training Loss: 2.4546\n",
      "Epoch 179/500, Training Loss: 2.4354\n",
      "Epoch 180/500, Training Loss: 2.6863\n",
      "Epoch 181/500, Training Loss: 2.9427\n",
      "Epoch 182/500, Training Loss: 2.7093\n",
      "Epoch 183/500, Training Loss: 2.5033\n",
      "Epoch 184/500, Training Loss: 2.6592\n",
      "Epoch 185/500, Training Loss: 2.6397\n",
      "Epoch 186/500, Training Loss: 2.8524\n",
      "Epoch 187/500, Training Loss: 2.7272\n",
      "Epoch 188/500, Training Loss: 2.5522\n",
      "Epoch 189/500, Training Loss: 2.5534\n",
      "Epoch 190/500, Training Loss: 2.8296\n",
      "Epoch 191/500, Training Loss: 2.6900\n",
      "Epoch 192/500, Training Loss: 2.7860\n",
      "Epoch 193/500, Training Loss: 2.7551\n",
      "Epoch 194/500, Training Loss: 2.8402\n",
      "Epoch 195/500, Training Loss: 2.7262\n",
      "Epoch 196/500, Training Loss: 2.6966\n",
      "Epoch 197/500, Training Loss: 2.8135\n",
      "Epoch 198/500, Training Loss: 2.8312\n",
      "Epoch 199/500, Training Loss: 2.8667\n",
      "Epoch 200/500, Training Loss: 2.7611\n",
      "Epoch 201/500, Training Loss: 2.7381\n",
      "Epoch 202/500, Training Loss: 2.6369\n",
      "Epoch 203/500, Training Loss: 2.8228\n",
      "Epoch 204/500, Training Loss: 2.7414\n",
      "Epoch 205/500, Training Loss: 2.5162\n",
      "Epoch 206/500, Training Loss: 2.5816\n",
      "Epoch 207/500, Training Loss: 2.6672\n",
      "Epoch 208/500, Training Loss: 2.7722\n",
      "Epoch 209/500, Training Loss: 2.8563\n",
      "Epoch 210/500, Training Loss: 2.8224\n",
      "Epoch 211/500, Training Loss: 2.7704\n",
      "Epoch 212/500, Training Loss: 2.6484\n",
      "Epoch 213/500, Training Loss: 2.4903\n",
      "Epoch 214/500, Training Loss: 2.6540\n",
      "Epoch 215/500, Training Loss: 2.6711\n",
      "Epoch 216/500, Training Loss: 2.7418\n",
      "Epoch 217/500, Training Loss: 2.5947\n",
      "Epoch 218/500, Training Loss: 2.7044\n",
      "Epoch 219/500, Training Loss: 2.6227\n",
      "Epoch 220/500, Training Loss: 2.6460\n",
      "Epoch 221/500, Training Loss: 2.5011\n",
      "Epoch 222/500, Training Loss: 2.7339\n",
      "Epoch 223/500, Training Loss: 2.6879\n",
      "Epoch 224/500, Training Loss: 2.6097\n",
      "Epoch 225/500, Training Loss: 2.5900\n",
      "Epoch 226/500, Training Loss: 2.4584\n",
      "Epoch 227/500, Training Loss: 2.6987\n",
      "Epoch 228/500, Training Loss: 2.6888\n",
      "Epoch 229/500, Training Loss: 2.6938\n",
      "Epoch 230/500, Training Loss: 2.5341\n",
      "Epoch 231/500, Training Loss: 2.6062\n",
      "Epoch 232/500, Training Loss: 2.5610\n",
      "Epoch 233/500, Training Loss: 2.5276\n",
      "Epoch 234/500, Training Loss: 2.7448\n",
      "Epoch 235/500, Training Loss: 2.4591\n",
      "Epoch 236/500, Training Loss: 2.4918\n",
      "Epoch 237/500, Training Loss: 2.8141\n",
      "Epoch 238/500, Training Loss: 2.4800\n",
      "Epoch 239/500, Training Loss: 2.6072\n",
      "Epoch 240/500, Training Loss: 2.7359\n",
      "Epoch 241/500, Training Loss: 2.7814\n",
      "Epoch 242/500, Training Loss: 2.5201\n",
      "Epoch 243/500, Training Loss: 2.6410\n",
      "Epoch 244/500, Training Loss: 2.7689\n",
      "Epoch 245/500, Training Loss: 2.6925\n",
      "Epoch 246/500, Training Loss: 2.4631\n",
      "Epoch 247/500, Training Loss: 2.6646\n",
      "Epoch 248/500, Training Loss: 2.6263\n",
      "Epoch 249/500, Training Loss: 2.5148\n",
      "Epoch 250/500, Training Loss: 2.5884\n",
      "Epoch 251/500, Training Loss: 2.7267\n",
      "Epoch 252/500, Training Loss: 2.6272\n",
      "Epoch 253/500, Training Loss: 2.7354\n",
      "Epoch 254/500, Training Loss: 2.5918\n",
      "Epoch 255/500, Training Loss: 2.6649\n",
      "Epoch 256/500, Training Loss: 2.7110\n",
      "Epoch 257/500, Training Loss: 2.4568\n",
      "Epoch 258/500, Training Loss: 2.6283\n",
      "Epoch 259/500, Training Loss: 2.7555\n",
      "Epoch 260/500, Training Loss: 2.6601\n",
      "Epoch 261/500, Training Loss: 2.8379\n",
      "Epoch 262/500, Training Loss: 2.5754\n",
      "Epoch 263/500, Training Loss: 2.6651\n",
      "Epoch 264/500, Training Loss: 2.7290\n",
      "Epoch 265/500, Training Loss: 2.5383\n",
      "Epoch 266/500, Training Loss: 2.7361\n",
      "Epoch 267/500, Training Loss: 2.5927\n",
      "Epoch 268/500, Training Loss: 2.5737\n",
      "Epoch 269/500, Training Loss: 2.5732\n",
      "Epoch 270/500, Training Loss: 2.7262\n",
      "Epoch 271/500, Training Loss: 2.6119\n",
      "Epoch 272/500, Training Loss: 2.4158\n",
      "Epoch 273/500, Training Loss: 2.6490\n",
      "Epoch 274/500, Training Loss: 2.7035\n",
      "Epoch 275/500, Training Loss: 2.7477\n",
      "Epoch 276/500, Training Loss: 2.7988\n",
      "Epoch 277/500, Training Loss: 2.5948\n",
      "Epoch 278/500, Training Loss: 2.6261\n",
      "Epoch 279/500, Training Loss: 2.5040\n",
      "Epoch 280/500, Training Loss: 2.6333\n",
      "Epoch 281/500, Training Loss: 2.6172\n",
      "Epoch 282/500, Training Loss: 2.5244\n",
      "Epoch 283/500, Training Loss: 2.8136\n",
      "Epoch 284/500, Training Loss: 2.4598\n",
      "Epoch 285/500, Training Loss: 2.5086\n",
      "Epoch 286/500, Training Loss: 2.7562\n",
      "Epoch 287/500, Training Loss: 2.7588\n",
      "Epoch 288/500, Training Loss: 2.6914\n",
      "Epoch 289/500, Training Loss: 2.4839\n",
      "Epoch 290/500, Training Loss: 2.4125\n",
      "Epoch 291/500, Training Loss: 2.7073\n",
      "Epoch 292/500, Training Loss: 2.6286\n",
      "Epoch 293/500, Training Loss: 2.5837\n",
      "Epoch 294/500, Training Loss: 2.5782\n",
      "Epoch 295/500, Training Loss: 2.6693\n",
      "Epoch 296/500, Training Loss: 2.4176\n",
      "Epoch 297/500, Training Loss: 2.7258\n",
      "Epoch 298/500, Training Loss: 2.6540\n",
      "Epoch 299/500, Training Loss: 2.9494\n",
      "Epoch 300/500, Training Loss: 2.4907\n",
      "Epoch 301/500, Training Loss: 2.6512\n",
      "Epoch 302/500, Training Loss: 2.4767\n",
      "Epoch 303/500, Training Loss: 2.5874\n",
      "Epoch 304/500, Training Loss: 2.6555\n",
      "Epoch 305/500, Training Loss: 2.5710\n",
      "Epoch 306/500, Training Loss: 2.6444\n",
      "Epoch 307/500, Training Loss: 2.4423\n",
      "Epoch 308/500, Training Loss: 2.4657\n",
      "Epoch 309/500, Training Loss: 2.6209\n",
      "Epoch 310/500, Training Loss: 2.7859\n",
      "Epoch 311/500, Training Loss: 2.6343\n",
      "Epoch 312/500, Training Loss: 2.6219\n",
      "Epoch 313/500, Training Loss: 2.7347\n",
      "Epoch 314/500, Training Loss: 2.5580\n",
      "Epoch 315/500, Training Loss: 2.7781\n",
      "Epoch 316/500, Training Loss: 2.4525\n",
      "Epoch 317/500, Training Loss: 2.7307\n",
      "Epoch 318/500, Training Loss: 2.5053\n",
      "Epoch 319/500, Training Loss: 2.5266\n",
      "Epoch 320/500, Training Loss: 2.4723\n",
      "Epoch 321/500, Training Loss: 2.6186\n",
      "Epoch 322/500, Training Loss: 2.6533\n",
      "Epoch 323/500, Training Loss: 2.5654\n",
      "Epoch 324/500, Training Loss: 2.6857\n",
      "Epoch 325/500, Training Loss: 2.6813\n",
      "Epoch 326/500, Training Loss: 2.5751\n",
      "Epoch 327/500, Training Loss: 2.5749\n",
      "Epoch 328/500, Training Loss: 2.5078\n",
      "Epoch 329/500, Training Loss: 2.6872\n",
      "Epoch 330/500, Training Loss: 2.5736\n",
      "Epoch 331/500, Training Loss: 2.5768\n",
      "Epoch 332/500, Training Loss: 2.6535\n",
      "Epoch 333/500, Training Loss: 2.5435\n",
      "Epoch 334/500, Training Loss: 2.5165\n",
      "Epoch 335/500, Training Loss: 2.6729\n",
      "Epoch 336/500, Training Loss: 2.6751\n",
      "Epoch 337/500, Training Loss: 2.5700\n",
      "Epoch 338/500, Training Loss: 2.6175\n",
      "Epoch 339/500, Training Loss: 2.4487\n",
      "Epoch 340/500, Training Loss: 2.5042\n",
      "Epoch 341/500, Training Loss: 2.5770\n",
      "Epoch 342/500, Training Loss: 2.4591\n",
      "Epoch 343/500, Training Loss: 2.7193\n",
      "Epoch 344/500, Training Loss: 2.7430\n",
      "Epoch 345/500, Training Loss: 2.4862\n",
      "Epoch 346/500, Training Loss: 2.4899\n",
      "Epoch 347/500, Training Loss: 2.6222\n",
      "Epoch 348/500, Training Loss: 2.6901\n",
      "Epoch 349/500, Training Loss: 2.6747\n",
      "Epoch 350/500, Training Loss: 2.7664\n",
      "Epoch 351/500, Training Loss: 2.7080\n",
      "Epoch 352/500, Training Loss: 2.6713\n",
      "Epoch 353/500, Training Loss: 2.5944\n",
      "Epoch 354/500, Training Loss: 2.6512\n",
      "Epoch 355/500, Training Loss: 2.7170\n",
      "Epoch 356/500, Training Loss: 2.6521\n",
      "Epoch 357/500, Training Loss: 2.5974\n",
      "Epoch 358/500, Training Loss: 2.6382\n",
      "Epoch 359/500, Training Loss: 2.6557\n",
      "Epoch 360/500, Training Loss: 2.8140\n",
      "Epoch 361/500, Training Loss: 2.7534\n",
      "Epoch 362/500, Training Loss: 2.4789\n",
      "Epoch 363/500, Training Loss: 2.6727\n",
      "Epoch 364/500, Training Loss: 2.6503\n",
      "Epoch 365/500, Training Loss: 2.7323\n",
      "Epoch 366/500, Training Loss: 2.6498\n",
      "Epoch 367/500, Training Loss: 2.6001\n",
      "Epoch 368/500, Training Loss: 2.6893\n",
      "Epoch 369/500, Training Loss: 2.5765\n",
      "Epoch 370/500, Training Loss: 2.5799\n",
      "Epoch 371/500, Training Loss: 2.5387\n",
      "Epoch 372/500, Training Loss: 2.7391\n",
      "Epoch 373/500, Training Loss: 2.5721\n",
      "Epoch 374/500, Training Loss: 2.6005\n",
      "Epoch 375/500, Training Loss: 2.5863\n",
      "Epoch 376/500, Training Loss: 2.6835\n",
      "Epoch 377/500, Training Loss: 2.5146\n",
      "Epoch 378/500, Training Loss: 2.6587\n",
      "Epoch 379/500, Training Loss: 2.5903\n",
      "Epoch 380/500, Training Loss: 2.3912\n",
      "Epoch 381/500, Training Loss: 2.6472\n",
      "Epoch 382/500, Training Loss: 2.6491\n",
      "Epoch 383/500, Training Loss: 2.5977\n",
      "Epoch 384/500, Training Loss: 2.5559\n",
      "Epoch 385/500, Training Loss: 2.3452\n",
      "Epoch 386/500, Training Loss: 2.5385\n",
      "Epoch 387/500, Training Loss: 2.4570\n",
      "Epoch 388/500, Training Loss: 2.6011\n",
      "Epoch 389/500, Training Loss: 2.6300\n",
      "Epoch 390/500, Training Loss: 2.4949\n",
      "Epoch 391/500, Training Loss: 2.6738\n",
      "Epoch 392/500, Training Loss: 2.5438\n",
      "Epoch 393/500, Training Loss: 2.4755\n",
      "Epoch 394/500, Training Loss: 2.3972\n",
      "Epoch 395/500, Training Loss: 2.5824\n",
      "Epoch 396/500, Training Loss: 2.5689\n",
      "Epoch 397/500, Training Loss: 2.3648\n",
      "Epoch 398/500, Training Loss: 2.6851\n",
      "Epoch 399/500, Training Loss: 2.5904\n",
      "Epoch 400/500, Training Loss: 2.6779\n",
      "Epoch 401/500, Training Loss: 2.5907\n",
      "Epoch 402/500, Training Loss: 2.4688\n",
      "Epoch 403/500, Training Loss: 2.4834\n",
      "Epoch 404/500, Training Loss: 2.4981\n",
      "Epoch 405/500, Training Loss: 2.5675\n",
      "Epoch 406/500, Training Loss: 2.6603\n",
      "Epoch 407/500, Training Loss: 2.6140\n",
      "Epoch 408/500, Training Loss: 2.5625\n",
      "Epoch 409/500, Training Loss: 2.6423\n",
      "Epoch 410/500, Training Loss: 2.6370\n",
      "Epoch 411/500, Training Loss: 2.5680\n",
      "Epoch 412/500, Training Loss: 2.4698\n",
      "Epoch 413/500, Training Loss: 2.6358\n",
      "Epoch 414/500, Training Loss: 2.7415\n",
      "Epoch 415/500, Training Loss: 2.6565\n",
      "Epoch 416/500, Training Loss: 2.6424\n",
      "Epoch 417/500, Training Loss: 2.4354\n",
      "Epoch 418/500, Training Loss: 2.6611\n",
      "Epoch 419/500, Training Loss: 2.6419\n",
      "Epoch 420/500, Training Loss: 2.4967\n",
      "Epoch 421/500, Training Loss: 2.6013\n",
      "Epoch 422/500, Training Loss: 2.5698\n",
      "Epoch 423/500, Training Loss: 2.5494\n",
      "Epoch 424/500, Training Loss: 2.6734\n",
      "Epoch 425/500, Training Loss: 2.4750\n",
      "Epoch 426/500, Training Loss: 2.6136\n",
      "Epoch 427/500, Training Loss: 2.6711\n",
      "Epoch 428/500, Training Loss: 2.6187\n",
      "Epoch 429/500, Training Loss: 2.5716\n",
      "Epoch 430/500, Training Loss: 2.5210\n",
      "Epoch 431/500, Training Loss: 2.4041\n",
      "Epoch 432/500, Training Loss: 2.5712\n",
      "Epoch 433/500, Training Loss: 2.4498\n",
      "Epoch 434/500, Training Loss: 2.5442\n",
      "Epoch 435/500, Training Loss: 2.6707\n",
      "Epoch 436/500, Training Loss: 2.6505\n",
      "Epoch 437/500, Training Loss: 2.5796\n",
      "Epoch 438/500, Training Loss: 2.4768\n",
      "Epoch 439/500, Training Loss: 2.6001\n",
      "Epoch 440/500, Training Loss: 2.5924\n",
      "Epoch 441/500, Training Loss: 2.4757\n",
      "Epoch 442/500, Training Loss: 2.5411\n",
      "Epoch 443/500, Training Loss: 2.7106\n",
      "Epoch 444/500, Training Loss: 2.7712\n",
      "Epoch 445/500, Training Loss: 2.5102\n",
      "Epoch 446/500, Training Loss: 2.4672\n",
      "Epoch 447/500, Training Loss: 2.5152\n",
      "Epoch 448/500, Training Loss: 2.6979\n",
      "Epoch 449/500, Training Loss: 2.5235\n",
      "Epoch 450/500, Training Loss: 2.6259\n",
      "Epoch 451/500, Training Loss: 2.7201\n",
      "Epoch 452/500, Training Loss: 2.6238\n",
      "Epoch 453/500, Training Loss: 2.6668\n",
      "Epoch 454/500, Training Loss: 2.6767\n",
      "Epoch 455/500, Training Loss: 2.5737\n",
      "Epoch 456/500, Training Loss: 2.4788\n",
      "Epoch 457/500, Training Loss: 2.6445\n",
      "Epoch 458/500, Training Loss: 2.5735\n",
      "Epoch 459/500, Training Loss: 2.6873\n",
      "Epoch 460/500, Training Loss: 2.5969\n",
      "Epoch 461/500, Training Loss: 2.7175\n",
      "Epoch 462/500, Training Loss: 2.5848\n",
      "Epoch 463/500, Training Loss: 2.5307\n",
      "Epoch 464/500, Training Loss: 2.5072\n",
      "Epoch 465/500, Training Loss: 2.6377\n",
      "Epoch 466/500, Training Loss: 2.6374\n",
      "Epoch 467/500, Training Loss: 2.5034\n",
      "Epoch 468/500, Training Loss: 2.6641\n",
      "Epoch 469/500, Training Loss: 2.5829\n",
      "Epoch 470/500, Training Loss: 2.6828\n",
      "Epoch 471/500, Training Loss: 2.6801\n",
      "Epoch 472/500, Training Loss: 2.7470\n",
      "Epoch 473/500, Training Loss: 2.8087\n",
      "Epoch 474/500, Training Loss: 2.5384\n",
      "Epoch 475/500, Training Loss: 2.6306\n",
      "Epoch 476/500, Training Loss: 2.3998\n",
      "Epoch 477/500, Training Loss: 2.6969\n",
      "Epoch 478/500, Training Loss: 2.4151\n",
      "Epoch 479/500, Training Loss: 2.6025\n",
      "Epoch 480/500, Training Loss: 2.7070\n",
      "Epoch 481/500, Training Loss: 2.7902\n",
      "Epoch 482/500, Training Loss: 2.6911\n",
      "Epoch 483/500, Training Loss: 2.4614\n",
      "Epoch 484/500, Training Loss: 2.6417\n",
      "Epoch 485/500, Training Loss: 2.5425\n",
      "Epoch 486/500, Training Loss: 2.6260\n",
      "Epoch 487/500, Training Loss: 2.5571\n",
      "Epoch 488/500, Training Loss: 2.6559\n",
      "Epoch 489/500, Training Loss: 2.6412\n",
      "Epoch 490/500, Training Loss: 2.6137\n",
      "Epoch 491/500, Training Loss: 2.5020\n",
      "Epoch 492/500, Training Loss: 2.4066\n",
      "Epoch 493/500, Training Loss: 2.5789\n",
      "Epoch 494/500, Training Loss: 2.5752\n",
      "Epoch 495/500, Training Loss: 2.6230\n",
      "Epoch 496/500, Training Loss: 2.4204\n",
      "Epoch 497/500, Training Loss: 2.5218\n",
      "Epoch 498/500, Training Loss: 2.6987\n",
      "Epoch 499/500, Training Loss: 2.4487\n",
      "Epoch 500/500, Training Loss: 2.5653\n",
      "Validation Loss: 2.7323\n",
      "Model 3, Fold 1: Validation Loss = 2.7323\n",
      "Best overall model index: 3 with average validation loss 2.7912\n",
      "Retraining best model on full dataset...\n",
      "Epoch 1/500, Training Loss: 36.7029\n",
      "Epoch 2/500, Training Loss: 4.7897\n",
      "Epoch 3/500, Training Loss: 4.0147\n",
      "Epoch 4/500, Training Loss: 4.0214\n",
      "Epoch 5/500, Training Loss: 2.8291\n",
      "Epoch 6/500, Training Loss: 2.8571\n",
      "Epoch 7/500, Training Loss: 3.2380\n",
      "Epoch 8/500, Training Loss: 2.8739\n",
      "Epoch 9/500, Training Loss: 3.2005\n",
      "Epoch 10/500, Training Loss: 3.1550\n",
      "Epoch 11/500, Training Loss: 2.9344\n",
      "Epoch 12/500, Training Loss: 2.8965\n",
      "Epoch 13/500, Training Loss: 2.7841\n",
      "Epoch 14/500, Training Loss: 2.6684\n",
      "Epoch 15/500, Training Loss: 2.6830\n",
      "Epoch 16/500, Training Loss: 2.9697\n",
      "Epoch 17/500, Training Loss: 2.8085\n",
      "Epoch 18/500, Training Loss: 3.0526\n",
      "Epoch 19/500, Training Loss: 2.7957\n",
      "Epoch 20/500, Training Loss: 2.7352\n",
      "Epoch 21/500, Training Loss: 2.6852\n",
      "Epoch 22/500, Training Loss: 2.6310\n",
      "Epoch 23/500, Training Loss: 2.6158\n",
      "Epoch 24/500, Training Loss: 2.6360\n",
      "Epoch 25/500, Training Loss: 2.7639\n",
      "Epoch 26/500, Training Loss: 2.6292\n",
      "Epoch 27/500, Training Loss: 2.7222\n",
      "Epoch 28/500, Training Loss: 2.5915\n",
      "Epoch 29/500, Training Loss: 2.7587\n",
      "Epoch 30/500, Training Loss: 2.7752\n",
      "Epoch 31/500, Training Loss: 2.8578\n",
      "Epoch 32/500, Training Loss: 2.7233\n",
      "Epoch 33/500, Training Loss: 2.6640\n",
      "Epoch 34/500, Training Loss: 2.6580\n",
      "Epoch 35/500, Training Loss: 2.7119\n",
      "Epoch 36/500, Training Loss: 2.8014\n",
      "Epoch 37/500, Training Loss: 2.5844\n",
      "Epoch 38/500, Training Loss: 2.7395\n",
      "Epoch 39/500, Training Loss: 2.6365\n",
      "Epoch 40/500, Training Loss: 2.7236\n",
      "Epoch 41/500, Training Loss: 2.6432\n",
      "Epoch 42/500, Training Loss: 2.6655\n",
      "Epoch 43/500, Training Loss: 2.5864\n",
      "Epoch 44/500, Training Loss: 2.5610\n",
      "Epoch 45/500, Training Loss: 2.7388\n",
      "Epoch 46/500, Training Loss: 2.6151\n",
      "Epoch 47/500, Training Loss: 2.6482\n",
      "Epoch 48/500, Training Loss: 2.5343\n",
      "Epoch 49/500, Training Loss: 2.6189\n",
      "Epoch 50/500, Training Loss: 2.6813\n",
      "Epoch 51/500, Training Loss: 2.5502\n",
      "Epoch 52/500, Training Loss: 2.7434\n",
      "Epoch 53/500, Training Loss: 2.6629\n",
      "Epoch 54/500, Training Loss: 2.7490\n",
      "Epoch 55/500, Training Loss: 2.6516\n",
      "Epoch 56/500, Training Loss: 2.6805\n",
      "Epoch 57/500, Training Loss: 2.7131\n",
      "Epoch 58/500, Training Loss: 2.6947\n",
      "Epoch 59/500, Training Loss: 2.5944\n",
      "Epoch 60/500, Training Loss: 2.6295\n",
      "Epoch 61/500, Training Loss: 2.6341\n",
      "Epoch 62/500, Training Loss: 2.5396\n",
      "Epoch 63/500, Training Loss: 2.7758\n",
      "Epoch 64/500, Training Loss: 2.6975\n",
      "Epoch 65/500, Training Loss: 2.7207\n",
      "Epoch 66/500, Training Loss: 2.6004\n",
      "Epoch 67/500, Training Loss: 2.7885\n",
      "Epoch 68/500, Training Loss: 2.7000\n",
      "Epoch 69/500, Training Loss: 2.6779\n",
      "Epoch 70/500, Training Loss: 2.6655\n",
      "Epoch 71/500, Training Loss: 2.6111\n",
      "Epoch 72/500, Training Loss: 2.6716\n",
      "Epoch 73/500, Training Loss: 2.6298\n",
      "Epoch 74/500, Training Loss: 2.7649\n",
      "Epoch 75/500, Training Loss: 2.5951\n",
      "Epoch 76/500, Training Loss: 2.5399\n",
      "Epoch 77/500, Training Loss: 2.7051\n",
      "Epoch 78/500, Training Loss: 2.7235\n",
      "Epoch 79/500, Training Loss: 2.5804\n",
      "Epoch 80/500, Training Loss: 2.7614\n",
      "Epoch 81/500, Training Loss: 2.7232\n",
      "Epoch 82/500, Training Loss: 2.5359\n",
      "Epoch 83/500, Training Loss: 2.5912\n",
      "Epoch 84/500, Training Loss: 2.5889\n",
      "Epoch 85/500, Training Loss: 2.5958\n",
      "Epoch 86/500, Training Loss: 2.5664\n",
      "Epoch 87/500, Training Loss: 2.5140\n",
      "Epoch 88/500, Training Loss: 2.6247\n",
      "Epoch 89/500, Training Loss: 2.6519\n",
      "Epoch 90/500, Training Loss: 2.6496\n",
      "Epoch 91/500, Training Loss: 2.6664\n",
      "Epoch 92/500, Training Loss: 2.6083\n",
      "Epoch 93/500, Training Loss: 2.6321\n",
      "Epoch 94/500, Training Loss: 2.6195\n",
      "Epoch 95/500, Training Loss: 2.7356\n",
      "Epoch 96/500, Training Loss: 2.6428\n",
      "Epoch 97/500, Training Loss: 2.6899\n",
      "Epoch 98/500, Training Loss: 2.7555\n",
      "Epoch 99/500, Training Loss: 2.7343\n",
      "Epoch 100/500, Training Loss: 2.6075\n",
      "Epoch 101/500, Training Loss: 2.5672\n",
      "Epoch 102/500, Training Loss: 2.5620\n",
      "Epoch 103/500, Training Loss: 2.6473\n",
      "Epoch 104/500, Training Loss: 2.5534\n",
      "Epoch 105/500, Training Loss: 2.6412\n",
      "Epoch 106/500, Training Loss: 2.7203\n",
      "Epoch 107/500, Training Loss: 2.5466\n",
      "Epoch 108/500, Training Loss: 2.7069\n",
      "Epoch 109/500, Training Loss: 2.7402\n",
      "Epoch 110/500, Training Loss: 2.6626\n",
      "Epoch 111/500, Training Loss: 2.5448\n",
      "Epoch 112/500, Training Loss: 2.6712\n",
      "Epoch 113/500, Training Loss: 2.7667\n",
      "Epoch 114/500, Training Loss: 2.7932\n",
      "Epoch 115/500, Training Loss: 2.6211\n",
      "Epoch 116/500, Training Loss: 2.6563\n",
      "Epoch 117/500, Training Loss: 2.5669\n",
      "Epoch 118/500, Training Loss: 2.7176\n",
      "Epoch 119/500, Training Loss: 2.6414\n",
      "Epoch 120/500, Training Loss: 2.5533\n",
      "Epoch 121/500, Training Loss: 2.7540\n",
      "Epoch 122/500, Training Loss: 2.6644\n",
      "Epoch 123/500, Training Loss: 2.5648\n",
      "Epoch 124/500, Training Loss: 2.5743\n",
      "Epoch 125/500, Training Loss: 2.6324\n",
      "Epoch 126/500, Training Loss: 2.6485\n",
      "Epoch 127/500, Training Loss: 2.7395\n",
      "Epoch 128/500, Training Loss: 2.7510\n",
      "Epoch 129/500, Training Loss: 2.5676\n",
      "Epoch 130/500, Training Loss: 2.5672\n",
      "Epoch 131/500, Training Loss: 2.6501\n",
      "Epoch 132/500, Training Loss: 2.6505\n",
      "Epoch 133/500, Training Loss: 2.5907\n",
      "Epoch 134/500, Training Loss: 2.6524\n",
      "Epoch 135/500, Training Loss: 2.5042\n",
      "Epoch 136/500, Training Loss: 2.6962\n",
      "Epoch 137/500, Training Loss: 2.6585\n",
      "Epoch 138/500, Training Loss: 2.6178\n",
      "Epoch 139/500, Training Loss: 2.6403\n",
      "Epoch 140/500, Training Loss: 2.6822\n",
      "Epoch 141/500, Training Loss: 2.7331\n",
      "Epoch 142/500, Training Loss: 2.5760\n",
      "Epoch 143/500, Training Loss: 2.6999\n",
      "Epoch 144/500, Training Loss: 2.6772\n",
      "Epoch 145/500, Training Loss: 2.6424\n",
      "Epoch 146/500, Training Loss: 2.6405\n",
      "Epoch 147/500, Training Loss: 2.6289\n",
      "Epoch 148/500, Training Loss: 2.6509\n",
      "Epoch 149/500, Training Loss: 2.6616\n",
      "Epoch 150/500, Training Loss: 2.5812\n",
      "Epoch 151/500, Training Loss: 2.6803\n",
      "Epoch 152/500, Training Loss: 2.5996\n",
      "Epoch 153/500, Training Loss: 2.7252\n",
      "Epoch 154/500, Training Loss: 2.6164\n",
      "Epoch 155/500, Training Loss: 2.6178\n",
      "Epoch 156/500, Training Loss: 2.5289\n",
      "Epoch 157/500, Training Loss: 2.4044\n",
      "Epoch 158/500, Training Loss: 2.5878\n",
      "Epoch 159/500, Training Loss: 2.5572\n",
      "Epoch 160/500, Training Loss: 2.6541\n",
      "Epoch 161/500, Training Loss: 2.6732\n",
      "Epoch 162/500, Training Loss: 2.7049\n",
      "Epoch 163/500, Training Loss: 2.7974\n",
      "Epoch 164/500, Training Loss: 2.5909\n",
      "Epoch 165/500, Training Loss: 2.6421\n",
      "Epoch 166/500, Training Loss: 2.6660\n",
      "Epoch 167/500, Training Loss: 2.6719\n",
      "Epoch 168/500, Training Loss: 2.6447\n",
      "Epoch 169/500, Training Loss: 2.6757\n",
      "Epoch 170/500, Training Loss: 2.6442\n",
      "Epoch 171/500, Training Loss: 2.5944\n",
      "Epoch 172/500, Training Loss: 2.6342\n",
      "Epoch 173/500, Training Loss: 2.5890\n",
      "Epoch 174/500, Training Loss: 2.6530\n",
      "Epoch 175/500, Training Loss: 2.6216\n",
      "Epoch 176/500, Training Loss: 2.6467\n",
      "Epoch 177/500, Training Loss: 2.7200\n",
      "Epoch 178/500, Training Loss: 2.5954\n",
      "Epoch 179/500, Training Loss: 2.6491\n",
      "Epoch 180/500, Training Loss: 2.6222\n",
      "Epoch 181/500, Training Loss: 2.6461\n",
      "Epoch 182/500, Training Loss: 2.6231\n",
      "Epoch 183/500, Training Loss: 2.6360\n",
      "Epoch 184/500, Training Loss: 2.5823\n",
      "Epoch 185/500, Training Loss: 2.5785\n",
      "Epoch 186/500, Training Loss: 2.6568\n",
      "Epoch 187/500, Training Loss: 2.6175\n",
      "Epoch 188/500, Training Loss: 2.6666\n",
      "Epoch 189/500, Training Loss: 2.5936\n",
      "Epoch 190/500, Training Loss: 2.7055\n",
      "Epoch 191/500, Training Loss: 2.6228\n",
      "Epoch 192/500, Training Loss: 2.5709\n",
      "Epoch 193/500, Training Loss: 2.5984\n",
      "Epoch 194/500, Training Loss: 2.7177\n",
      "Epoch 195/500, Training Loss: 2.6074\n",
      "Epoch 196/500, Training Loss: 2.6755\n",
      "Epoch 197/500, Training Loss: 2.5302\n",
      "Epoch 198/500, Training Loss: 2.5828\n",
      "Epoch 199/500, Training Loss: 2.6191\n",
      "Epoch 200/500, Training Loss: 2.5653\n",
      "Epoch 201/500, Training Loss: 2.5359\n",
      "Epoch 202/500, Training Loss: 2.7065\n",
      "Epoch 203/500, Training Loss: 2.5641\n",
      "Epoch 204/500, Training Loss: 2.6650\n",
      "Epoch 205/500, Training Loss: 2.6239\n",
      "Epoch 206/500, Training Loss: 2.7056\n",
      "Epoch 207/500, Training Loss: 2.6294\n",
      "Epoch 208/500, Training Loss: 2.7093\n",
      "Epoch 209/500, Training Loss: 2.5743\n",
      "Epoch 210/500, Training Loss: 2.6532\n",
      "Epoch 211/500, Training Loss: 2.7374\n",
      "Epoch 212/500, Training Loss: 2.5700\n",
      "Epoch 213/500, Training Loss: 2.5658\n",
      "Epoch 214/500, Training Loss: 2.5102\n",
      "Epoch 215/500, Training Loss: 2.5377\n",
      "Epoch 216/500, Training Loss: 2.5148\n",
      "Epoch 217/500, Training Loss: 2.6284\n",
      "Epoch 218/500, Training Loss: 2.5672\n",
      "Epoch 219/500, Training Loss: 2.6698\n",
      "Epoch 220/500, Training Loss: 2.5726\n",
      "Epoch 221/500, Training Loss: 2.5908\n",
      "Epoch 222/500, Training Loss: 2.7203\n",
      "Epoch 223/500, Training Loss: 2.6532\n",
      "Epoch 224/500, Training Loss: 2.5513\n",
      "Epoch 225/500, Training Loss: 2.6541\n",
      "Epoch 226/500, Training Loss: 2.6299\n",
      "Epoch 227/500, Training Loss: 2.6351\n",
      "Epoch 228/500, Training Loss: 2.5817\n",
      "Epoch 229/500, Training Loss: 2.6432\n",
      "Epoch 230/500, Training Loss: 2.6949\n",
      "Epoch 231/500, Training Loss: 2.5220\n",
      "Epoch 232/500, Training Loss: 2.6368\n",
      "Epoch 233/500, Training Loss: 2.5664\n",
      "Epoch 234/500, Training Loss: 2.6128\n",
      "Epoch 235/500, Training Loss: 2.6048\n",
      "Epoch 236/500, Training Loss: 2.6849\n",
      "Epoch 237/500, Training Loss: 2.6331\n",
      "Epoch 238/500, Training Loss: 2.5947\n",
      "Epoch 239/500, Training Loss: 2.5730\n",
      "Epoch 240/500, Training Loss: 2.5448\n",
      "Epoch 241/500, Training Loss: 2.6140\n",
      "Epoch 242/500, Training Loss: 2.5895\n",
      "Epoch 243/500, Training Loss: 2.5941\n",
      "Epoch 244/500, Training Loss: 2.5924\n",
      "Epoch 245/500, Training Loss: 2.5014\n",
      "Epoch 246/500, Training Loss: 2.6102\n",
      "Epoch 247/500, Training Loss: 2.7160\n",
      "Epoch 248/500, Training Loss: 2.6395\n",
      "Epoch 249/500, Training Loss: 2.6511\n",
      "Epoch 250/500, Training Loss: 2.5402\n",
      "Epoch 251/500, Training Loss: 2.6250\n",
      "Epoch 252/500, Training Loss: 2.5139\n",
      "Epoch 253/500, Training Loss: 2.6200\n",
      "Epoch 254/500, Training Loss: 2.6213\n",
      "Epoch 255/500, Training Loss: 2.7283\n",
      "Epoch 256/500, Training Loss: 2.6978\n",
      "Epoch 257/500, Training Loss: 2.6575\n",
      "Epoch 258/500, Training Loss: 2.6264\n",
      "Epoch 259/500, Training Loss: 2.6365\n",
      "Epoch 260/500, Training Loss: 2.4793\n",
      "Epoch 261/500, Training Loss: 2.5343\n",
      "Epoch 262/500, Training Loss: 2.5670\n",
      "Epoch 263/500, Training Loss: 2.6128\n",
      "Epoch 264/500, Training Loss: 2.6005\n",
      "Epoch 265/500, Training Loss: 2.6080\n",
      "Epoch 266/500, Training Loss: 2.5478\n",
      "Epoch 267/500, Training Loss: 2.6894\n",
      "Epoch 268/500, Training Loss: 2.6476\n",
      "Epoch 269/500, Training Loss: 2.7449\n",
      "Epoch 270/500, Training Loss: 2.7420\n",
      "Epoch 271/500, Training Loss: 2.6009\n",
      "Epoch 272/500, Training Loss: 2.6623\n",
      "Epoch 273/500, Training Loss: 2.7500\n",
      "Epoch 274/500, Training Loss: 2.6546\n",
      "Epoch 275/500, Training Loss: 2.6854\n",
      "Epoch 276/500, Training Loss: 2.6133\n",
      "Epoch 277/500, Training Loss: 2.6388\n",
      "Epoch 278/500, Training Loss: 2.6112\n",
      "Epoch 279/500, Training Loss: 2.5927\n",
      "Epoch 280/500, Training Loss: 2.6326\n",
      "Epoch 281/500, Training Loss: 2.5972\n",
      "Epoch 282/500, Training Loss: 2.7145\n",
      "Epoch 283/500, Training Loss: 2.7063\n",
      "Epoch 284/500, Training Loss: 2.6380\n",
      "Epoch 285/500, Training Loss: 2.6060\n",
      "Epoch 286/500, Training Loss: 2.6776\n",
      "Epoch 287/500, Training Loss: 2.5825\n",
      "Epoch 288/500, Training Loss: 2.5273\n",
      "Epoch 289/500, Training Loss: 2.5588\n",
      "Epoch 290/500, Training Loss: 2.5934\n",
      "Epoch 291/500, Training Loss: 2.5831\n",
      "Epoch 292/500, Training Loss: 2.6936\n",
      "Epoch 293/500, Training Loss: 2.6416\n",
      "Epoch 294/500, Training Loss: 2.6308\n",
      "Epoch 295/500, Training Loss: 2.5227\n",
      "Epoch 296/500, Training Loss: 2.5770\n",
      "Epoch 297/500, Training Loss: 2.4922\n",
      "Epoch 298/500, Training Loss: 2.5829\n",
      "Epoch 299/500, Training Loss: 2.7051\n",
      "Epoch 300/500, Training Loss: 2.6960\n",
      "Epoch 301/500, Training Loss: 2.5788\n",
      "Epoch 302/500, Training Loss: 2.5985\n",
      "Epoch 303/500, Training Loss: 2.5887\n",
      "Epoch 304/500, Training Loss: 2.5282\n",
      "Epoch 305/500, Training Loss: 2.6113\n",
      "Epoch 306/500, Training Loss: 2.6218\n",
      "Epoch 307/500, Training Loss: 2.5501\n",
      "Epoch 308/500, Training Loss: 2.6137\n",
      "Epoch 309/500, Training Loss: 2.6168\n",
      "Epoch 310/500, Training Loss: 2.6971\n",
      "Epoch 311/500, Training Loss: 2.6167\n",
      "Epoch 312/500, Training Loss: 2.6098\n",
      "Epoch 313/500, Training Loss: 2.7560\n",
      "Epoch 314/500, Training Loss: 2.6040\n",
      "Epoch 315/500, Training Loss: 2.6453\n",
      "Epoch 316/500, Training Loss: 2.6914\n",
      "Epoch 317/500, Training Loss: 2.5754\n",
      "Epoch 318/500, Training Loss: 2.6768\n",
      "Epoch 319/500, Training Loss: 2.6167\n",
      "Epoch 320/500, Training Loss: 2.5858\n",
      "Epoch 321/500, Training Loss: 2.5502\n",
      "Epoch 322/500, Training Loss: 2.6394\n",
      "Epoch 323/500, Training Loss: 2.6537\n",
      "Epoch 324/500, Training Loss: 2.6830\n",
      "Epoch 325/500, Training Loss: 2.6025\n",
      "Epoch 326/500, Training Loss: 2.6864\n",
      "Epoch 327/500, Training Loss: 2.6242\n",
      "Epoch 328/500, Training Loss: 2.6322\n",
      "Epoch 329/500, Training Loss: 2.6465\n",
      "Epoch 330/500, Training Loss: 2.6093\n",
      "Epoch 331/500, Training Loss: 2.5873\n",
      "Epoch 332/500, Training Loss: 2.7508\n",
      "Epoch 333/500, Training Loss: 2.5831\n",
      "Epoch 334/500, Training Loss: 2.6588\n",
      "Epoch 335/500, Training Loss: 2.6674\n",
      "Epoch 336/500, Training Loss: 2.6868\n",
      "Epoch 337/500, Training Loss: 2.5884\n",
      "Epoch 338/500, Training Loss: 2.6302\n",
      "Epoch 339/500, Training Loss: 2.6237\n",
      "Epoch 340/500, Training Loss: 2.6006\n",
      "Epoch 341/500, Training Loss: 2.7069\n",
      "Epoch 342/500, Training Loss: 2.5602\n",
      "Epoch 343/500, Training Loss: 2.5963\n",
      "Epoch 344/500, Training Loss: 2.6068\n",
      "Epoch 345/500, Training Loss: 2.7112\n",
      "Epoch 346/500, Training Loss: 2.6425\n",
      "Epoch 347/500, Training Loss: 2.7102\n",
      "Epoch 348/500, Training Loss: 2.7145\n",
      "Epoch 349/500, Training Loss: 2.7652\n",
      "Epoch 350/500, Training Loss: 2.6467\n",
      "Epoch 351/500, Training Loss: 2.5975\n",
      "Epoch 352/500, Training Loss: 2.7241\n",
      "Epoch 353/500, Training Loss: 2.6156\n",
      "Epoch 354/500, Training Loss: 2.6894\n",
      "Epoch 355/500, Training Loss: 2.6516\n",
      "Epoch 356/500, Training Loss: 2.5808\n",
      "Epoch 357/500, Training Loss: 2.6115\n",
      "Epoch 358/500, Training Loss: 2.6877\n",
      "Epoch 359/500, Training Loss: 2.6341\n",
      "Epoch 360/500, Training Loss: 2.5608\n",
      "Epoch 361/500, Training Loss: 2.6365\n",
      "Epoch 362/500, Training Loss: 2.6344\n",
      "Epoch 363/500, Training Loss: 2.5847\n",
      "Epoch 364/500, Training Loss: 2.5537\n",
      "Epoch 365/500, Training Loss: 2.5878\n",
      "Epoch 366/500, Training Loss: 2.6206\n",
      "Epoch 367/500, Training Loss: 2.4853\n",
      "Epoch 368/500, Training Loss: 2.5767\n",
      "Epoch 369/500, Training Loss: 2.6053\n",
      "Epoch 370/500, Training Loss: 2.5825\n",
      "Epoch 371/500, Training Loss: 2.5409\n",
      "Epoch 372/500, Training Loss: 2.6565\n",
      "Epoch 373/500, Training Loss: 2.6246\n",
      "Epoch 374/500, Training Loss: 2.5340\n",
      "Epoch 375/500, Training Loss: 2.4509\n",
      "Epoch 376/500, Training Loss: 2.6036\n",
      "Epoch 377/500, Training Loss: 2.6613\n",
      "Epoch 378/500, Training Loss: 2.6493\n",
      "Epoch 379/500, Training Loss: 2.5489\n",
      "Epoch 380/500, Training Loss: 2.6749\n",
      "Epoch 381/500, Training Loss: 2.5732\n",
      "Epoch 382/500, Training Loss: 2.4962\n",
      "Epoch 383/500, Training Loss: 2.6404\n",
      "Epoch 384/500, Training Loss: 2.5798\n",
      "Epoch 385/500, Training Loss: 2.5759\n",
      "Epoch 386/500, Training Loss: 2.5014\n",
      "Epoch 387/500, Training Loss: 2.6407\n",
      "Epoch 388/500, Training Loss: 2.5473\n",
      "Epoch 389/500, Training Loss: 2.6247\n",
      "Epoch 390/500, Training Loss: 2.5727\n",
      "Epoch 391/500, Training Loss: 2.6313\n",
      "Epoch 392/500, Training Loss: 2.7118\n",
      "Epoch 393/500, Training Loss: 2.6718\n",
      "Epoch 394/500, Training Loss: 2.6306\n",
      "Epoch 395/500, Training Loss: 2.6803\n",
      "Epoch 396/500, Training Loss: 2.6150\n",
      "Epoch 397/500, Training Loss: 2.5414\n",
      "Epoch 398/500, Training Loss: 2.5528\n",
      "Epoch 399/500, Training Loss: 2.6023\n",
      "Epoch 400/500, Training Loss: 2.5407\n",
      "Epoch 401/500, Training Loss: 2.5876\n",
      "Epoch 402/500, Training Loss: 2.7291\n",
      "Epoch 403/500, Training Loss: 2.5613\n",
      "Epoch 404/500, Training Loss: 2.5932\n",
      "Epoch 405/500, Training Loss: 2.5749\n",
      "Epoch 406/500, Training Loss: 2.6612\n",
      "Epoch 407/500, Training Loss: 2.6168\n",
      "Epoch 408/500, Training Loss: 2.5888\n",
      "Epoch 409/500, Training Loss: 2.5683\n",
      "Epoch 410/500, Training Loss: 2.6457\n",
      "Epoch 411/500, Training Loss: 2.6725\n",
      "Epoch 412/500, Training Loss: 2.5221\n",
      "Epoch 413/500, Training Loss: 2.6527\n",
      "Epoch 414/500, Training Loss: 2.7169\n",
      "Epoch 415/500, Training Loss: 2.6343\n",
      "Epoch 416/500, Training Loss: 2.5321\n",
      "Epoch 417/500, Training Loss: 2.6904\n",
      "Epoch 418/500, Training Loss: 2.5420\n",
      "Epoch 419/500, Training Loss: 2.6039\n",
      "Epoch 420/500, Training Loss: 2.6264\n",
      "Epoch 421/500, Training Loss: 2.7249\n",
      "Epoch 422/500, Training Loss: 2.5307\n",
      "Epoch 423/500, Training Loss: 2.5601\n",
      "Epoch 424/500, Training Loss: 2.5651\n",
      "Epoch 425/500, Training Loss: 2.5789\n",
      "Epoch 426/500, Training Loss: 2.6726\n",
      "Epoch 427/500, Training Loss: 2.6504\n",
      "Epoch 428/500, Training Loss: 2.5895\n",
      "Epoch 429/500, Training Loss: 2.7222\n",
      "Epoch 430/500, Training Loss: 2.5445\n",
      "Epoch 431/500, Training Loss: 2.5200\n",
      "Epoch 432/500, Training Loss: 2.5367\n",
      "Epoch 433/500, Training Loss: 2.5381\n",
      "Epoch 434/500, Training Loss: 2.5998\n",
      "Epoch 435/500, Training Loss: 2.6385\n",
      "Epoch 436/500, Training Loss: 2.4721\n",
      "Epoch 437/500, Training Loss: 2.6559\n",
      "Epoch 438/500, Training Loss: 2.6841\n",
      "Epoch 439/500, Training Loss: 2.6439\n",
      "Epoch 440/500, Training Loss: 2.6182\n",
      "Epoch 441/500, Training Loss: 2.5480\n",
      "Epoch 442/500, Training Loss: 2.6957\n",
      "Epoch 443/500, Training Loss: 2.7100\n",
      "Epoch 444/500, Training Loss: 2.6359\n",
      "Epoch 445/500, Training Loss: 2.5155\n",
      "Epoch 446/500, Training Loss: 2.5130\n",
      "Epoch 447/500, Training Loss: 2.5841\n",
      "Epoch 448/500, Training Loss: 2.6670\n",
      "Epoch 449/500, Training Loss: 2.5640\n",
      "Epoch 450/500, Training Loss: 2.5207\n",
      "Epoch 451/500, Training Loss: 2.6484\n",
      "Epoch 452/500, Training Loss: 2.6044\n",
      "Epoch 453/500, Training Loss: 2.5640\n",
      "Epoch 454/500, Training Loss: 2.6132\n",
      "Epoch 455/500, Training Loss: 2.6225\n",
      "Epoch 456/500, Training Loss: 2.6235\n",
      "Epoch 457/500, Training Loss: 2.6755\n",
      "Epoch 458/500, Training Loss: 2.6271\n",
      "Epoch 459/500, Training Loss: 2.5383\n",
      "Epoch 460/500, Training Loss: 2.6386\n",
      "Epoch 461/500, Training Loss: 2.5437\n",
      "Epoch 462/500, Training Loss: 2.5763\n",
      "Epoch 463/500, Training Loss: 2.5615\n",
      "Epoch 464/500, Training Loss: 2.4507\n",
      "Epoch 465/500, Training Loss: 2.6342\n",
      "Epoch 466/500, Training Loss: 2.4799\n",
      "Epoch 467/500, Training Loss: 2.5595\n",
      "Epoch 468/500, Training Loss: 2.6328\n",
      "Epoch 469/500, Training Loss: 2.5892\n",
      "Epoch 470/500, Training Loss: 2.5735\n",
      "Epoch 471/500, Training Loss: 2.7019\n",
      "Epoch 472/500, Training Loss: 2.5186\n",
      "Epoch 473/500, Training Loss: 2.5810\n",
      "Epoch 474/500, Training Loss: 2.5145\n",
      "Epoch 475/500, Training Loss: 2.6265\n",
      "Epoch 476/500, Training Loss: 2.6412\n",
      "Epoch 477/500, Training Loss: 2.5872\n",
      "Epoch 478/500, Training Loss: 2.5165\n",
      "Epoch 479/500, Training Loss: 2.6264\n",
      "Epoch 480/500, Training Loss: 2.4964\n",
      "Epoch 481/500, Training Loss: 2.5651\n",
      "Epoch 482/500, Training Loss: 2.7289\n",
      "Epoch 483/500, Training Loss: 2.6774\n",
      "Epoch 484/500, Training Loss: 2.5048\n",
      "Epoch 485/500, Training Loss: 2.6009\n",
      "Epoch 486/500, Training Loss: 2.6783\n",
      "Epoch 487/500, Training Loss: 2.5398\n",
      "Epoch 488/500, Training Loss: 2.5782\n",
      "Epoch 489/500, Training Loss: 2.7079\n",
      "Epoch 490/500, Training Loss: 2.6382\n",
      "Epoch 491/500, Training Loss: 2.6516\n",
      "Epoch 492/500, Training Loss: 2.4750\n",
      "Epoch 493/500, Training Loss: 2.5128\n",
      "Epoch 494/500, Training Loss: 2.6444\n",
      "Epoch 495/500, Training Loss: 2.6193\n",
      "Epoch 496/500, Training Loss: 2.6082\n",
      "Epoch 497/500, Training Loss: 2.5524\n",
      "Epoch 498/500, Training Loss: 2.5180\n",
      "Epoch 499/500, Training Loss: 2.5158\n",
      "Epoch 500/500, Training Loss: 2.5858\n",
      "Best overall model index: 3 with average validation loss 2.7912\n"
     ]
    }
   ],
   "source": [
    "# Training with L2\n",
    "loss= FlowLoss()\n",
    "final_model_l2, (best_index_l2, val_loss_l2) = validate(\n",
    "        models_gauss,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs = hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_l2} with average validation loss {val_loss_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94248c62-dd95-4a7e-8a22-ba2032f18da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.5519\n",
      "Epoch 2/100, Training Loss: 1.4694\n",
      "Epoch 3/100, Training Loss: 1.5214\n",
      "Epoch 4/100, Training Loss: 1.4979\n",
      "Epoch 5/100, Training Loss: 1.4953\n",
      "Epoch 6/100, Training Loss: 1.5075\n",
      "Epoch 7/100, Training Loss: 1.4582\n",
      "Epoch 8/100, Training Loss: 1.3975\n",
      "Epoch 9/100, Training Loss: 1.5174\n",
      "Epoch 10/100, Training Loss: 1.5019\n",
      "Epoch 11/100, Training Loss: 1.4288\n",
      "Epoch 12/100, Training Loss: 1.5300\n",
      "Epoch 13/100, Training Loss: 1.5052\n",
      "Epoch 14/100, Training Loss: 1.5069\n",
      "Epoch 15/100, Training Loss: 1.5267\n",
      "Epoch 16/100, Training Loss: 1.4294\n",
      "Epoch 17/100, Training Loss: 1.4520\n",
      "Epoch 18/100, Training Loss: 1.4511\n",
      "Epoch 19/100, Training Loss: 1.5124\n",
      "Epoch 20/100, Training Loss: 1.4165\n",
      "Epoch 21/100, Training Loss: 1.5160\n",
      "Epoch 22/100, Training Loss: 1.4815\n",
      "Epoch 23/100, Training Loss: 1.4918\n",
      "Epoch 24/100, Training Loss: 1.5093\n",
      "Epoch 25/100, Training Loss: 1.4453\n",
      "Epoch 26/100, Training Loss: 1.4344\n",
      "Epoch 27/100, Training Loss: 1.4524\n",
      "Epoch 28/100, Training Loss: 1.4364\n",
      "Epoch 29/100, Training Loss: 1.4654\n",
      "Epoch 30/100, Training Loss: 1.4451\n",
      "Epoch 31/100, Training Loss: 1.4580\n",
      "Epoch 32/100, Training Loss: 1.4901\n",
      "Epoch 33/100, Training Loss: 1.4653\n",
      "Epoch 34/100, Training Loss: 1.4543\n",
      "Epoch 35/100, Training Loss: 1.4960\n",
      "Epoch 36/100, Training Loss: 1.4676\n",
      "Epoch 37/100, Training Loss: 1.3903\n",
      "Epoch 38/100, Training Loss: 1.4531\n",
      "Epoch 39/100, Training Loss: 1.4496\n",
      "Epoch 40/100, Training Loss: 1.4712\n",
      "Epoch 41/100, Training Loss: 1.4495\n",
      "Epoch 42/100, Training Loss: 1.5241\n",
      "Epoch 43/100, Training Loss: 1.4791\n",
      "Epoch 44/100, Training Loss: 1.4750\n",
      "Epoch 45/100, Training Loss: 1.5313\n",
      "Epoch 46/100, Training Loss: 1.5386\n",
      "Epoch 47/100, Training Loss: 1.4498\n",
      "Epoch 48/100, Training Loss: 1.4557\n",
      "Epoch 49/100, Training Loss: 1.4356\n",
      "Epoch 50/100, Training Loss: 1.4674\n",
      "Epoch 51/100, Training Loss: 1.4748\n",
      "Epoch 52/100, Training Loss: 1.5220\n",
      "Epoch 53/100, Training Loss: 1.4319\n",
      "Epoch 54/100, Training Loss: 1.4649\n",
      "Epoch 55/100, Training Loss: 1.4325\n",
      "Epoch 56/100, Training Loss: 1.4893\n",
      "Epoch 57/100, Training Loss: 1.5066\n",
      "Epoch 58/100, Training Loss: 1.4631\n",
      "Epoch 59/100, Training Loss: 1.4785\n",
      "Epoch 60/100, Training Loss: 1.4980\n",
      "Epoch 61/100, Training Loss: 1.4432\n",
      "Epoch 62/100, Training Loss: 1.4896\n",
      "Epoch 63/100, Training Loss: 1.4019\n",
      "Epoch 64/100, Training Loss: 1.4614\n",
      "Epoch 65/100, Training Loss: 1.4876\n",
      "Epoch 66/100, Training Loss: 1.4772\n",
      "Epoch 67/100, Training Loss: 1.4570\n",
      "Epoch 68/100, Training Loss: 1.4375\n",
      "Epoch 69/100, Training Loss: 1.4937\n",
      "Epoch 70/100, Training Loss: 1.4092\n",
      "Epoch 71/100, Training Loss: 1.4857\n",
      "Epoch 72/100, Training Loss: 1.4678\n",
      "Epoch 73/100, Training Loss: 1.4397\n",
      "Epoch 74/100, Training Loss: 1.3924\n",
      "Epoch 75/100, Training Loss: 1.4921\n",
      "Epoch 76/100, Training Loss: 1.4757\n",
      "Epoch 77/100, Training Loss: 1.3954\n",
      "Epoch 78/100, Training Loss: 1.4526\n",
      "Epoch 79/100, Training Loss: 1.4383\n",
      "Epoch 80/100, Training Loss: 1.4392\n",
      "Epoch 81/100, Training Loss: 1.4530\n",
      "Epoch 82/100, Training Loss: 1.4488\n",
      "Epoch 83/100, Training Loss: 1.4459\n",
      "Epoch 84/100, Training Loss: 1.4363\n",
      "Epoch 85/100, Training Loss: 1.4744\n",
      "Epoch 86/100, Training Loss: 1.4241\n",
      "Epoch 87/100, Training Loss: 1.4403\n",
      "Epoch 88/100, Training Loss: 1.4373\n",
      "Epoch 89/100, Training Loss: 1.4281\n",
      "Epoch 90/100, Training Loss: 1.3913\n",
      "Epoch 91/100, Training Loss: 1.4457\n",
      "Epoch 92/100, Training Loss: 1.4643\n",
      "Epoch 93/100, Training Loss: 1.4336\n",
      "Epoch 94/100, Training Loss: 1.4447\n",
      "Epoch 95/100, Training Loss: 1.4588\n",
      "Epoch 96/100, Training Loss: 1.5018\n",
      "Epoch 97/100, Training Loss: 1.4888\n",
      "Epoch 98/100, Training Loss: 1.4331\n",
      "Epoch 99/100, Training Loss: 1.4921\n",
      "Epoch 100/100, Training Loss: 1.4884\n",
      "Validation Loss: 1.4451\n",
      "Model 0, Fold 0: Validation Loss = 1.4451\n",
      "Epoch 1/100, Training Loss: 1.4735\n",
      "Epoch 2/100, Training Loss: 1.4918\n",
      "Epoch 3/100, Training Loss: 1.4819\n",
      "Epoch 4/100, Training Loss: 1.4522\n",
      "Epoch 5/100, Training Loss: 1.5050\n",
      "Epoch 6/100, Training Loss: 1.4092\n",
      "Epoch 7/100, Training Loss: 1.4952\n",
      "Epoch 8/100, Training Loss: 1.4341\n",
      "Epoch 9/100, Training Loss: 1.4294\n",
      "Epoch 10/100, Training Loss: 1.4154\n",
      "Epoch 11/100, Training Loss: 1.4545\n",
      "Epoch 12/100, Training Loss: 1.4500\n",
      "Epoch 13/100, Training Loss: 1.4834\n",
      "Epoch 14/100, Training Loss: 1.4322\n",
      "Epoch 15/100, Training Loss: 1.4073\n",
      "Epoch 16/100, Training Loss: 1.3996\n",
      "Epoch 17/100, Training Loss: 1.5384\n",
      "Epoch 18/100, Training Loss: 1.4995\n",
      "Epoch 19/100, Training Loss: 1.4080\n",
      "Epoch 20/100, Training Loss: 1.4871\n",
      "Epoch 21/100, Training Loss: 1.4270\n",
      "Epoch 22/100, Training Loss: 1.4694\n",
      "Epoch 23/100, Training Loss: 1.4004\n",
      "Epoch 24/100, Training Loss: 1.4675\n",
      "Epoch 25/100, Training Loss: 1.3953\n",
      "Epoch 26/100, Training Loss: 1.4709\n",
      "Epoch 27/100, Training Loss: 1.4450\n",
      "Epoch 28/100, Training Loss: 1.4276\n",
      "Epoch 29/100, Training Loss: 1.4833\n",
      "Epoch 30/100, Training Loss: 1.4970\n",
      "Epoch 31/100, Training Loss: 1.4661\n",
      "Epoch 32/100, Training Loss: 1.4119\n",
      "Epoch 33/100, Training Loss: 1.4889\n",
      "Epoch 34/100, Training Loss: 1.4496\n",
      "Epoch 35/100, Training Loss: 1.3819\n",
      "Epoch 36/100, Training Loss: 1.4359\n",
      "Epoch 37/100, Training Loss: 1.3864\n",
      "Epoch 38/100, Training Loss: 1.4864\n",
      "Epoch 39/100, Training Loss: 1.4967\n",
      "Epoch 40/100, Training Loss: 1.4272\n",
      "Epoch 41/100, Training Loss: 1.4497\n",
      "Epoch 42/100, Training Loss: 1.4315\n",
      "Epoch 43/100, Training Loss: 1.4373\n",
      "Epoch 44/100, Training Loss: 1.4428\n",
      "Epoch 45/100, Training Loss: 1.4391\n",
      "Epoch 46/100, Training Loss: 1.4339\n",
      "Epoch 47/100, Training Loss: 1.4476\n",
      "Epoch 48/100, Training Loss: 1.5054\n",
      "Epoch 49/100, Training Loss: 1.4551\n",
      "Epoch 50/100, Training Loss: 1.4165\n",
      "Epoch 51/100, Training Loss: 1.4575\n",
      "Epoch 52/100, Training Loss: 1.4201\n",
      "Epoch 53/100, Training Loss: 1.4480\n",
      "Epoch 54/100, Training Loss: 1.4488\n",
      "Epoch 55/100, Training Loss: 1.4568\n",
      "Epoch 56/100, Training Loss: 1.4240\n",
      "Epoch 57/100, Training Loss: 1.4472\n",
      "Epoch 58/100, Training Loss: 1.4411\n",
      "Epoch 59/100, Training Loss: 1.3863\n",
      "Epoch 60/100, Training Loss: 1.4955\n",
      "Epoch 61/100, Training Loss: 1.4507\n",
      "Epoch 62/100, Training Loss: 1.3706\n",
      "Epoch 63/100, Training Loss: 1.3983\n",
      "Epoch 64/100, Training Loss: 1.3742\n",
      "Epoch 65/100, Training Loss: 1.4478\n",
      "Epoch 66/100, Training Loss: 1.4492\n",
      "Epoch 67/100, Training Loss: 1.4410\n",
      "Epoch 68/100, Training Loss: 1.4207\n",
      "Epoch 69/100, Training Loss: 1.4614\n",
      "Epoch 70/100, Training Loss: 1.4762\n",
      "Epoch 71/100, Training Loss: 1.3852\n",
      "Epoch 72/100, Training Loss: 1.4853\n",
      "Epoch 73/100, Training Loss: 1.4402\n",
      "Epoch 74/100, Training Loss: 1.3917\n",
      "Epoch 75/100, Training Loss: 1.4330\n",
      "Epoch 76/100, Training Loss: 1.3732\n",
      "Epoch 77/100, Training Loss: 1.4106\n",
      "Epoch 78/100, Training Loss: 1.4162\n",
      "Epoch 79/100, Training Loss: 1.4609\n",
      "Epoch 80/100, Training Loss: 1.4308\n",
      "Epoch 81/100, Training Loss: 1.4791\n",
      "Epoch 82/100, Training Loss: 1.4179\n",
      "Epoch 83/100, Training Loss: 1.4524\n",
      "Epoch 84/100, Training Loss: 1.4738\n",
      "Epoch 85/100, Training Loss: 1.4271\n",
      "Epoch 86/100, Training Loss: 1.4321\n",
      "Epoch 87/100, Training Loss: 1.3708\n",
      "Epoch 88/100, Training Loss: 1.4829\n",
      "Epoch 89/100, Training Loss: 1.4722\n",
      "Epoch 90/100, Training Loss: 1.4406\n",
      "Epoch 91/100, Training Loss: 1.4076\n",
      "Epoch 92/100, Training Loss: 1.4624\n",
      "Epoch 93/100, Training Loss: 1.4439\n",
      "Epoch 94/100, Training Loss: 1.4715\n",
      "Epoch 95/100, Training Loss: 1.4183\n",
      "Epoch 96/100, Training Loss: 1.4544\n",
      "Epoch 97/100, Training Loss: 1.4650\n",
      "Epoch 98/100, Training Loss: 1.4106\n",
      "Epoch 99/100, Training Loss: 1.4726\n",
      "Epoch 100/100, Training Loss: 1.4779\n",
      "Validation Loss: 1.4734\n",
      "Model 0, Fold 1: Validation Loss = 1.4734\n",
      "Epoch 1/100, Training Loss: 1.6689\n",
      "Epoch 2/100, Training Loss: 1.5886\n",
      "Epoch 3/100, Training Loss: 1.5486\n",
      "Epoch 4/100, Training Loss: 1.5151\n",
      "Epoch 5/100, Training Loss: 1.5009\n",
      "Epoch 6/100, Training Loss: 1.5881\n",
      "Epoch 7/100, Training Loss: 1.4595\n",
      "Epoch 8/100, Training Loss: 1.4943\n",
      "Epoch 9/100, Training Loss: 1.4562\n",
      "Epoch 10/100, Training Loss: 1.4357\n",
      "Epoch 11/100, Training Loss: 1.4910\n",
      "Epoch 12/100, Training Loss: 1.4898\n",
      "Epoch 13/100, Training Loss: 1.4799\n",
      "Epoch 14/100, Training Loss: 1.4765\n",
      "Epoch 15/100, Training Loss: 1.4608\n",
      "Epoch 16/100, Training Loss: 1.4278\n",
      "Epoch 17/100, Training Loss: 1.4910\n",
      "Epoch 18/100, Training Loss: 1.4477\n",
      "Epoch 19/100, Training Loss: 1.4494\n",
      "Epoch 20/100, Training Loss: 1.4455\n",
      "Epoch 21/100, Training Loss: 1.3955\n",
      "Epoch 22/100, Training Loss: 1.4886\n",
      "Epoch 23/100, Training Loss: 1.5064\n",
      "Epoch 24/100, Training Loss: 1.4690\n",
      "Epoch 25/100, Training Loss: 1.4821\n",
      "Epoch 26/100, Training Loss: 1.4700\n",
      "Epoch 27/100, Training Loss: 1.4458\n",
      "Epoch 28/100, Training Loss: 1.4135\n",
      "Epoch 29/100, Training Loss: 1.4216\n",
      "Epoch 30/100, Training Loss: 1.4986\n",
      "Epoch 31/100, Training Loss: 1.4629\n",
      "Epoch 32/100, Training Loss: 1.4676\n",
      "Epoch 33/100, Training Loss: 1.4982\n",
      "Epoch 34/100, Training Loss: 1.4808\n",
      "Epoch 35/100, Training Loss: 1.4695\n",
      "Epoch 36/100, Training Loss: 1.4418\n",
      "Epoch 37/100, Training Loss: 1.4266\n",
      "Epoch 38/100, Training Loss: 1.4536\n",
      "Epoch 39/100, Training Loss: 1.4740\n",
      "Epoch 40/100, Training Loss: 1.4489\n",
      "Epoch 41/100, Training Loss: 1.4361\n",
      "Epoch 42/100, Training Loss: 1.4497\n",
      "Epoch 43/100, Training Loss: 1.4212\n",
      "Epoch 44/100, Training Loss: 1.5394\n",
      "Epoch 45/100, Training Loss: 1.4663\n",
      "Epoch 46/100, Training Loss: 1.4244\n",
      "Epoch 47/100, Training Loss: 1.4723\n",
      "Epoch 48/100, Training Loss: 1.4684\n",
      "Epoch 49/100, Training Loss: 1.4606\n",
      "Epoch 50/100, Training Loss: 1.4514\n",
      "Epoch 51/100, Training Loss: 1.4516\n",
      "Epoch 52/100, Training Loss: 1.4469\n",
      "Epoch 53/100, Training Loss: 1.4677\n",
      "Epoch 54/100, Training Loss: 1.4199\n",
      "Epoch 55/100, Training Loss: 1.4229\n",
      "Epoch 56/100, Training Loss: 1.4170\n",
      "Epoch 57/100, Training Loss: 1.5134\n",
      "Epoch 58/100, Training Loss: 1.4283\n",
      "Epoch 59/100, Training Loss: 1.4751\n",
      "Epoch 60/100, Training Loss: 1.4720\n",
      "Epoch 61/100, Training Loss: 1.4157\n",
      "Epoch 62/100, Training Loss: 1.4644\n",
      "Epoch 63/100, Training Loss: 1.4795\n",
      "Epoch 64/100, Training Loss: 1.4735\n",
      "Epoch 65/100, Training Loss: 1.5177\n",
      "Epoch 66/100, Training Loss: 1.4270\n",
      "Epoch 67/100, Training Loss: 1.4328\n",
      "Epoch 68/100, Training Loss: 1.4111\n",
      "Epoch 69/100, Training Loss: 1.4396\n",
      "Epoch 70/100, Training Loss: 1.4375\n",
      "Epoch 71/100, Training Loss: 1.4337\n",
      "Epoch 72/100, Training Loss: 1.4908\n",
      "Epoch 73/100, Training Loss: 1.4655\n",
      "Epoch 74/100, Training Loss: 1.4427\n",
      "Epoch 75/100, Training Loss: 1.3660\n",
      "Epoch 76/100, Training Loss: 1.5320\n",
      "Epoch 77/100, Training Loss: 1.4652\n",
      "Epoch 78/100, Training Loss: 1.4668\n",
      "Epoch 79/100, Training Loss: 1.4966\n",
      "Epoch 80/100, Training Loss: 1.4362\n",
      "Epoch 81/100, Training Loss: 1.4535\n",
      "Epoch 82/100, Training Loss: 1.4188\n",
      "Epoch 83/100, Training Loss: 1.5023\n",
      "Epoch 84/100, Training Loss: 1.4215\n",
      "Epoch 85/100, Training Loss: 1.5224\n",
      "Epoch 86/100, Training Loss: 1.4984\n",
      "Epoch 87/100, Training Loss: 1.4413\n",
      "Epoch 88/100, Training Loss: 1.4720\n",
      "Epoch 89/100, Training Loss: 1.4229\n",
      "Epoch 90/100, Training Loss: 1.4622\n",
      "Epoch 91/100, Training Loss: 1.4285\n",
      "Epoch 92/100, Training Loss: 1.4817\n",
      "Epoch 93/100, Training Loss: 1.4760\n",
      "Epoch 94/100, Training Loss: 1.5176\n",
      "Epoch 95/100, Training Loss: 1.4500\n",
      "Epoch 96/100, Training Loss: 1.4279\n",
      "Epoch 97/100, Training Loss: 1.4872\n",
      "Epoch 98/100, Training Loss: 1.4968\n",
      "Epoch 99/100, Training Loss: 1.4658\n",
      "Epoch 100/100, Training Loss: 1.4436\n",
      "Validation Loss: 1.4503\n",
      "Model 1, Fold 0: Validation Loss = 1.4503\n",
      "Epoch 1/100, Training Loss: 1.6841\n",
      "Epoch 2/100, Training Loss: 1.4994\n",
      "Epoch 3/100, Training Loss: 1.4464\n",
      "Epoch 4/100, Training Loss: 1.4220\n",
      "Epoch 5/100, Training Loss: 1.4560\n",
      "Epoch 6/100, Training Loss: 1.4888\n",
      "Epoch 7/100, Training Loss: 1.4502\n",
      "Epoch 8/100, Training Loss: 1.4702\n",
      "Epoch 9/100, Training Loss: 1.4036\n",
      "Epoch 10/100, Training Loss: 1.4406\n",
      "Epoch 11/100, Training Loss: 1.4719\n",
      "Epoch 12/100, Training Loss: 1.4648\n",
      "Epoch 13/100, Training Loss: 1.4526\n",
      "Epoch 14/100, Training Loss: 1.3974\n",
      "Epoch 15/100, Training Loss: 1.4419\n",
      "Epoch 16/100, Training Loss: 1.4506\n",
      "Epoch 17/100, Training Loss: 1.4155\n",
      "Epoch 18/100, Training Loss: 1.3667\n",
      "Epoch 19/100, Training Loss: 1.4726\n",
      "Epoch 20/100, Training Loss: 1.4913\n",
      "Epoch 21/100, Training Loss: 1.4476\n",
      "Epoch 22/100, Training Loss: 1.3872\n",
      "Epoch 23/100, Training Loss: 1.4083\n",
      "Epoch 24/100, Training Loss: 1.4866\n",
      "Epoch 25/100, Training Loss: 1.4624\n",
      "Epoch 26/100, Training Loss: 1.4910\n",
      "Epoch 27/100, Training Loss: 1.4911\n",
      "Epoch 28/100, Training Loss: 1.4381\n",
      "Epoch 29/100, Training Loss: 1.4252\n",
      "Epoch 30/100, Training Loss: 1.4782\n",
      "Epoch 31/100, Training Loss: 1.4622\n",
      "Epoch 32/100, Training Loss: 1.4015\n",
      "Epoch 33/100, Training Loss: 1.4388\n",
      "Epoch 34/100, Training Loss: 1.4512\n",
      "Epoch 35/100, Training Loss: 1.4386\n",
      "Epoch 36/100, Training Loss: 1.3839\n",
      "Epoch 37/100, Training Loss: 1.4807\n",
      "Epoch 38/100, Training Loss: 1.4279\n",
      "Epoch 39/100, Training Loss: 1.4326\n",
      "Epoch 40/100, Training Loss: 1.4340\n",
      "Epoch 41/100, Training Loss: 1.4314\n",
      "Epoch 42/100, Training Loss: 1.4485\n",
      "Epoch 43/100, Training Loss: 1.4284\n",
      "Epoch 44/100, Training Loss: 1.4445\n",
      "Epoch 45/100, Training Loss: 1.5047\n",
      "Epoch 46/100, Training Loss: 1.4824\n",
      "Epoch 47/100, Training Loss: 1.4117\n",
      "Epoch 48/100, Training Loss: 1.4186\n",
      "Epoch 49/100, Training Loss: 1.4318\n",
      "Epoch 50/100, Training Loss: 1.4844\n",
      "Epoch 51/100, Training Loss: 1.4058\n",
      "Epoch 52/100, Training Loss: 1.4394\n",
      "Epoch 53/100, Training Loss: 1.4226\n",
      "Epoch 54/100, Training Loss: 1.4522\n",
      "Epoch 55/100, Training Loss: 1.4446\n",
      "Epoch 56/100, Training Loss: 1.4708\n",
      "Epoch 57/100, Training Loss: 1.4363\n",
      "Epoch 58/100, Training Loss: 1.4366\n",
      "Epoch 59/100, Training Loss: 1.4219\n",
      "Epoch 60/100, Training Loss: 1.4164\n",
      "Epoch 61/100, Training Loss: 1.4497\n",
      "Epoch 62/100, Training Loss: 1.4497\n",
      "Epoch 63/100, Training Loss: 1.4443\n",
      "Epoch 64/100, Training Loss: 1.3981\n",
      "Epoch 65/100, Training Loss: 1.4264\n",
      "Epoch 66/100, Training Loss: 1.3924\n",
      "Epoch 67/100, Training Loss: 1.4396\n",
      "Epoch 68/100, Training Loss: 1.4532\n",
      "Epoch 69/100, Training Loss: 1.3903\n",
      "Epoch 70/100, Training Loss: 1.4719\n",
      "Epoch 71/100, Training Loss: 1.4346\n",
      "Epoch 72/100, Training Loss: 1.4418\n",
      "Epoch 73/100, Training Loss: 1.3886\n",
      "Epoch 74/100, Training Loss: 1.4212\n",
      "Epoch 75/100, Training Loss: 1.4484\n",
      "Epoch 76/100, Training Loss: 1.4493\n",
      "Epoch 77/100, Training Loss: 1.4793\n",
      "Epoch 78/100, Training Loss: 1.3726\n",
      "Epoch 79/100, Training Loss: 1.4581\n",
      "Epoch 80/100, Training Loss: 1.3807\n",
      "Epoch 81/100, Training Loss: 1.3964\n",
      "Epoch 82/100, Training Loss: 1.4033\n",
      "Epoch 83/100, Training Loss: 1.4237\n",
      "Epoch 84/100, Training Loss: 1.4627\n",
      "Epoch 85/100, Training Loss: 1.4526\n",
      "Epoch 86/100, Training Loss: 1.3824\n",
      "Epoch 87/100, Training Loss: 1.4473\n",
      "Epoch 88/100, Training Loss: 1.4299\n",
      "Epoch 89/100, Training Loss: 1.4126\n",
      "Epoch 90/100, Training Loss: 1.4221\n",
      "Epoch 91/100, Training Loss: 1.4367\n",
      "Epoch 92/100, Training Loss: 1.4274\n",
      "Epoch 93/100, Training Loss: 1.4097\n",
      "Epoch 94/100, Training Loss: 1.3900\n",
      "Epoch 95/100, Training Loss: 1.4111\n",
      "Epoch 96/100, Training Loss: 1.3824\n",
      "Epoch 97/100, Training Loss: 1.4406\n",
      "Epoch 98/100, Training Loss: 1.4588\n",
      "Epoch 99/100, Training Loss: 1.4340\n",
      "Epoch 100/100, Training Loss: 1.3893\n",
      "Validation Loss: 1.4916\n",
      "Model 1, Fold 1: Validation Loss = 1.4916\n",
      "Epoch 1/100, Training Loss: 2.0648\n",
      "Epoch 2/100, Training Loss: 1.6952\n",
      "Epoch 3/100, Training Loss: 1.5810\n",
      "Epoch 4/100, Training Loss: 1.4978\n",
      "Epoch 5/100, Training Loss: 1.4726\n",
      "Epoch 6/100, Training Loss: 1.5395\n",
      "Epoch 7/100, Training Loss: 1.5249\n",
      "Epoch 8/100, Training Loss: 1.5059\n",
      "Epoch 9/100, Training Loss: 1.4493\n",
      "Epoch 10/100, Training Loss: 1.5348\n",
      "Epoch 11/100, Training Loss: 1.4777\n",
      "Epoch 12/100, Training Loss: 1.4919\n",
      "Epoch 13/100, Training Loss: 1.4697\n",
      "Epoch 14/100, Training Loss: 1.3999\n",
      "Epoch 15/100, Training Loss: 1.4404\n",
      "Epoch 16/100, Training Loss: 1.4736\n",
      "Epoch 17/100, Training Loss: 1.5115\n",
      "Epoch 18/100, Training Loss: 1.4561\n",
      "Epoch 19/100, Training Loss: 1.4826\n",
      "Epoch 20/100, Training Loss: 1.4568\n",
      "Epoch 21/100, Training Loss: 1.4658\n",
      "Epoch 22/100, Training Loss: 1.4633\n",
      "Epoch 23/100, Training Loss: 1.4653\n",
      "Epoch 24/100, Training Loss: 1.4567\n",
      "Epoch 25/100, Training Loss: 1.4897\n",
      "Epoch 26/100, Training Loss: 1.4637\n",
      "Epoch 27/100, Training Loss: 1.4336\n",
      "Epoch 28/100, Training Loss: 1.4351\n",
      "Epoch 29/100, Training Loss: 1.4331\n",
      "Epoch 30/100, Training Loss: 1.4690\n",
      "Epoch 31/100, Training Loss: 1.4724\n",
      "Epoch 32/100, Training Loss: 1.4937\n",
      "Epoch 33/100, Training Loss: 1.4741\n",
      "Epoch 34/100, Training Loss: 1.5048\n",
      "Epoch 35/100, Training Loss: 1.5217\n",
      "Epoch 36/100, Training Loss: 1.4613\n",
      "Epoch 37/100, Training Loss: 1.5005\n",
      "Epoch 38/100, Training Loss: 1.4413\n",
      "Epoch 39/100, Training Loss: 1.4762\n",
      "Epoch 40/100, Training Loss: 1.4484\n",
      "Epoch 41/100, Training Loss: 1.4388\n",
      "Epoch 42/100, Training Loss: 1.4632\n",
      "Epoch 43/100, Training Loss: 1.4556\n",
      "Epoch 44/100, Training Loss: 1.4323\n",
      "Epoch 45/100, Training Loss: 1.4367\n",
      "Epoch 46/100, Training Loss: 1.4517\n",
      "Epoch 47/100, Training Loss: 1.4515\n",
      "Epoch 48/100, Training Loss: 1.4225\n",
      "Epoch 49/100, Training Loss: 1.4233\n",
      "Epoch 50/100, Training Loss: 1.4576\n",
      "Epoch 51/100, Training Loss: 1.4403\n",
      "Epoch 52/100, Training Loss: 1.4334\n",
      "Epoch 53/100, Training Loss: 1.4720\n",
      "Epoch 54/100, Training Loss: 1.4303\n",
      "Epoch 55/100, Training Loss: 1.4848\n",
      "Epoch 56/100, Training Loss: 1.4824\n",
      "Epoch 57/100, Training Loss: 1.3856\n",
      "Epoch 58/100, Training Loss: 1.4013\n",
      "Epoch 59/100, Training Loss: 1.3982\n",
      "Epoch 60/100, Training Loss: 1.4193\n",
      "Epoch 61/100, Training Loss: 1.4743\n",
      "Epoch 62/100, Training Loss: 1.4318\n",
      "Epoch 63/100, Training Loss: 1.4218\n",
      "Epoch 64/100, Training Loss: 1.4367\n",
      "Epoch 65/100, Training Loss: 1.4448\n",
      "Epoch 66/100, Training Loss: 1.4951\n",
      "Epoch 67/100, Training Loss: 1.3898\n",
      "Epoch 68/100, Training Loss: 1.4835\n",
      "Epoch 69/100, Training Loss: 1.4176\n",
      "Epoch 70/100, Training Loss: 1.4605\n",
      "Epoch 71/100, Training Loss: 1.4241\n",
      "Epoch 72/100, Training Loss: 1.4643\n",
      "Epoch 73/100, Training Loss: 1.4385\n",
      "Epoch 74/100, Training Loss: 1.4433\n",
      "Epoch 75/100, Training Loss: 1.5046\n",
      "Epoch 76/100, Training Loss: 1.4269\n",
      "Epoch 77/100, Training Loss: 1.4435\n",
      "Epoch 78/100, Training Loss: 1.4973\n",
      "Epoch 79/100, Training Loss: 1.4625\n",
      "Epoch 80/100, Training Loss: 1.4738\n",
      "Epoch 81/100, Training Loss: 1.4563\n",
      "Epoch 82/100, Training Loss: 1.4498\n",
      "Epoch 83/100, Training Loss: 1.4773\n",
      "Epoch 84/100, Training Loss: 1.4235\n",
      "Epoch 85/100, Training Loss: 1.4170\n",
      "Epoch 86/100, Training Loss: 1.4745\n",
      "Epoch 87/100, Training Loss: 1.4385\n",
      "Epoch 88/100, Training Loss: 1.4324\n",
      "Epoch 89/100, Training Loss: 1.4028\n",
      "Epoch 90/100, Training Loss: 1.4704\n",
      "Epoch 91/100, Training Loss: 1.4266\n",
      "Epoch 92/100, Training Loss: 1.4376\n",
      "Epoch 93/100, Training Loss: 1.4956\n",
      "Epoch 94/100, Training Loss: 1.4708\n",
      "Epoch 95/100, Training Loss: 1.4245\n",
      "Epoch 96/100, Training Loss: 1.4142\n",
      "Epoch 97/100, Training Loss: 1.4906\n",
      "Epoch 98/100, Training Loss: 1.4675\n",
      "Epoch 99/100, Training Loss: 1.4234\n",
      "Epoch 100/100, Training Loss: 1.4473\n",
      "Validation Loss: 1.4508\n",
      "Model 2, Fold 0: Validation Loss = 1.4508\n",
      "Epoch 1/100, Training Loss: 1.9625\n",
      "Epoch 2/100, Training Loss: 1.6884\n",
      "Epoch 3/100, Training Loss: 1.5841\n",
      "Epoch 4/100, Training Loss: 1.6266\n",
      "Epoch 5/100, Training Loss: 1.4828\n",
      "Epoch 6/100, Training Loss: 1.4862\n",
      "Epoch 7/100, Training Loss: 1.4475\n",
      "Epoch 8/100, Training Loss: 1.4419\n",
      "Epoch 9/100, Training Loss: 1.4221\n",
      "Epoch 10/100, Training Loss: 1.4436\n",
      "Epoch 11/100, Training Loss: 1.4371\n",
      "Epoch 12/100, Training Loss: 1.4932\n",
      "Epoch 13/100, Training Loss: 1.4209\n",
      "Epoch 14/100, Training Loss: 1.3949\n",
      "Epoch 15/100, Training Loss: 1.4627\n",
      "Epoch 16/100, Training Loss: 1.4407\n",
      "Epoch 17/100, Training Loss: 1.4175\n",
      "Epoch 18/100, Training Loss: 1.3998\n",
      "Epoch 19/100, Training Loss: 1.3660\n",
      "Epoch 20/100, Training Loss: 1.4918\n",
      "Epoch 21/100, Training Loss: 1.4532\n",
      "Epoch 22/100, Training Loss: 1.4096\n",
      "Epoch 23/100, Training Loss: 1.3664\n",
      "Epoch 24/100, Training Loss: 1.4532\n",
      "Epoch 25/100, Training Loss: 1.4373\n",
      "Epoch 26/100, Training Loss: 1.4667\n",
      "Epoch 27/100, Training Loss: 1.4761\n",
      "Epoch 28/100, Training Loss: 1.3888\n",
      "Epoch 29/100, Training Loss: 1.4354\n",
      "Epoch 30/100, Training Loss: 1.4281\n",
      "Epoch 31/100, Training Loss: 1.4469\n",
      "Epoch 32/100, Training Loss: 1.4325\n",
      "Epoch 33/100, Training Loss: 1.4378\n",
      "Epoch 34/100, Training Loss: 1.4570\n",
      "Epoch 35/100, Training Loss: 1.4262\n",
      "Epoch 36/100, Training Loss: 1.4733\n",
      "Epoch 37/100, Training Loss: 1.4542\n",
      "Epoch 38/100, Training Loss: 1.4829\n",
      "Epoch 39/100, Training Loss: 1.4452\n",
      "Epoch 40/100, Training Loss: 1.4480\n",
      "Epoch 41/100, Training Loss: 1.4527\n",
      "Epoch 42/100, Training Loss: 1.4294\n",
      "Epoch 43/100, Training Loss: 1.4237\n",
      "Epoch 44/100, Training Loss: 1.4418\n",
      "Epoch 45/100, Training Loss: 1.4844\n",
      "Epoch 46/100, Training Loss: 1.4970\n",
      "Epoch 47/100, Training Loss: 1.4492\n",
      "Epoch 48/100, Training Loss: 1.3966\n",
      "Epoch 49/100, Training Loss: 1.4535\n",
      "Epoch 50/100, Training Loss: 1.4388\n",
      "Epoch 51/100, Training Loss: 1.4371\n",
      "Epoch 52/100, Training Loss: 1.4499\n",
      "Epoch 53/100, Training Loss: 1.5025\n",
      "Epoch 54/100, Training Loss: 1.4427\n",
      "Epoch 55/100, Training Loss: 1.4332\n",
      "Epoch 56/100, Training Loss: 1.4738\n",
      "Epoch 57/100, Training Loss: 1.3849\n",
      "Epoch 58/100, Training Loss: 1.5090\n",
      "Epoch 59/100, Training Loss: 1.4001\n",
      "Epoch 60/100, Training Loss: 1.4311\n",
      "Epoch 61/100, Training Loss: 1.3561\n",
      "Epoch 62/100, Training Loss: 1.5084\n",
      "Epoch 63/100, Training Loss: 1.4018\n",
      "Epoch 64/100, Training Loss: 1.3942\n",
      "Epoch 65/100, Training Loss: 1.4414\n",
      "Epoch 66/100, Training Loss: 1.4496\n",
      "Epoch 67/100, Training Loss: 1.4066\n",
      "Epoch 68/100, Training Loss: 1.4162\n",
      "Epoch 69/100, Training Loss: 1.4539\n",
      "Epoch 70/100, Training Loss: 1.4494\n",
      "Epoch 71/100, Training Loss: 1.4245\n",
      "Epoch 72/100, Training Loss: 1.4033\n",
      "Epoch 73/100, Training Loss: 1.4325\n",
      "Epoch 74/100, Training Loss: 1.4686\n",
      "Epoch 75/100, Training Loss: 1.3973\n",
      "Epoch 76/100, Training Loss: 1.4228\n",
      "Epoch 77/100, Training Loss: 1.4033\n",
      "Epoch 78/100, Training Loss: 1.4640\n",
      "Epoch 79/100, Training Loss: 1.4068\n",
      "Epoch 80/100, Training Loss: 1.3594\n",
      "Epoch 81/100, Training Loss: 1.3701\n",
      "Epoch 82/100, Training Loss: 1.4039\n",
      "Epoch 83/100, Training Loss: 1.4531\n",
      "Epoch 84/100, Training Loss: 1.5148\n",
      "Epoch 85/100, Training Loss: 1.4520\n",
      "Epoch 86/100, Training Loss: 1.4279\n",
      "Epoch 87/100, Training Loss: 1.5027\n",
      "Epoch 88/100, Training Loss: 1.4492\n",
      "Epoch 89/100, Training Loss: 1.4409\n",
      "Epoch 90/100, Training Loss: 1.4615\n",
      "Epoch 91/100, Training Loss: 1.3466\n",
      "Epoch 92/100, Training Loss: 1.4215\n",
      "Epoch 93/100, Training Loss: 1.4449\n",
      "Epoch 94/100, Training Loss: 1.4302\n",
      "Epoch 95/100, Training Loss: 1.4253\n",
      "Epoch 96/100, Training Loss: 1.4352\n",
      "Epoch 97/100, Training Loss: 1.4360\n",
      "Epoch 98/100, Training Loss: 1.4356\n",
      "Epoch 99/100, Training Loss: 1.4211\n",
      "Epoch 100/100, Training Loss: 1.3785\n",
      "Validation Loss: 1.4740\n",
      "Model 2, Fold 1: Validation Loss = 1.4740\n",
      "Epoch 1/100, Training Loss: 2.1885\n",
      "Epoch 2/100, Training Loss: 1.7389\n",
      "Epoch 3/100, Training Loss: 1.7894\n",
      "Epoch 4/100, Training Loss: 1.8584\n",
      "Epoch 5/100, Training Loss: 1.7909\n",
      "Epoch 6/100, Training Loss: 1.8546\n",
      "Epoch 7/100, Training Loss: 1.8345\n",
      "Epoch 8/100, Training Loss: 1.7907\n",
      "Epoch 9/100, Training Loss: 1.7682\n",
      "Epoch 10/100, Training Loss: 1.6943\n",
      "Epoch 11/100, Training Loss: 1.6368\n",
      "Epoch 12/100, Training Loss: 1.6537\n",
      "Epoch 13/100, Training Loss: 1.6931\n",
      "Epoch 14/100, Training Loss: 1.7317\n",
      "Epoch 15/100, Training Loss: 1.7702\n",
      "Epoch 16/100, Training Loss: 1.7752\n",
      "Epoch 17/100, Training Loss: 1.7226\n",
      "Epoch 18/100, Training Loss: 1.7617\n",
      "Epoch 19/100, Training Loss: 1.7418\n",
      "Epoch 20/100, Training Loss: 1.7086\n",
      "Epoch 21/100, Training Loss: 1.6491\n",
      "Epoch 22/100, Training Loss: 1.6181\n",
      "Epoch 23/100, Training Loss: 1.5923\n",
      "Epoch 24/100, Training Loss: 1.5873\n",
      "Epoch 25/100, Training Loss: 1.6354\n",
      "Epoch 26/100, Training Loss: 1.5590\n",
      "Epoch 27/100, Training Loss: 1.6154\n",
      "Epoch 28/100, Training Loss: 1.5958\n",
      "Epoch 29/100, Training Loss: 1.5674\n",
      "Epoch 30/100, Training Loss: 1.5694\n",
      "Epoch 31/100, Training Loss: 1.5738\n",
      "Epoch 32/100, Training Loss: 1.5716\n",
      "Epoch 33/100, Training Loss: 1.5444\n",
      "Epoch 34/100, Training Loss: 1.5362\n",
      "Epoch 35/100, Training Loss: 1.5927\n",
      "Epoch 36/100, Training Loss: 1.6655\n",
      "Epoch 37/100, Training Loss: 1.6178\n",
      "Epoch 38/100, Training Loss: 1.6464\n",
      "Epoch 39/100, Training Loss: 1.6362\n",
      "Epoch 40/100, Training Loss: 1.5952\n",
      "Epoch 41/100, Training Loss: 1.6129\n",
      "Epoch 42/100, Training Loss: 1.6110\n",
      "Epoch 43/100, Training Loss: 1.6683\n",
      "Epoch 44/100, Training Loss: 1.6845\n",
      "Epoch 45/100, Training Loss: 1.6377\n",
      "Epoch 46/100, Training Loss: 1.6542\n",
      "Epoch 47/100, Training Loss: 1.6795\n",
      "Epoch 48/100, Training Loss: 1.7111\n",
      "Epoch 49/100, Training Loss: 1.7272\n",
      "Epoch 50/100, Training Loss: 1.7106\n",
      "Epoch 51/100, Training Loss: 1.6587\n",
      "Epoch 52/100, Training Loss: 1.6758\n",
      "Epoch 53/100, Training Loss: 1.7077\n",
      "Epoch 54/100, Training Loss: 1.6744\n",
      "Epoch 55/100, Training Loss: 1.6551\n",
      "Epoch 56/100, Training Loss: 1.7193\n",
      "Epoch 57/100, Training Loss: 1.7191\n",
      "Epoch 58/100, Training Loss: 1.7319\n",
      "Epoch 59/100, Training Loss: 1.6830\n",
      "Epoch 60/100, Training Loss: 1.5951\n",
      "Epoch 61/100, Training Loss: 1.5614\n",
      "Epoch 62/100, Training Loss: 1.5616\n",
      "Epoch 63/100, Training Loss: 1.5589\n",
      "Epoch 64/100, Training Loss: 1.4954\n",
      "Epoch 65/100, Training Loss: 1.5762\n",
      "Epoch 66/100, Training Loss: 1.5098\n",
      "Epoch 67/100, Training Loss: 1.5934\n",
      "Epoch 68/100, Training Loss: 1.5126\n",
      "Epoch 69/100, Training Loss: 1.5304\n",
      "Epoch 70/100, Training Loss: 1.5016\n",
      "Epoch 71/100, Training Loss: 1.4989\n",
      "Epoch 72/100, Training Loss: 1.5840\n",
      "Epoch 73/100, Training Loss: 1.5014\n",
      "Epoch 74/100, Training Loss: 1.5734\n",
      "Epoch 75/100, Training Loss: 1.5634\n",
      "Epoch 76/100, Training Loss: 1.5316\n",
      "Epoch 77/100, Training Loss: 1.5125\n",
      "Epoch 78/100, Training Loss: 1.5763\n",
      "Epoch 79/100, Training Loss: 1.5576\n",
      "Epoch 80/100, Training Loss: 1.5341\n",
      "Epoch 81/100, Training Loss: 1.5832\n",
      "Epoch 82/100, Training Loss: 1.6610\n",
      "Epoch 83/100, Training Loss: 1.5760\n",
      "Epoch 84/100, Training Loss: 1.6274\n",
      "Epoch 85/100, Training Loss: 1.5465\n",
      "Epoch 86/100, Training Loss: 1.6615\n",
      "Epoch 87/100, Training Loss: 1.5674\n",
      "Epoch 88/100, Training Loss: 1.6198\n",
      "Epoch 89/100, Training Loss: 1.6105\n",
      "Epoch 90/100, Training Loss: 1.6248\n",
      "Epoch 91/100, Training Loss: 1.6023\n",
      "Epoch 92/100, Training Loss: 1.6268\n",
      "Epoch 93/100, Training Loss: 1.6666\n",
      "Epoch 94/100, Training Loss: 1.5790\n",
      "Epoch 95/100, Training Loss: 1.6102\n",
      "Epoch 96/100, Training Loss: 1.5787\n",
      "Epoch 97/100, Training Loss: 1.5894\n",
      "Epoch 98/100, Training Loss: 1.5274\n",
      "Epoch 99/100, Training Loss: 1.6182\n",
      "Epoch 100/100, Training Loss: 1.5927\n",
      "Validation Loss: 1.5235\n",
      "Model 3, Fold 0: Validation Loss = 1.5235\n",
      "Epoch 1/100, Training Loss: 2.4579\n",
      "Epoch 2/100, Training Loss: 1.9339\n",
      "Epoch 3/100, Training Loss: 1.8343\n",
      "Epoch 4/100, Training Loss: 1.7154\n",
      "Epoch 5/100, Training Loss: 1.6065\n",
      "Epoch 6/100, Training Loss: 1.9403\n",
      "Epoch 7/100, Training Loss: 2.3968\n",
      "Epoch 8/100, Training Loss: 1.7927\n",
      "Epoch 9/100, Training Loss: 1.5851\n",
      "Epoch 10/100, Training Loss: 1.5320\n",
      "Epoch 11/100, Training Loss: 1.5507\n",
      "Epoch 12/100, Training Loss: 1.5599\n",
      "Epoch 13/100, Training Loss: 1.7121\n",
      "Epoch 14/100, Training Loss: 1.6731\n",
      "Epoch 15/100, Training Loss: 1.6056\n",
      "Epoch 16/100, Training Loss: 1.5463\n",
      "Epoch 17/100, Training Loss: 1.5862\n",
      "Epoch 18/100, Training Loss: 1.5494\n",
      "Epoch 19/100, Training Loss: 1.6007\n",
      "Epoch 20/100, Training Loss: 1.5255\n",
      "Epoch 21/100, Training Loss: 1.4319\n",
      "Epoch 22/100, Training Loss: 1.5211\n",
      "Epoch 23/100, Training Loss: 1.4310\n",
      "Epoch 24/100, Training Loss: 1.4522\n",
      "Epoch 25/100, Training Loss: 1.4662\n",
      "Epoch 26/100, Training Loss: 1.4634\n",
      "Epoch 27/100, Training Loss: 1.3975\n",
      "Epoch 28/100, Training Loss: 1.4922\n",
      "Epoch 29/100, Training Loss: 1.5164\n",
      "Epoch 30/100, Training Loss: 1.5328\n",
      "Epoch 31/100, Training Loss: 1.6066\n",
      "Epoch 32/100, Training Loss: 1.5596\n",
      "Epoch 33/100, Training Loss: 1.6355\n",
      "Epoch 34/100, Training Loss: 1.5353\n",
      "Epoch 35/100, Training Loss: 1.6343\n",
      "Epoch 36/100, Training Loss: 1.5244\n",
      "Epoch 37/100, Training Loss: 1.5656\n",
      "Epoch 38/100, Training Loss: 1.5403\n",
      "Epoch 39/100, Training Loss: 1.5802\n",
      "Epoch 40/100, Training Loss: 1.5135\n",
      "Epoch 41/100, Training Loss: 1.4584\n",
      "Epoch 42/100, Training Loss: 1.5147\n",
      "Epoch 43/100, Training Loss: 1.5150\n",
      "Epoch 44/100, Training Loss: 1.5320\n",
      "Epoch 45/100, Training Loss: 1.5197\n",
      "Epoch 46/100, Training Loss: 1.6281\n",
      "Epoch 47/100, Training Loss: 1.6441\n",
      "Epoch 48/100, Training Loss: 1.6596\n",
      "Epoch 49/100, Training Loss: 1.6177\n",
      "Epoch 50/100, Training Loss: 1.6677\n",
      "Epoch 51/100, Training Loss: 1.6649\n",
      "Epoch 52/100, Training Loss: 1.6480\n",
      "Epoch 53/100, Training Loss: 1.6397\n",
      "Epoch 54/100, Training Loss: 1.6278\n",
      "Epoch 55/100, Training Loss: 1.5834\n",
      "Epoch 56/100, Training Loss: 1.5731\n",
      "Epoch 57/100, Training Loss: 1.5497\n",
      "Epoch 58/100, Training Loss: 1.5682\n",
      "Epoch 59/100, Training Loss: 1.5184\n",
      "Epoch 60/100, Training Loss: 1.5353\n",
      "Epoch 61/100, Training Loss: 1.5503\n",
      "Epoch 62/100, Training Loss: 1.4995\n",
      "Epoch 63/100, Training Loss: 1.4795\n",
      "Epoch 64/100, Training Loss: 1.4968\n",
      "Epoch 65/100, Training Loss: 1.5410\n",
      "Epoch 66/100, Training Loss: 1.4879\n",
      "Epoch 67/100, Training Loss: 1.5304\n",
      "Epoch 68/100, Training Loss: 1.4983\n",
      "Epoch 69/100, Training Loss: 1.4957\n",
      "Epoch 70/100, Training Loss: 1.5141\n",
      "Epoch 71/100, Training Loss: 1.5090\n",
      "Epoch 72/100, Training Loss: 1.5088\n",
      "Epoch 73/100, Training Loss: 1.5400\n",
      "Epoch 74/100, Training Loss: 1.6652\n",
      "Epoch 75/100, Training Loss: 1.6438\n",
      "Epoch 76/100, Training Loss: 1.6468\n",
      "Epoch 77/100, Training Loss: 1.6415\n",
      "Epoch 78/100, Training Loss: 1.6238\n",
      "Epoch 79/100, Training Loss: 1.5918\n",
      "Epoch 80/100, Training Loss: 1.5831\n",
      "Epoch 81/100, Training Loss: 1.6207\n",
      "Epoch 82/100, Training Loss: 1.5931\n",
      "Epoch 83/100, Training Loss: 1.5784\n",
      "Epoch 84/100, Training Loss: 1.6127\n",
      "Epoch 85/100, Training Loss: 1.6017\n",
      "Epoch 86/100, Training Loss: 1.6019\n",
      "Epoch 87/100, Training Loss: 1.5923\n",
      "Epoch 88/100, Training Loss: 1.5681\n",
      "Epoch 89/100, Training Loss: 1.5874\n",
      "Epoch 90/100, Training Loss: 1.5586\n",
      "Epoch 91/100, Training Loss: 1.4976\n",
      "Epoch 92/100, Training Loss: 1.5159\n",
      "Epoch 93/100, Training Loss: 1.5113\n",
      "Epoch 94/100, Training Loss: 1.5561\n",
      "Epoch 95/100, Training Loss: 1.5018\n",
      "Epoch 96/100, Training Loss: 1.5175\n",
      "Epoch 97/100, Training Loss: 1.4968\n",
      "Epoch 98/100, Training Loss: 1.5465\n",
      "Epoch 99/100, Training Loss: 1.4994\n",
      "Epoch 100/100, Training Loss: 1.5064\n",
      "Validation Loss: 1.5547\n",
      "Model 3, Fold 1: Validation Loss = 1.5547\n",
      "Best overall model index: 0 with average validation loss 1.4593\n",
      "Retraining best model on full dataset...\n",
      "Epoch 1/100, Training Loss: 1.5355\n",
      "Epoch 2/100, Training Loss: 1.4775\n",
      "Epoch 3/100, Training Loss: 1.4593\n",
      "Epoch 4/100, Training Loss: 1.4514\n",
      "Epoch 5/100, Training Loss: 1.4277\n",
      "Epoch 6/100, Training Loss: 1.4221\n",
      "Epoch 7/100, Training Loss: 1.4574\n",
      "Epoch 8/100, Training Loss: 1.4897\n",
      "Epoch 9/100, Training Loss: 1.4093\n",
      "Epoch 10/100, Training Loss: 1.4400\n",
      "Epoch 11/100, Training Loss: 1.4676\n",
      "Epoch 12/100, Training Loss: 1.4415\n",
      "Epoch 13/100, Training Loss: 1.4410\n",
      "Epoch 14/100, Training Loss: 1.4342\n",
      "Epoch 15/100, Training Loss: 1.4190\n",
      "Epoch 16/100, Training Loss: 1.4529\n",
      "Epoch 17/100, Training Loss: 1.4182\n",
      "Epoch 18/100, Training Loss: 1.4500\n",
      "Epoch 19/100, Training Loss: 1.4760\n",
      "Epoch 20/100, Training Loss: 1.4581\n",
      "Epoch 21/100, Training Loss: 1.4612\n",
      "Epoch 22/100, Training Loss: 1.4507\n",
      "Epoch 23/100, Training Loss: 1.4535\n",
      "Epoch 24/100, Training Loss: 1.4459\n",
      "Epoch 25/100, Training Loss: 1.4109\n",
      "Epoch 26/100, Training Loss: 1.4497\n",
      "Epoch 27/100, Training Loss: 1.4483\n",
      "Epoch 28/100, Training Loss: 1.4489\n",
      "Epoch 29/100, Training Loss: 1.4622\n",
      "Epoch 30/100, Training Loss: 1.4456\n",
      "Epoch 31/100, Training Loss: 1.4161\n",
      "Epoch 32/100, Training Loss: 1.4347\n",
      "Epoch 33/100, Training Loss: 1.4598\n",
      "Epoch 34/100, Training Loss: 1.4362\n",
      "Epoch 35/100, Training Loss: 1.4572\n",
      "Epoch 36/100, Training Loss: 1.4484\n",
      "Epoch 37/100, Training Loss: 1.4605\n",
      "Epoch 38/100, Training Loss: 1.4430\n",
      "Epoch 39/100, Training Loss: 1.4375\n",
      "Epoch 40/100, Training Loss: 1.4799\n",
      "Epoch 41/100, Training Loss: 1.4126\n",
      "Epoch 42/100, Training Loss: 1.4621\n",
      "Epoch 43/100, Training Loss: 1.4552\n",
      "Epoch 44/100, Training Loss: 1.4464\n",
      "Epoch 45/100, Training Loss: 1.4661\n",
      "Epoch 46/100, Training Loss: 1.4512\n",
      "Epoch 47/100, Training Loss: 1.4527\n",
      "Epoch 48/100, Training Loss: 1.4528\n",
      "Epoch 49/100, Training Loss: 1.4862\n",
      "Epoch 50/100, Training Loss: 1.4730\n",
      "Epoch 51/100, Training Loss: 1.4197\n",
      "Epoch 52/100, Training Loss: 1.4682\n",
      "Epoch 53/100, Training Loss: 1.4313\n",
      "Epoch 54/100, Training Loss: 1.4155\n",
      "Epoch 55/100, Training Loss: 1.4023\n",
      "Epoch 56/100, Training Loss: 1.4293\n",
      "Epoch 57/100, Training Loss: 1.4599\n",
      "Epoch 58/100, Training Loss: 1.4570\n",
      "Epoch 59/100, Training Loss: 1.4521\n",
      "Epoch 60/100, Training Loss: 1.4867\n",
      "Epoch 61/100, Training Loss: 1.4936\n",
      "Epoch 62/100, Training Loss: 1.4553\n",
      "Epoch 63/100, Training Loss: 1.4303\n",
      "Epoch 64/100, Training Loss: 1.4814\n",
      "Epoch 65/100, Training Loss: 1.4235\n",
      "Epoch 66/100, Training Loss: 1.4339\n",
      "Epoch 67/100, Training Loss: 1.4083\n",
      "Epoch 68/100, Training Loss: 1.4337\n",
      "Epoch 69/100, Training Loss: 1.4553\n",
      "Epoch 70/100, Training Loss: 1.4788\n",
      "Epoch 71/100, Training Loss: 1.4816\n",
      "Epoch 72/100, Training Loss: 1.5207\n",
      "Epoch 73/100, Training Loss: 1.4650\n",
      "Epoch 74/100, Training Loss: 1.4180\n",
      "Epoch 75/100, Training Loss: 1.4719\n",
      "Epoch 76/100, Training Loss: 1.4727\n",
      "Epoch 77/100, Training Loss: 1.4622\n",
      "Epoch 78/100, Training Loss: 1.4845\n",
      "Epoch 79/100, Training Loss: 1.4590\n",
      "Epoch 80/100, Training Loss: 1.4193\n",
      "Epoch 81/100, Training Loss: 1.4778\n",
      "Epoch 82/100, Training Loss: 1.4522\n",
      "Epoch 83/100, Training Loss: 1.4537\n",
      "Epoch 84/100, Training Loss: 1.5014\n",
      "Epoch 85/100, Training Loss: 1.4791\n",
      "Epoch 86/100, Training Loss: 1.4399\n",
      "Epoch 87/100, Training Loss: 1.4425\n",
      "Epoch 88/100, Training Loss: 1.4639\n",
      "Epoch 89/100, Training Loss: 1.4397\n",
      "Epoch 90/100, Training Loss: 1.4380\n",
      "Epoch 91/100, Training Loss: 1.4358\n",
      "Epoch 92/100, Training Loss: 1.4417\n",
      "Epoch 93/100, Training Loss: 1.4579\n",
      "Epoch 94/100, Training Loss: 1.4602\n",
      "Epoch 95/100, Training Loss: 1.4272\n",
      "Epoch 96/100, Training Loss: 1.4601\n",
      "Epoch 97/100, Training Loss: 1.4493\n",
      "Epoch 98/100, Training Loss: 1.4432\n",
      "Epoch 99/100, Training Loss: 1.4357\n",
      "Epoch 100/100, Training Loss: 1.4145\n",
      "Best overall model index: 0 with average validation loss 1.4593\n"
     ]
    }
   ],
   "source": [
    "# Training with L1\n",
    "loss= FlowLoss()\n",
    "final_model_l1, (best_index_l1, val_loss_l1) = validate(\n",
    "        models_laplace,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs=hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_l1} with average validation loss {val_loss_l1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0bb155-4222-4711-bae4-22c324ae7793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: -0.5418\n",
      "Epoch 2/100, Training Loss: -0.5472\n",
      "Epoch 3/100, Training Loss: -0.5720\n",
      "Epoch 4/100, Training Loss: -0.5525\n",
      "Epoch 5/100, Training Loss: -0.5360\n",
      "Epoch 6/100, Training Loss: -0.5446\n",
      "Epoch 7/100, Training Loss: -0.5683\n",
      "Epoch 8/100, Training Loss: -0.5473\n",
      "Epoch 9/100, Training Loss: -0.5424\n",
      "Epoch 10/100, Training Loss: -0.5376\n",
      "Epoch 11/100, Training Loss: -0.5637\n",
      "Epoch 12/100, Training Loss: -0.5566\n",
      "Epoch 13/100, Training Loss: -0.5452\n",
      "Epoch 14/100, Training Loss: -0.5445\n",
      "Epoch 15/100, Training Loss: -0.5534\n",
      "Epoch 16/100, Training Loss: -0.5652\n",
      "Epoch 17/100, Training Loss: -0.5590\n",
      "Epoch 18/100, Training Loss: -0.5445\n",
      "Epoch 19/100, Training Loss: -0.5489\n",
      "Epoch 20/100, Training Loss: -0.5600\n",
      "Epoch 21/100, Training Loss: -0.5519\n",
      "Epoch 22/100, Training Loss: -0.5523\n",
      "Epoch 23/100, Training Loss: -0.5604\n",
      "Epoch 24/100, Training Loss: -0.5493\n",
      "Epoch 25/100, Training Loss: -0.5648\n",
      "Epoch 26/100, Training Loss: -0.5391\n",
      "Epoch 27/100, Training Loss: -0.5404\n",
      "Epoch 28/100, Training Loss: -0.5320\n",
      "Epoch 29/100, Training Loss: -0.5703\n",
      "Epoch 30/100, Training Loss: -0.5644\n",
      "Epoch 31/100, Training Loss: -0.5715\n",
      "Epoch 32/100, Training Loss: -0.5592\n",
      "Epoch 33/100, Training Loss: -0.5599\n",
      "Epoch 34/100, Training Loss: -0.5679\n",
      "Epoch 35/100, Training Loss: -0.5289\n",
      "Epoch 36/100, Training Loss: -0.5515\n",
      "Epoch 37/100, Training Loss: -0.5588\n",
      "Epoch 38/100, Training Loss: -0.5757\n",
      "Epoch 39/100, Training Loss: -0.5509\n",
      "Epoch 40/100, Training Loss: -0.5607\n",
      "Epoch 41/100, Training Loss: -0.5600\n",
      "Epoch 42/100, Training Loss: -0.5504\n",
      "Epoch 43/100, Training Loss: -0.5372\n",
      "Epoch 44/100, Training Loss: -0.5562\n",
      "Epoch 45/100, Training Loss: -0.5451\n",
      "Epoch 46/100, Training Loss: -0.5549\n",
      "Epoch 47/100, Training Loss: -0.5523\n",
      "Epoch 48/100, Training Loss: -0.5381\n",
      "Epoch 49/100, Training Loss: -0.5703\n",
      "Epoch 50/100, Training Loss: -0.5592\n",
      "Epoch 51/100, Training Loss: -0.5582\n",
      "Epoch 52/100, Training Loss: -0.5581\n",
      "Epoch 53/100, Training Loss: -0.5537\n",
      "Epoch 54/100, Training Loss: -0.5451\n",
      "Epoch 55/100, Training Loss: -0.5487\n",
      "Epoch 56/100, Training Loss: -0.5673\n",
      "Epoch 57/100, Training Loss: -0.5532\n",
      "Epoch 58/100, Training Loss: -0.5561\n",
      "Epoch 59/100, Training Loss: -0.5826\n",
      "Epoch 60/100, Training Loss: -0.5567\n",
      "Epoch 61/100, Training Loss: -0.5547\n",
      "Epoch 62/100, Training Loss: -0.5372\n",
      "Epoch 63/100, Training Loss: -0.5397\n",
      "Epoch 64/100, Training Loss: -0.5426\n",
      "Epoch 65/100, Training Loss: -0.5492\n",
      "Epoch 66/100, Training Loss: -0.5530\n",
      "Epoch 67/100, Training Loss: -0.5364\n",
      "Epoch 68/100, Training Loss: -0.5594\n",
      "Epoch 69/100, Training Loss: -0.5433\n",
      "Epoch 70/100, Training Loss: -0.5606\n",
      "Epoch 71/100, Training Loss: -0.5434\n",
      "Epoch 72/100, Training Loss: -0.5671\n",
      "Epoch 73/100, Training Loss: -0.5772\n",
      "Epoch 74/100, Training Loss: -0.5431\n",
      "Epoch 75/100, Training Loss: -0.5478\n",
      "Epoch 76/100, Training Loss: -0.5598\n",
      "Epoch 77/100, Training Loss: -0.5649\n",
      "Epoch 78/100, Training Loss: -0.5445\n",
      "Epoch 79/100, Training Loss: -0.5635\n",
      "Epoch 80/100, Training Loss: -0.5667\n",
      "Epoch 81/100, Training Loss: -0.5574\n",
      "Epoch 82/100, Training Loss: -0.5716\n",
      "Epoch 83/100, Training Loss: -0.5494\n",
      "Epoch 84/100, Training Loss: -0.5417\n",
      "Epoch 85/100, Training Loss: -0.5722\n",
      "Epoch 86/100, Training Loss: -0.5438\n",
      "Epoch 87/100, Training Loss: -0.5171\n",
      "Epoch 88/100, Training Loss: -0.5491\n",
      "Epoch 89/100, Training Loss: -0.5466\n",
      "Epoch 90/100, Training Loss: -0.5748\n",
      "Epoch 91/100, Training Loss: -0.5791\n",
      "Epoch 92/100, Training Loss: -0.5620\n",
      "Epoch 93/100, Training Loss: -0.5276\n",
      "Epoch 94/100, Training Loss: -0.5549\n",
      "Epoch 95/100, Training Loss: -0.5421\n",
      "Epoch 96/100, Training Loss: -0.5472\n",
      "Epoch 97/100, Training Loss: -0.5345\n",
      "Epoch 98/100, Training Loss: -0.5735\n",
      "Epoch 99/100, Training Loss: -0.5487\n",
      "Epoch 100/100, Training Loss: -0.5532\n",
      "Validation Loss: -0.4488\n",
      "Model 0, Fold 0: Validation Loss = -0.4488\n",
      "Epoch 1/100, Training Loss: -0.5597\n",
      "Epoch 2/100, Training Loss: -0.5645\n",
      "Epoch 3/100, Training Loss: -0.5655\n",
      "Epoch 4/100, Training Loss: -0.5652\n",
      "Epoch 5/100, Training Loss: -0.5657\n",
      "Epoch 6/100, Training Loss: -0.5764\n",
      "Epoch 7/100, Training Loss: -0.5627\n",
      "Epoch 8/100, Training Loss: -0.5821\n",
      "Epoch 9/100, Training Loss: -0.5772\n",
      "Epoch 10/100, Training Loss: -0.5481\n",
      "Epoch 11/100, Training Loss: -0.5626\n",
      "Epoch 12/100, Training Loss: -0.5688\n",
      "Epoch 13/100, Training Loss: -0.5632\n",
      "Epoch 14/100, Training Loss: -0.5565\n",
      "Epoch 15/100, Training Loss: -0.5939\n",
      "Epoch 16/100, Training Loss: -0.5660\n",
      "Epoch 17/100, Training Loss: -0.5915\n",
      "Epoch 18/100, Training Loss: -0.5717\n",
      "Epoch 19/100, Training Loss: -0.5673\n",
      "Epoch 20/100, Training Loss: -0.5792\n",
      "Epoch 21/100, Training Loss: -0.5729\n",
      "Epoch 22/100, Training Loss: -0.5497\n",
      "Epoch 23/100, Training Loss: -0.5729\n",
      "Epoch 24/100, Training Loss: -0.5668\n",
      "Epoch 25/100, Training Loss: -0.5683\n",
      "Epoch 26/100, Training Loss: -0.5569\n",
      "Epoch 27/100, Training Loss: -0.5645\n",
      "Epoch 28/100, Training Loss: -0.5800\n",
      "Epoch 29/100, Training Loss: -0.5430\n",
      "Epoch 30/100, Training Loss: -0.5556\n",
      "Epoch 31/100, Training Loss: -0.5608\n",
      "Epoch 32/100, Training Loss: -0.5645\n",
      "Epoch 33/100, Training Loss: -0.5421\n",
      "Epoch 34/100, Training Loss: -0.5586\n",
      "Epoch 35/100, Training Loss: -0.5737\n",
      "Epoch 36/100, Training Loss: -0.5640\n",
      "Epoch 37/100, Training Loss: -0.5886\n",
      "Epoch 38/100, Training Loss: -0.5595\n",
      "Epoch 39/100, Training Loss: -0.5616\n",
      "Epoch 40/100, Training Loss: -0.5598\n",
      "Epoch 41/100, Training Loss: -0.5532\n",
      "Epoch 42/100, Training Loss: -0.5562\n",
      "Epoch 43/100, Training Loss: -0.5524\n",
      "Epoch 44/100, Training Loss: -0.5558\n",
      "Epoch 45/100, Training Loss: -0.5836\n",
      "Epoch 46/100, Training Loss: -0.5440\n",
      "Epoch 47/100, Training Loss: -0.5885\n",
      "Epoch 48/100, Training Loss: -0.5840\n",
      "Epoch 49/100, Training Loss: -0.5819\n",
      "Epoch 50/100, Training Loss: -0.5630\n",
      "Epoch 51/100, Training Loss: -0.5578\n",
      "Epoch 52/100, Training Loss: -0.5545\n",
      "Epoch 53/100, Training Loss: -0.5800\n",
      "Epoch 54/100, Training Loss: -0.5660\n",
      "Epoch 55/100, Training Loss: -0.5866\n",
      "Epoch 56/100, Training Loss: -0.5661\n",
      "Epoch 57/100, Training Loss: -0.5584\n",
      "Epoch 58/100, Training Loss: -0.5604\n",
      "Epoch 59/100, Training Loss: -0.5717\n",
      "Epoch 60/100, Training Loss: -0.5658\n",
      "Epoch 61/100, Training Loss: -0.5949\n",
      "Epoch 62/100, Training Loss: -0.5609\n",
      "Epoch 63/100, Training Loss: -0.5794\n",
      "Epoch 64/100, Training Loss: -0.5776\n",
      "Epoch 65/100, Training Loss: -0.5375\n",
      "Epoch 66/100, Training Loss: -0.5547\n",
      "Epoch 67/100, Training Loss: -0.5687\n",
      "Epoch 68/100, Training Loss: -0.5457\n",
      "Epoch 69/100, Training Loss: -0.5648\n",
      "Epoch 70/100, Training Loss: -0.5584\n",
      "Epoch 71/100, Training Loss: -0.5601\n",
      "Epoch 72/100, Training Loss: -0.5617\n",
      "Epoch 73/100, Training Loss: -0.5871\n",
      "Epoch 74/100, Training Loss: -0.5509\n",
      "Epoch 75/100, Training Loss: -0.5691\n",
      "Epoch 76/100, Training Loss: -0.5704\n",
      "Epoch 77/100, Training Loss: -0.5544\n",
      "Epoch 78/100, Training Loss: -0.5552\n",
      "Epoch 79/100, Training Loss: -0.5730\n",
      "Epoch 80/100, Training Loss: -0.5623\n",
      "Epoch 81/100, Training Loss: -0.5589\n",
      "Epoch 82/100, Training Loss: -0.5463\n",
      "Epoch 83/100, Training Loss: -0.5604\n",
      "Epoch 84/100, Training Loss: -0.5907\n",
      "Epoch 85/100, Training Loss: -0.5461\n",
      "Epoch 86/100, Training Loss: -0.5724\n",
      "Epoch 87/100, Training Loss: -0.5698\n",
      "Epoch 88/100, Training Loss: -0.5497\n",
      "Epoch 89/100, Training Loss: -0.5656\n",
      "Epoch 90/100, Training Loss: -0.5579\n",
      "Epoch 91/100, Training Loss: -0.5715\n",
      "Epoch 92/100, Training Loss: -0.5666\n",
      "Epoch 93/100, Training Loss: -0.5660\n",
      "Epoch 94/100, Training Loss: -0.5859\n",
      "Epoch 95/100, Training Loss: -0.5659\n",
      "Epoch 96/100, Training Loss: -0.5718\n",
      "Epoch 97/100, Training Loss: -0.5820\n",
      "Epoch 98/100, Training Loss: -0.5566\n",
      "Epoch 99/100, Training Loss: -0.5720\n",
      "Epoch 100/100, Training Loss: -0.5835\n",
      "Validation Loss: -0.4393\n",
      "Model 0, Fold 1: Validation Loss = -0.4393\n",
      "Epoch 1/100, Training Loss: -0.4845\n",
      "Epoch 2/100, Training Loss: -0.5336\n",
      "Epoch 3/100, Training Loss: -0.5449\n",
      "Epoch 4/100, Training Loss: -0.5799\n",
      "Epoch 5/100, Training Loss: -0.5587\n",
      "Epoch 6/100, Training Loss: -0.5429\n",
      "Epoch 7/100, Training Loss: -0.5459\n",
      "Epoch 8/100, Training Loss: -0.5515\n",
      "Epoch 9/100, Training Loss: -0.5421\n",
      "Epoch 10/100, Training Loss: -0.5632\n",
      "Epoch 11/100, Training Loss: -0.5526\n",
      "Epoch 12/100, Training Loss: -0.5575\n",
      "Epoch 13/100, Training Loss: -0.5269\n",
      "Epoch 14/100, Training Loss: -0.5422\n",
      "Epoch 15/100, Training Loss: -0.5488\n",
      "Epoch 16/100, Training Loss: -0.5522\n",
      "Epoch 17/100, Training Loss: -0.5486\n",
      "Epoch 18/100, Training Loss: -0.5675\n",
      "Epoch 19/100, Training Loss: -0.5498\n",
      "Epoch 20/100, Training Loss: -0.5499\n",
      "Epoch 21/100, Training Loss: -0.5553\n",
      "Epoch 22/100, Training Loss: -0.5541\n",
      "Epoch 23/100, Training Loss: -0.5571\n",
      "Epoch 24/100, Training Loss: -0.5609\n",
      "Epoch 25/100, Training Loss: -0.5515\n",
      "Epoch 26/100, Training Loss: -0.5509\n",
      "Epoch 27/100, Training Loss: -0.5500\n",
      "Epoch 28/100, Training Loss: -0.5436\n",
      "Epoch 29/100, Training Loss: -0.5590\n",
      "Epoch 30/100, Training Loss: -0.5442\n",
      "Epoch 31/100, Training Loss: -0.5622\n",
      "Epoch 32/100, Training Loss: -0.5449\n",
      "Epoch 33/100, Training Loss: -0.5860\n",
      "Epoch 34/100, Training Loss: -0.5388\n",
      "Epoch 35/100, Training Loss: -0.5441\n",
      "Epoch 36/100, Training Loss: -0.5459\n",
      "Epoch 37/100, Training Loss: -0.5506\n",
      "Epoch 38/100, Training Loss: -0.5682\n",
      "Epoch 39/100, Training Loss: -0.5817\n",
      "Epoch 40/100, Training Loss: -0.5629\n",
      "Epoch 41/100, Training Loss: -0.5687\n",
      "Epoch 42/100, Training Loss: -0.5435\n",
      "Epoch 43/100, Training Loss: -0.5625\n",
      "Epoch 44/100, Training Loss: -0.5663\n",
      "Epoch 45/100, Training Loss: -0.5594\n",
      "Epoch 46/100, Training Loss: -0.5629\n",
      "Epoch 47/100, Training Loss: -0.5725\n",
      "Epoch 48/100, Training Loss: -0.5537\n",
      "Epoch 49/100, Training Loss: -0.5454\n",
      "Epoch 50/100, Training Loss: -0.5629\n",
      "Epoch 51/100, Training Loss: -0.5359\n",
      "Epoch 52/100, Training Loss: -0.5430\n",
      "Epoch 53/100, Training Loss: -0.5330\n",
      "Epoch 54/100, Training Loss: -0.5372\n",
      "Epoch 55/100, Training Loss: -0.5551\n",
      "Epoch 56/100, Training Loss: -0.5605\n",
      "Epoch 57/100, Training Loss: -0.5696\n",
      "Epoch 58/100, Training Loss: -0.5490\n",
      "Epoch 59/100, Training Loss: -0.5641\n",
      "Epoch 60/100, Training Loss: -0.5475\n",
      "Epoch 61/100, Training Loss: -0.5471\n",
      "Epoch 62/100, Training Loss: -0.5619\n",
      "Epoch 63/100, Training Loss: -0.5494\n",
      "Epoch 64/100, Training Loss: -0.5711\n",
      "Epoch 65/100, Training Loss: -0.5462\n",
      "Epoch 66/100, Training Loss: -0.5688\n",
      "Epoch 67/100, Training Loss: -0.5769\n",
      "Epoch 68/100, Training Loss: -0.5507\n",
      "Epoch 69/100, Training Loss: -0.5627\n",
      "Epoch 70/100, Training Loss: -0.5606\n",
      "Epoch 71/100, Training Loss: -0.5583\n",
      "Epoch 72/100, Training Loss: -0.5559\n",
      "Epoch 73/100, Training Loss: -0.5385\n",
      "Epoch 74/100, Training Loss: -0.5675\n",
      "Epoch 75/100, Training Loss: -0.5712\n",
      "Epoch 76/100, Training Loss: -0.5390\n",
      "Epoch 77/100, Training Loss: -0.5591\n",
      "Epoch 78/100, Training Loss: -0.5557\n",
      "Epoch 79/100, Training Loss: -0.5660\n",
      "Epoch 80/100, Training Loss: -0.5559\n",
      "Epoch 81/100, Training Loss: -0.5450\n",
      "Epoch 82/100, Training Loss: -0.5608\n",
      "Epoch 83/100, Training Loss: -0.5460\n",
      "Epoch 84/100, Training Loss: -0.5620\n",
      "Epoch 85/100, Training Loss: -0.5564\n",
      "Epoch 86/100, Training Loss: -0.5499\n",
      "Epoch 87/100, Training Loss: -0.5511\n",
      "Epoch 88/100, Training Loss: -0.5653\n",
      "Epoch 89/100, Training Loss: -0.5595\n",
      "Epoch 90/100, Training Loss: -0.5576\n",
      "Epoch 91/100, Training Loss: -0.5470\n",
      "Epoch 92/100, Training Loss: -0.5575\n",
      "Epoch 93/100, Training Loss: -0.5573\n",
      "Epoch 94/100, Training Loss: -0.5522\n",
      "Epoch 95/100, Training Loss: -0.5526\n",
      "Epoch 96/100, Training Loss: -0.5741\n",
      "Epoch 97/100, Training Loss: -0.5942\n",
      "Epoch 98/100, Training Loss: -0.5423\n",
      "Epoch 99/100, Training Loss: -0.5762\n",
      "Epoch 100/100, Training Loss: -0.5517\n",
      "Validation Loss: -0.4475\n",
      "Model 1, Fold 0: Validation Loss = -0.4475\n",
      "Epoch 1/100, Training Loss: -0.4864\n",
      "Epoch 2/100, Training Loss: -0.5274\n",
      "Epoch 3/100, Training Loss: -0.5631\n",
      "Epoch 4/100, Training Loss: -0.5728\n",
      "Epoch 5/100, Training Loss: -0.5462\n",
      "Epoch 6/100, Training Loss: -0.5681\n",
      "Epoch 7/100, Training Loss: -0.5453\n",
      "Epoch 8/100, Training Loss: -0.5721\n",
      "Epoch 9/100, Training Loss: -0.5489\n",
      "Epoch 10/100, Training Loss: -0.5622\n",
      "Epoch 11/100, Training Loss: -0.5498\n",
      "Epoch 12/100, Training Loss: -0.5522\n",
      "Epoch 13/100, Training Loss: -0.5981\n",
      "Epoch 14/100, Training Loss: -0.5610\n",
      "Epoch 15/100, Training Loss: -0.5692\n",
      "Epoch 16/100, Training Loss: -0.5509\n",
      "Epoch 17/100, Training Loss: -0.5670\n",
      "Epoch 18/100, Training Loss: -0.5637\n",
      "Epoch 19/100, Training Loss: -0.5727\n",
      "Epoch 20/100, Training Loss: -0.5824\n",
      "Epoch 21/100, Training Loss: -0.5547\n",
      "Epoch 22/100, Training Loss: -0.5625\n",
      "Epoch 23/100, Training Loss: -0.5441\n",
      "Epoch 24/100, Training Loss: -0.5517\n",
      "Epoch 25/100, Training Loss: -0.5742\n",
      "Epoch 26/100, Training Loss: -0.5462\n",
      "Epoch 27/100, Training Loss: -0.5784\n",
      "Epoch 28/100, Training Loss: -0.5434\n",
      "Epoch 29/100, Training Loss: -0.5574\n",
      "Epoch 30/100, Training Loss: -0.5519\n",
      "Epoch 31/100, Training Loss: -0.5722\n",
      "Epoch 32/100, Training Loss: -0.5642\n",
      "Epoch 33/100, Training Loss: -0.5767\n",
      "Epoch 34/100, Training Loss: -0.6076\n",
      "Epoch 35/100, Training Loss: -0.5500\n",
      "Epoch 36/100, Training Loss: -0.5626\n",
      "Epoch 37/100, Training Loss: -0.5735\n",
      "Epoch 38/100, Training Loss: -0.5676\n",
      "Epoch 39/100, Training Loss: -0.5735\n",
      "Epoch 40/100, Training Loss: -0.5693\n",
      "Epoch 41/100, Training Loss: -0.5551\n",
      "Epoch 42/100, Training Loss: -0.5544\n",
      "Epoch 43/100, Training Loss: -0.5597\n",
      "Epoch 44/100, Training Loss: -0.5643\n",
      "Epoch 45/100, Training Loss: -0.5385\n",
      "Epoch 46/100, Training Loss: -0.5583\n",
      "Epoch 47/100, Training Loss: -0.5936\n",
      "Epoch 48/100, Training Loss: -0.5512\n",
      "Epoch 49/100, Training Loss: -0.5446\n",
      "Epoch 50/100, Training Loss: -0.5788\n",
      "Epoch 51/100, Training Loss: -0.5716\n",
      "Epoch 52/100, Training Loss: -0.5723\n",
      "Epoch 53/100, Training Loss: -0.5587\n",
      "Epoch 54/100, Training Loss: -0.5565\n",
      "Epoch 55/100, Training Loss: -0.5546\n",
      "Epoch 56/100, Training Loss: -0.5591\n",
      "Epoch 57/100, Training Loss: -0.5618\n",
      "Epoch 58/100, Training Loss: -0.5643\n",
      "Epoch 59/100, Training Loss: -0.5704\n",
      "Epoch 60/100, Training Loss: -0.5795\n",
      "Epoch 61/100, Training Loss: -0.5572\n",
      "Epoch 62/100, Training Loss: -0.5309\n",
      "Epoch 63/100, Training Loss: -0.5965\n",
      "Epoch 64/100, Training Loss: -0.5559\n",
      "Epoch 65/100, Training Loss: -0.5798\n",
      "Epoch 66/100, Training Loss: -0.5574\n",
      "Epoch 67/100, Training Loss: -0.5738\n",
      "Epoch 68/100, Training Loss: -0.5742\n",
      "Epoch 69/100, Training Loss: -0.5814\n",
      "Epoch 70/100, Training Loss: -0.5705\n",
      "Epoch 71/100, Training Loss: -0.5560\n",
      "Epoch 72/100, Training Loss: -0.5832\n",
      "Epoch 73/100, Training Loss: -0.5671\n",
      "Epoch 74/100, Training Loss: -0.5552\n",
      "Epoch 75/100, Training Loss: -0.5838\n",
      "Epoch 76/100, Training Loss: -0.5611\n",
      "Epoch 77/100, Training Loss: -0.5646\n",
      "Epoch 78/100, Training Loss: -0.5752\n",
      "Epoch 79/100, Training Loss: -0.5713\n",
      "Epoch 80/100, Training Loss: -0.5684\n",
      "Epoch 81/100, Training Loss: -0.5556\n",
      "Epoch 82/100, Training Loss: -0.5547\n",
      "Epoch 83/100, Training Loss: -0.5849\n",
      "Epoch 84/100, Training Loss: -0.5632\n",
      "Epoch 85/100, Training Loss: -0.5804\n",
      "Epoch 86/100, Training Loss: -0.5618\n",
      "Epoch 87/100, Training Loss: -0.5697\n",
      "Epoch 88/100, Training Loss: -0.5570\n",
      "Epoch 89/100, Training Loss: -0.5606\n",
      "Epoch 90/100, Training Loss: -0.5533\n",
      "Epoch 91/100, Training Loss: -0.5726\n",
      "Epoch 92/100, Training Loss: -0.5841\n",
      "Epoch 93/100, Training Loss: -0.5906\n",
      "Epoch 94/100, Training Loss: -0.5660\n",
      "Epoch 95/100, Training Loss: -0.5640\n",
      "Epoch 96/100, Training Loss: -0.5629\n",
      "Epoch 97/100, Training Loss: -0.5639\n",
      "Epoch 98/100, Training Loss: -0.5840\n",
      "Epoch 99/100, Training Loss: -0.5770\n",
      "Epoch 100/100, Training Loss: -0.5400\n",
      "Validation Loss: -0.4392\n",
      "Model 1, Fold 1: Validation Loss = -0.4392\n",
      "Epoch 1/100, Training Loss: -0.4614\n",
      "Epoch 2/100, Training Loss: -0.5290\n",
      "Epoch 3/100, Training Loss: -0.5505\n",
      "Epoch 4/100, Training Loss: -0.5467\n",
      "Epoch 5/100, Training Loss: -0.5302\n",
      "Epoch 6/100, Training Loss: -0.5400\n",
      "Epoch 7/100, Training Loss: -0.5506\n",
      "Epoch 8/100, Training Loss: -0.5327\n",
      "Epoch 9/100, Training Loss: -0.5501\n",
      "Epoch 10/100, Training Loss: -0.5450\n",
      "Epoch 11/100, Training Loss: -0.5386\n",
      "Epoch 12/100, Training Loss: -0.5736\n",
      "Epoch 13/100, Training Loss: -0.5626\n",
      "Epoch 14/100, Training Loss: -0.5644\n",
      "Epoch 15/100, Training Loss: -0.5664\n",
      "Epoch 16/100, Training Loss: -0.5779\n",
      "Epoch 17/100, Training Loss: -0.5317\n",
      "Epoch 18/100, Training Loss: -0.5409\n",
      "Epoch 19/100, Training Loss: -0.5598\n",
      "Epoch 20/100, Training Loss: -0.5282\n",
      "Epoch 21/100, Training Loss: -0.5640\n",
      "Epoch 22/100, Training Loss: -0.5454\n",
      "Epoch 23/100, Training Loss: -0.5543\n",
      "Epoch 24/100, Training Loss: -0.5501\n",
      "Epoch 25/100, Training Loss: -0.5669\n",
      "Epoch 26/100, Training Loss: -0.5603\n",
      "Epoch 27/100, Training Loss: -0.5486\n",
      "Epoch 28/100, Training Loss: -0.5681\n",
      "Epoch 29/100, Training Loss: -0.5533\n",
      "Epoch 30/100, Training Loss: -0.5483\n",
      "Epoch 31/100, Training Loss: -0.5531\n",
      "Epoch 32/100, Training Loss: -0.5322\n",
      "Epoch 33/100, Training Loss: -0.5688\n",
      "Epoch 34/100, Training Loss: -0.5706\n",
      "Epoch 35/100, Training Loss: -0.5484\n",
      "Epoch 36/100, Training Loss: -0.5643\n",
      "Epoch 37/100, Training Loss: -0.5585\n",
      "Epoch 38/100, Training Loss: -0.5561\n",
      "Epoch 39/100, Training Loss: -0.5450\n",
      "Epoch 40/100, Training Loss: -0.5990\n",
      "Epoch 41/100, Training Loss: -0.5448\n",
      "Epoch 42/100, Training Loss: -0.5462\n",
      "Epoch 43/100, Training Loss: -0.5610\n",
      "Epoch 44/100, Training Loss: -0.5538\n",
      "Epoch 45/100, Training Loss: -0.5507\n",
      "Epoch 46/100, Training Loss: -0.5602\n",
      "Epoch 47/100, Training Loss: -0.5668\n",
      "Epoch 48/100, Training Loss: -0.5634\n",
      "Epoch 49/100, Training Loss: -0.5503\n",
      "Epoch 50/100, Training Loss: -0.5357\n",
      "Epoch 51/100, Training Loss: -0.5515\n",
      "Epoch 52/100, Training Loss: -0.5478\n",
      "Epoch 53/100, Training Loss: -0.5477\n",
      "Epoch 54/100, Training Loss: -0.5772\n",
      "Epoch 55/100, Training Loss: -0.5706\n",
      "Epoch 56/100, Training Loss: -0.5702\n",
      "Epoch 57/100, Training Loss: -0.5498\n",
      "Epoch 58/100, Training Loss: -0.5344\n",
      "Epoch 59/100, Training Loss: -0.5403\n",
      "Epoch 60/100, Training Loss: -0.5480\n",
      "Epoch 61/100, Training Loss: -0.5785\n",
      "Epoch 62/100, Training Loss: -0.5521\n",
      "Epoch 63/100, Training Loss: -0.5490\n",
      "Epoch 64/100, Training Loss: -0.5341\n",
      "Epoch 65/100, Training Loss: -0.5526\n",
      "Epoch 66/100, Training Loss: -0.5405\n",
      "Epoch 67/100, Training Loss: -0.5486\n",
      "Epoch 68/100, Training Loss: -0.5574\n",
      "Epoch 69/100, Training Loss: -0.5810\n",
      "Epoch 70/100, Training Loss: -0.5657\n",
      "Epoch 71/100, Training Loss: -0.5308\n",
      "Epoch 72/100, Training Loss: -0.5710\n",
      "Epoch 73/100, Training Loss: -0.5537\n",
      "Epoch 74/100, Training Loss: -0.5573\n",
      "Epoch 75/100, Training Loss: -0.5549\n",
      "Epoch 76/100, Training Loss: -0.5563\n",
      "Epoch 77/100, Training Loss: -0.5632\n",
      "Epoch 78/100, Training Loss: -0.5499\n",
      "Epoch 79/100, Training Loss: -0.5559\n",
      "Epoch 80/100, Training Loss: -0.5514\n",
      "Epoch 81/100, Training Loss: -0.5533\n",
      "Epoch 82/100, Training Loss: -0.5611\n",
      "Epoch 83/100, Training Loss: -0.5651\n",
      "Epoch 84/100, Training Loss: -0.5504\n",
      "Epoch 85/100, Training Loss: -0.5607\n",
      "Epoch 86/100, Training Loss: -0.5761\n",
      "Epoch 87/100, Training Loss: -0.5503\n",
      "Epoch 88/100, Training Loss: -0.5444\n",
      "Epoch 89/100, Training Loss: -0.5568\n",
      "Epoch 90/100, Training Loss: -0.5470\n",
      "Epoch 91/100, Training Loss: -0.5359\n",
      "Epoch 92/100, Training Loss: -0.5581\n",
      "Epoch 93/100, Training Loss: -0.5455\n",
      "Epoch 94/100, Training Loss: -0.5626\n",
      "Epoch 95/100, Training Loss: -0.5622\n",
      "Epoch 96/100, Training Loss: -0.5643\n",
      "Epoch 97/100, Training Loss: -0.5247\n",
      "Epoch 98/100, Training Loss: -0.5589\n",
      "Epoch 99/100, Training Loss: -0.5739\n",
      "Epoch 100/100, Training Loss: -0.5388\n",
      "Validation Loss: -0.4455\n",
      "Model 2, Fold 0: Validation Loss = -0.4455\n",
      "Epoch 1/100, Training Loss: -0.4794\n",
      "Epoch 2/100, Training Loss: -0.5429\n",
      "Epoch 3/100, Training Loss: -0.5204\n",
      "Epoch 4/100, Training Loss: -0.5609\n",
      "Epoch 5/100, Training Loss: -0.5298\n",
      "Epoch 6/100, Training Loss: -0.5582\n",
      "Epoch 7/100, Training Loss: -0.5638\n",
      "Epoch 8/100, Training Loss: -0.5405\n",
      "Epoch 9/100, Training Loss: -0.5534\n",
      "Epoch 10/100, Training Loss: -0.5715\n",
      "Epoch 11/100, Training Loss: -0.5434\n",
      "Epoch 12/100, Training Loss: -0.5642\n",
      "Epoch 13/100, Training Loss: -0.5750\n",
      "Epoch 14/100, Training Loss: -0.5739\n",
      "Epoch 15/100, Training Loss: -0.5379\n",
      "Epoch 16/100, Training Loss: -0.5804\n",
      "Epoch 17/100, Training Loss: -0.5794\n",
      "Epoch 18/100, Training Loss: -0.5376\n",
      "Epoch 19/100, Training Loss: -0.5539\n",
      "Epoch 20/100, Training Loss: -0.5754\n",
      "Epoch 21/100, Training Loss: -0.5559\n",
      "Epoch 22/100, Training Loss: -0.5358\n",
      "Epoch 23/100, Training Loss: -0.5583\n",
      "Epoch 24/100, Training Loss: -0.5710\n",
      "Epoch 25/100, Training Loss: -0.5775\n",
      "Epoch 26/100, Training Loss: -0.5745\n",
      "Epoch 27/100, Training Loss: -0.5677\n",
      "Epoch 28/100, Training Loss: -0.5804\n",
      "Epoch 29/100, Training Loss: -0.5742\n",
      "Epoch 30/100, Training Loss: -0.5800\n",
      "Epoch 31/100, Training Loss: -0.5741\n",
      "Epoch 32/100, Training Loss: -0.5619\n",
      "Epoch 33/100, Training Loss: -0.5532\n",
      "Epoch 34/100, Training Loss: -0.5731\n",
      "Epoch 35/100, Training Loss: -0.5651\n",
      "Epoch 36/100, Training Loss: -0.5664\n",
      "Epoch 37/100, Training Loss: -0.5559\n",
      "Epoch 38/100, Training Loss: -0.5708\n",
      "Epoch 39/100, Training Loss: -0.5872\n",
      "Epoch 40/100, Training Loss: -0.5750\n",
      "Epoch 41/100, Training Loss: -0.5778\n",
      "Epoch 42/100, Training Loss: -0.5469\n",
      "Epoch 43/100, Training Loss: -0.5576\n",
      "Epoch 44/100, Training Loss: -0.5728\n",
      "Epoch 45/100, Training Loss: -0.5562\n",
      "Epoch 46/100, Training Loss: -0.5505\n",
      "Epoch 47/100, Training Loss: -0.5682\n",
      "Epoch 48/100, Training Loss: -0.5606\n",
      "Epoch 49/100, Training Loss: -0.5505\n",
      "Epoch 50/100, Training Loss: -0.5720\n",
      "Epoch 51/100, Training Loss: -0.5741\n",
      "Epoch 52/100, Training Loss: -0.5557\n",
      "Epoch 53/100, Training Loss: -0.5620\n",
      "Epoch 54/100, Training Loss: -0.5748\n",
      "Epoch 55/100, Training Loss: -0.5767\n",
      "Epoch 56/100, Training Loss: -0.5874\n",
      "Epoch 57/100, Training Loss: -0.5734\n",
      "Epoch 58/100, Training Loss: -0.5575\n",
      "Epoch 59/100, Training Loss: -0.5531\n",
      "Epoch 60/100, Training Loss: -0.5766\n",
      "Epoch 61/100, Training Loss: -0.5477\n",
      "Epoch 62/100, Training Loss: -0.5551\n",
      "Epoch 63/100, Training Loss: -0.5460\n",
      "Epoch 64/100, Training Loss: -0.5489\n",
      "Epoch 65/100, Training Loss: -0.5760\n",
      "Epoch 66/100, Training Loss: -0.5762\n",
      "Epoch 67/100, Training Loss: -0.5351\n",
      "Epoch 68/100, Training Loss: -0.5685\n",
      "Epoch 69/100, Training Loss: -0.5730\n",
      "Epoch 70/100, Training Loss: -0.5660\n",
      "Epoch 71/100, Training Loss: -0.5643\n",
      "Epoch 72/100, Training Loss: -0.5527\n",
      "Epoch 73/100, Training Loss: -0.5649\n",
      "Epoch 74/100, Training Loss: -0.5426\n",
      "Epoch 75/100, Training Loss: -0.5707\n",
      "Epoch 76/100, Training Loss: -0.5498\n",
      "Epoch 77/100, Training Loss: -0.5824\n",
      "Epoch 78/100, Training Loss: -0.5669\n",
      "Epoch 79/100, Training Loss: -0.5546\n",
      "Epoch 80/100, Training Loss: -0.5638\n",
      "Epoch 81/100, Training Loss: -0.5693\n",
      "Epoch 82/100, Training Loss: -0.5735\n",
      "Epoch 83/100, Training Loss: -0.5523\n",
      "Epoch 84/100, Training Loss: -0.5761\n",
      "Epoch 85/100, Training Loss: -0.5691\n",
      "Epoch 86/100, Training Loss: -0.5715\n",
      "Epoch 87/100, Training Loss: -0.5670\n",
      "Epoch 88/100, Training Loss: -0.5730\n",
      "Epoch 89/100, Training Loss: -0.5759\n",
      "Epoch 90/100, Training Loss: -0.5603\n",
      "Epoch 91/100, Training Loss: -0.5703\n",
      "Epoch 92/100, Training Loss: -0.5838\n",
      "Epoch 93/100, Training Loss: -0.5620\n",
      "Epoch 94/100, Training Loss: -0.5467\n",
      "Epoch 95/100, Training Loss: -0.5858\n",
      "Epoch 96/100, Training Loss: -0.5689\n",
      "Epoch 97/100, Training Loss: -0.5668\n",
      "Epoch 98/100, Training Loss: -0.5534\n",
      "Epoch 99/100, Training Loss: -0.5658\n",
      "Epoch 100/100, Training Loss: -0.5572\n",
      "Validation Loss: -0.4370\n",
      "Model 2, Fold 1: Validation Loss = -0.4370\n",
      "Epoch 1/100, Training Loss: -0.4631\n",
      "Epoch 2/100, Training Loss: -0.5243\n",
      "Epoch 3/100, Training Loss: -0.5366\n",
      "Epoch 4/100, Training Loss: -0.5374\n",
      "Epoch 5/100, Training Loss: -0.5425\n",
      "Epoch 6/100, Training Loss: -0.5418\n",
      "Epoch 7/100, Training Loss: -0.5510\n",
      "Epoch 8/100, Training Loss: -0.5616\n",
      "Epoch 9/100, Training Loss: -0.5442\n",
      "Epoch 10/100, Training Loss: -0.5469\n",
      "Epoch 11/100, Training Loss: -0.5339\n",
      "Epoch 12/100, Training Loss: -0.5663\n",
      "Epoch 13/100, Training Loss: -0.5599\n",
      "Epoch 14/100, Training Loss: -0.5652\n",
      "Epoch 15/100, Training Loss: -0.5669\n",
      "Epoch 16/100, Training Loss: -0.5593\n",
      "Epoch 17/100, Training Loss: -0.5522\n",
      "Epoch 18/100, Training Loss: -0.5602\n",
      "Epoch 19/100, Training Loss: -0.5247\n",
      "Epoch 20/100, Training Loss: -0.5689\n",
      "Epoch 21/100, Training Loss: -0.5467\n",
      "Epoch 22/100, Training Loss: -0.5506\n",
      "Epoch 23/100, Training Loss: -0.5421\n",
      "Epoch 24/100, Training Loss: -0.5499\n",
      "Epoch 25/100, Training Loss: -0.5447\n",
      "Epoch 26/100, Training Loss: -0.5435\n",
      "Epoch 27/100, Training Loss: -0.5585\n",
      "Epoch 28/100, Training Loss: -0.5587\n",
      "Epoch 29/100, Training Loss: -0.5675\n",
      "Epoch 30/100, Training Loss: -0.5402\n",
      "Epoch 31/100, Training Loss: -0.5536\n",
      "Epoch 32/100, Training Loss: -0.5385\n",
      "Epoch 33/100, Training Loss: -0.5586\n",
      "Epoch 34/100, Training Loss: -0.5542\n",
      "Epoch 35/100, Training Loss: -0.5528\n",
      "Epoch 36/100, Training Loss: -0.5656\n",
      "Epoch 37/100, Training Loss: -0.5593\n",
      "Epoch 38/100, Training Loss: -0.5598\n",
      "Epoch 39/100, Training Loss: -0.5351\n",
      "Epoch 40/100, Training Loss: -0.5522\n",
      "Epoch 41/100, Training Loss: -0.5413\n",
      "Epoch 42/100, Training Loss: -0.5551\n",
      "Epoch 43/100, Training Loss: -0.5859\n",
      "Epoch 44/100, Training Loss: -0.5440\n",
      "Epoch 45/100, Training Loss: -0.5745\n",
      "Epoch 46/100, Training Loss: -0.5644\n",
      "Epoch 47/100, Training Loss: -0.5583\n",
      "Epoch 48/100, Training Loss: -0.5394\n",
      "Epoch 49/100, Training Loss: -0.5508\n",
      "Epoch 50/100, Training Loss: -0.5440\n",
      "Epoch 51/100, Training Loss: -0.5639\n",
      "Epoch 52/100, Training Loss: -0.5678\n",
      "Epoch 53/100, Training Loss: -0.5403\n",
      "Epoch 54/100, Training Loss: -0.5480\n",
      "Epoch 55/100, Training Loss: -0.5431\n",
      "Epoch 56/100, Training Loss: -0.5590\n",
      "Epoch 57/100, Training Loss: -0.5542\n",
      "Epoch 58/100, Training Loss: -0.5519\n",
      "Epoch 59/100, Training Loss: -0.5538\n",
      "Epoch 60/100, Training Loss: -0.5679\n",
      "Epoch 61/100, Training Loss: -0.5606\n",
      "Epoch 62/100, Training Loss: -0.5786\n",
      "Epoch 63/100, Training Loss: -0.5686\n",
      "Epoch 64/100, Training Loss: -0.5411\n",
      "Epoch 65/100, Training Loss: -0.5658\n",
      "Epoch 66/100, Training Loss: -0.5488\n",
      "Epoch 67/100, Training Loss: -0.5516\n",
      "Epoch 68/100, Training Loss: -0.5650\n",
      "Epoch 69/100, Training Loss: -0.5595\n",
      "Epoch 70/100, Training Loss: -0.5341\n",
      "Epoch 71/100, Training Loss: -0.5438\n",
      "Epoch 72/100, Training Loss: -0.5394\n",
      "Epoch 73/100, Training Loss: -0.5266\n",
      "Epoch 74/100, Training Loss: -0.5554\n",
      "Epoch 75/100, Training Loss: -0.5548\n",
      "Epoch 76/100, Training Loss: -0.5627\n",
      "Epoch 77/100, Training Loss: -0.5386\n",
      "Epoch 78/100, Training Loss: -0.5474\n",
      "Epoch 79/100, Training Loss: -0.5384\n",
      "Epoch 80/100, Training Loss: -0.5481\n",
      "Epoch 81/100, Training Loss: -0.5679\n",
      "Epoch 82/100, Training Loss: -0.5584\n",
      "Epoch 83/100, Training Loss: -0.5605\n",
      "Epoch 84/100, Training Loss: -0.5568\n",
      "Epoch 85/100, Training Loss: -0.5447\n",
      "Epoch 86/100, Training Loss: -0.5651\n",
      "Epoch 87/100, Training Loss: -0.5434\n",
      "Epoch 88/100, Training Loss: -0.5633\n",
      "Epoch 89/100, Training Loss: -0.5490\n",
      "Epoch 90/100, Training Loss: -0.5582\n",
      "Epoch 91/100, Training Loss: -0.5767\n",
      "Epoch 92/100, Training Loss: -0.5457\n",
      "Epoch 93/100, Training Loss: -0.5544\n",
      "Epoch 94/100, Training Loss: -0.5683\n",
      "Epoch 95/100, Training Loss: -0.5437\n",
      "Epoch 96/100, Training Loss: -0.5456\n",
      "Epoch 97/100, Training Loss: -0.5529\n",
      "Epoch 98/100, Training Loss: -0.5534\n",
      "Epoch 99/100, Training Loss: -0.5805\n",
      "Epoch 100/100, Training Loss: -0.5513\n",
      "Validation Loss: -0.4416\n",
      "Model 3, Fold 0: Validation Loss = -0.4416\n",
      "Epoch 1/100, Training Loss: -0.5061\n",
      "Epoch 2/100, Training Loss: -0.5459\n",
      "Epoch 3/100, Training Loss: -0.5350\n",
      "Epoch 4/100, Training Loss: -0.5642\n",
      "Epoch 5/100, Training Loss: -0.5531\n",
      "Epoch 6/100, Training Loss: -0.5741\n",
      "Epoch 7/100, Training Loss: -0.5694\n",
      "Epoch 8/100, Training Loss: -0.5529\n",
      "Epoch 9/100, Training Loss: -0.5736\n",
      "Epoch 10/100, Training Loss: -0.5641\n",
      "Epoch 11/100, Training Loss: -0.5750\n",
      "Epoch 12/100, Training Loss: -0.5620\n",
      "Epoch 13/100, Training Loss: -0.5855\n",
      "Epoch 14/100, Training Loss: -0.5597\n",
      "Epoch 15/100, Training Loss: -0.5624\n",
      "Epoch 16/100, Training Loss: -0.5499\n",
      "Epoch 17/100, Training Loss: -0.5655\n",
      "Epoch 18/100, Training Loss: -0.5544\n",
      "Epoch 19/100, Training Loss: -0.5494\n",
      "Epoch 20/100, Training Loss: -0.5638\n",
      "Epoch 21/100, Training Loss: -0.5440\n",
      "Epoch 22/100, Training Loss: -0.5503\n",
      "Epoch 23/100, Training Loss: -0.5773\n",
      "Epoch 24/100, Training Loss: -0.5734\n",
      "Epoch 25/100, Training Loss: -0.5620\n",
      "Epoch 26/100, Training Loss: -0.5617\n",
      "Epoch 27/100, Training Loss: -0.5783\n",
      "Epoch 28/100, Training Loss: -0.5483\n",
      "Epoch 29/100, Training Loss: -0.5463\n",
      "Epoch 30/100, Training Loss: -0.5789\n",
      "Epoch 31/100, Training Loss: -0.5614\n",
      "Epoch 32/100, Training Loss: -0.5731\n",
      "Epoch 33/100, Training Loss: -0.5440\n",
      "Epoch 34/100, Training Loss: -0.5373\n",
      "Epoch 35/100, Training Loss: -0.5543\n",
      "Epoch 36/100, Training Loss: -0.5816\n",
      "Epoch 37/100, Training Loss: -0.5481\n",
      "Epoch 38/100, Training Loss: -0.5482\n",
      "Epoch 39/100, Training Loss: -0.5651\n",
      "Epoch 40/100, Training Loss: -0.5680\n",
      "Epoch 41/100, Training Loss: -0.5719\n",
      "Epoch 42/100, Training Loss: -0.5527\n",
      "Epoch 43/100, Training Loss: -0.5728\n",
      "Epoch 44/100, Training Loss: -0.5716\n",
      "Epoch 45/100, Training Loss: -0.5796\n",
      "Epoch 46/100, Training Loss: -0.5621\n",
      "Epoch 47/100, Training Loss: -0.5577\n",
      "Epoch 48/100, Training Loss: -0.5546\n",
      "Epoch 49/100, Training Loss: -0.5742\n",
      "Epoch 50/100, Training Loss: -0.5591\n",
      "Epoch 51/100, Training Loss: -0.5768\n",
      "Epoch 52/100, Training Loss: -0.5740\n",
      "Epoch 53/100, Training Loss: -0.5879\n",
      "Epoch 54/100, Training Loss: -0.5596\n",
      "Epoch 55/100, Training Loss: -0.5449\n",
      "Epoch 56/100, Training Loss: -0.5540\n",
      "Epoch 57/100, Training Loss: -0.5718\n",
      "Epoch 58/100, Training Loss: -0.5705\n",
      "Epoch 59/100, Training Loss: -0.5529\n",
      "Epoch 60/100, Training Loss: -0.5645\n",
      "Epoch 61/100, Training Loss: -0.5728\n",
      "Epoch 62/100, Training Loss: -0.5575\n",
      "Epoch 63/100, Training Loss: -0.5561\n",
      "Epoch 64/100, Training Loss: -0.5853\n",
      "Epoch 65/100, Training Loss: -0.5731\n",
      "Epoch 66/100, Training Loss: -0.5876\n",
      "Epoch 67/100, Training Loss: -0.5588\n",
      "Epoch 68/100, Training Loss: -0.5690\n",
      "Epoch 69/100, Training Loss: -0.5949\n",
      "Epoch 70/100, Training Loss: -0.5686\n",
      "Epoch 71/100, Training Loss: -0.5648\n",
      "Epoch 72/100, Training Loss: -0.5762\n",
      "Epoch 73/100, Training Loss: -0.5558\n",
      "Epoch 74/100, Training Loss: -0.5686\n",
      "Epoch 75/100, Training Loss: -0.5879\n",
      "Epoch 76/100, Training Loss: -0.5875\n",
      "Epoch 77/100, Training Loss: -0.5756\n",
      "Epoch 78/100, Training Loss: -0.5725\n",
      "Epoch 79/100, Training Loss: -0.5566\n",
      "Epoch 80/100, Training Loss: -0.5531\n",
      "Epoch 81/100, Training Loss: -0.5465\n",
      "Epoch 82/100, Training Loss: -0.5847\n",
      "Epoch 83/100, Training Loss: -0.5739\n",
      "Epoch 84/100, Training Loss: -0.5489\n",
      "Epoch 85/100, Training Loss: -0.5828\n",
      "Epoch 86/100, Training Loss: -0.5602\n",
      "Epoch 87/100, Training Loss: -0.5862\n",
      "Epoch 88/100, Training Loss: -0.5742\n",
      "Epoch 89/100, Training Loss: -0.5577\n",
      "Epoch 90/100, Training Loss: -0.5886\n",
      "Epoch 91/100, Training Loss: -0.5742\n",
      "Epoch 92/100, Training Loss: -0.5561\n",
      "Epoch 93/100, Training Loss: -0.5585\n",
      "Epoch 94/100, Training Loss: -0.5635\n",
      "Epoch 95/100, Training Loss: -0.5523\n",
      "Epoch 96/100, Training Loss: -0.5553\n",
      "Epoch 97/100, Training Loss: -0.5491\n",
      "Epoch 98/100, Training Loss: -0.5641\n",
      "Epoch 99/100, Training Loss: -0.5523\n",
      "Epoch 100/100, Training Loss: -0.5946\n",
      "Validation Loss: -0.4367\n",
      "Model 3, Fold 1: Validation Loss = -0.4367\n",
      "Best overall model index: 0 with average validation loss -0.4440\n",
      "Retraining best model on full dataset...\n",
      "Epoch 1/100, Training Loss: -0.5537\n",
      "Epoch 2/100, Training Loss: -0.5341\n",
      "Epoch 3/100, Training Loss: -0.5546\n",
      "Epoch 4/100, Training Loss: -0.5645\n",
      "Epoch 5/100, Training Loss: -0.5337\n",
      "Epoch 6/100, Training Loss: -0.5471\n",
      "Epoch 7/100, Training Loss: -0.5554\n",
      "Epoch 8/100, Training Loss: -0.5494\n",
      "Epoch 9/100, Training Loss: -0.5746\n",
      "Epoch 10/100, Training Loss: -0.5525\n",
      "Epoch 11/100, Training Loss: -0.5653\n",
      "Epoch 12/100, Training Loss: -0.5793\n",
      "Epoch 13/100, Training Loss: -0.5487\n",
      "Epoch 14/100, Training Loss: -0.5567\n",
      "Epoch 15/100, Training Loss: -0.5645\n",
      "Epoch 16/100, Training Loss: -0.5486\n",
      "Epoch 17/100, Training Loss: -0.5624\n",
      "Epoch 18/100, Training Loss: -0.5575\n",
      "Epoch 19/100, Training Loss: -0.5766\n",
      "Epoch 20/100, Training Loss: -0.5551\n",
      "Epoch 21/100, Training Loss: -0.5550\n",
      "Epoch 22/100, Training Loss: -0.5583\n",
      "Epoch 23/100, Training Loss: -0.5654\n",
      "Epoch 24/100, Training Loss: -0.5450\n",
      "Epoch 25/100, Training Loss: -0.5647\n",
      "Epoch 26/100, Training Loss: -0.5512\n",
      "Epoch 27/100, Training Loss: -0.5578\n",
      "Epoch 28/100, Training Loss: -0.5616\n",
      "Epoch 29/100, Training Loss: -0.5580\n",
      "Epoch 30/100, Training Loss: -0.5589\n",
      "Epoch 31/100, Training Loss: -0.5416\n",
      "Epoch 32/100, Training Loss: -0.5569\n",
      "Epoch 33/100, Training Loss: -0.5569\n",
      "Epoch 34/100, Training Loss: -0.5739\n",
      "Epoch 35/100, Training Loss: -0.5606\n",
      "Epoch 36/100, Training Loss: -0.5412\n",
      "Epoch 37/100, Training Loss: -0.5641\n",
      "Epoch 38/100, Training Loss: -0.5601\n",
      "Epoch 39/100, Training Loss: -0.5483\n",
      "Epoch 40/100, Training Loss: -0.5501\n",
      "Epoch 41/100, Training Loss: -0.5621\n",
      "Epoch 42/100, Training Loss: -0.5588\n",
      "Epoch 43/100, Training Loss: -0.5559\n",
      "Epoch 44/100, Training Loss: -0.5548\n",
      "Epoch 45/100, Training Loss: -0.5534\n",
      "Epoch 46/100, Training Loss: -0.5579\n",
      "Epoch 47/100, Training Loss: -0.5638\n",
      "Epoch 48/100, Training Loss: -0.5501\n",
      "Epoch 49/100, Training Loss: -0.5578\n",
      "Epoch 50/100, Training Loss: -0.5479\n",
      "Epoch 51/100, Training Loss: -0.5489\n",
      "Epoch 52/100, Training Loss: -0.5542\n",
      "Epoch 53/100, Training Loss: -0.5649\n",
      "Epoch 54/100, Training Loss: -0.5623\n",
      "Epoch 55/100, Training Loss: -0.5570\n",
      "Epoch 56/100, Training Loss: -0.5563\n",
      "Epoch 57/100, Training Loss: -0.5614\n",
      "Epoch 58/100, Training Loss: -0.5568\n",
      "Epoch 59/100, Training Loss: -0.5519\n",
      "Epoch 60/100, Training Loss: -0.5556\n",
      "Epoch 61/100, Training Loss: -0.5575\n",
      "Epoch 62/100, Training Loss: -0.5790\n",
      "Epoch 63/100, Training Loss: -0.5599\n",
      "Epoch 64/100, Training Loss: -0.5660\n",
      "Epoch 65/100, Training Loss: -0.5468\n",
      "Epoch 66/100, Training Loss: -0.5562\n",
      "Epoch 67/100, Training Loss: -0.5770\n",
      "Epoch 68/100, Training Loss: -0.5575\n",
      "Epoch 69/100, Training Loss: -0.5703\n",
      "Epoch 70/100, Training Loss: -0.5573\n",
      "Epoch 71/100, Training Loss: -0.5578\n",
      "Epoch 72/100, Training Loss: -0.5391\n",
      "Epoch 73/100, Training Loss: -0.5518\n",
      "Epoch 74/100, Training Loss: -0.5679\n",
      "Epoch 75/100, Training Loss: -0.5740\n",
      "Epoch 76/100, Training Loss: -0.5597\n",
      "Epoch 77/100, Training Loss: -0.5697\n",
      "Epoch 78/100, Training Loss: -0.5658\n",
      "Epoch 79/100, Training Loss: -0.5435\n",
      "Epoch 80/100, Training Loss: -0.5594\n",
      "Epoch 81/100, Training Loss: -0.5616\n",
      "Epoch 82/100, Training Loss: -0.5459\n",
      "Epoch 83/100, Training Loss: -0.5539\n",
      "Epoch 84/100, Training Loss: -0.5579\n",
      "Epoch 85/100, Training Loss: -0.5571\n",
      "Epoch 86/100, Training Loss: -0.5436\n",
      "Epoch 87/100, Training Loss: -0.5534\n",
      "Epoch 88/100, Training Loss: -0.5522\n",
      "Epoch 89/100, Training Loss: -0.5518\n",
      "Epoch 90/100, Training Loss: -0.5604\n",
      "Epoch 91/100, Training Loss: -0.5647\n",
      "Epoch 92/100, Training Loss: -0.5686\n",
      "Epoch 93/100, Training Loss: -0.5597\n",
      "Epoch 94/100, Training Loss: -0.5610\n",
      "Epoch 95/100, Training Loss: -0.5474\n",
      "Epoch 96/100, Training Loss: -0.5432\n",
      "Epoch 97/100, Training Loss: -0.5752\n",
      "Epoch 98/100, Training Loss: -0.5599\n",
      "Epoch 99/100, Training Loss: -0.5575\n",
      "Epoch 100/100, Training Loss: -0.5424\n",
      "Best overall model index: 0 with average validation loss -0.4440\n"
     ]
    }
   ],
   "source": [
    "# Training with cocycles\n",
    "kernel = [gaussian_kernel()] * 2\n",
    "loss_factory = CocycleLossFactory(kernel)\n",
    "loss= loss_factory.build_loss(\"CMMD_V\", X, Y, subsamples=10**4)\n",
    "final_model_cmmdv, (best_index_cmmdv, val_loss_cmmdv) = validate(\n",
    "        models,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs=hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_cmmdv} with average validation loss {val_loss_cmmdv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc52af31-6c25-40a5-b038-afec881b59a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: -0.5368\n",
      "Epoch 2/100, Training Loss: -0.5318\n",
      "Epoch 3/100, Training Loss: -0.5337\n",
      "Epoch 4/100, Training Loss: -0.5448\n",
      "Epoch 5/100, Training Loss: -0.5687\n",
      "Epoch 6/100, Training Loss: -0.5579\n",
      "Epoch 7/100, Training Loss: -0.5561\n",
      "Epoch 8/100, Training Loss: -0.5567\n",
      "Epoch 9/100, Training Loss: -0.5317\n",
      "Epoch 10/100, Training Loss: -0.5418\n",
      "Epoch 11/100, Training Loss: -0.5531\n",
      "Epoch 12/100, Training Loss: -0.5414\n",
      "Epoch 13/100, Training Loss: -0.5447\n",
      "Epoch 14/100, Training Loss: -0.5619\n",
      "Epoch 15/100, Training Loss: -0.5526\n",
      "Epoch 16/100, Training Loss: -0.5497\n",
      "Epoch 17/100, Training Loss: -0.5620\n",
      "Epoch 18/100, Training Loss: -0.5393\n",
      "Epoch 19/100, Training Loss: -0.5401\n",
      "Epoch 20/100, Training Loss: -0.5449\n",
      "Epoch 21/100, Training Loss: -0.5438\n",
      "Epoch 22/100, Training Loss: -0.5591\n",
      "Epoch 23/100, Training Loss: -0.5573\n",
      "Epoch 24/100, Training Loss: -0.5537\n",
      "Epoch 25/100, Training Loss: -0.5253\n",
      "Epoch 26/100, Training Loss: -0.5593\n",
      "Epoch 27/100, Training Loss: -0.5531\n",
      "Epoch 28/100, Training Loss: -0.5652\n",
      "Epoch 29/100, Training Loss: -0.5450\n",
      "Epoch 30/100, Training Loss: -0.5433\n",
      "Epoch 31/100, Training Loss: -0.5539\n",
      "Epoch 32/100, Training Loss: -0.5437\n",
      "Epoch 33/100, Training Loss: -0.5415\n",
      "Epoch 34/100, Training Loss: -0.5167\n",
      "Epoch 35/100, Training Loss: -0.5698\n",
      "Epoch 36/100, Training Loss: -0.5532\n",
      "Epoch 37/100, Training Loss: -0.5561\n",
      "Epoch 38/100, Training Loss: -0.5357\n",
      "Epoch 39/100, Training Loss: -0.5724\n",
      "Epoch 40/100, Training Loss: -0.5451\n",
      "Epoch 41/100, Training Loss: -0.5597\n",
      "Epoch 42/100, Training Loss: -0.5564\n",
      "Epoch 43/100, Training Loss: -0.5548\n",
      "Epoch 44/100, Training Loss: -0.5421\n",
      "Epoch 45/100, Training Loss: -0.5445\n",
      "Epoch 46/100, Training Loss: -0.5488\n",
      "Epoch 47/100, Training Loss: -0.5447\n",
      "Epoch 48/100, Training Loss: -0.5567\n",
      "Epoch 49/100, Training Loss: -0.5736\n",
      "Epoch 50/100, Training Loss: -0.5317\n",
      "Epoch 51/100, Training Loss: -0.5481\n",
      "Epoch 52/100, Training Loss: -0.5509\n",
      "Epoch 53/100, Training Loss: -0.5371\n",
      "Epoch 54/100, Training Loss: -0.5695\n",
      "Epoch 55/100, Training Loss: -0.5591\n",
      "Epoch 56/100, Training Loss: -0.5433\n",
      "Epoch 57/100, Training Loss: -0.5628\n",
      "Epoch 58/100, Training Loss: -0.5342\n",
      "Epoch 59/100, Training Loss: -0.5426\n",
      "Epoch 60/100, Training Loss: -0.5499\n",
      "Epoch 61/100, Training Loss: -0.5534\n",
      "Epoch 62/100, Training Loss: -0.5632\n",
      "Epoch 63/100, Training Loss: -0.5513\n",
      "Epoch 64/100, Training Loss: -0.5313\n",
      "Epoch 65/100, Training Loss: -0.5480\n",
      "Epoch 66/100, Training Loss: -0.5444\n",
      "Epoch 67/100, Training Loss: -0.5514\n",
      "Epoch 68/100, Training Loss: -0.5531\n",
      "Epoch 69/100, Training Loss: -0.5477\n",
      "Epoch 70/100, Training Loss: -0.5422\n",
      "Epoch 71/100, Training Loss: -0.5568\n",
      "Epoch 72/100, Training Loss: -0.5412\n",
      "Epoch 73/100, Training Loss: -0.5562\n",
      "Epoch 74/100, Training Loss: -0.5545\n",
      "Epoch 75/100, Training Loss: -0.5497\n",
      "Epoch 76/100, Training Loss: -0.5762\n",
      "Epoch 77/100, Training Loss: -0.5387\n",
      "Epoch 78/100, Training Loss: -0.5484\n",
      "Epoch 79/100, Training Loss: -0.5266\n",
      "Epoch 80/100, Training Loss: -0.5461\n",
      "Epoch 81/100, Training Loss: -0.5418\n",
      "Epoch 82/100, Training Loss: -0.5382\n",
      "Epoch 83/100, Training Loss: -0.5649\n",
      "Epoch 84/100, Training Loss: -0.5426\n",
      "Epoch 85/100, Training Loss: -0.5527\n",
      "Epoch 86/100, Training Loss: -0.5470\n",
      "Epoch 87/100, Training Loss: -0.5762\n",
      "Epoch 88/100, Training Loss: -0.5468\n",
      "Epoch 89/100, Training Loss: -0.5559\n",
      "Epoch 90/100, Training Loss: -0.5717\n",
      "Epoch 91/100, Training Loss: -0.5456\n",
      "Epoch 92/100, Training Loss: -0.5510\n",
      "Epoch 93/100, Training Loss: -0.5522\n",
      "Epoch 94/100, Training Loss: -0.5764\n",
      "Epoch 95/100, Training Loss: -0.5335\n",
      "Epoch 96/100, Training Loss: -0.5470\n",
      "Epoch 97/100, Training Loss: -0.5306\n",
      "Epoch 98/100, Training Loss: -0.5283\n",
      "Epoch 99/100, Training Loss: -0.5623\n",
      "Epoch 100/100, Training Loss: -0.5471\n",
      "Validation Loss: -0.5594\n",
      "Model 0, Fold 0: Validation Loss = -0.5594\n",
      "Epoch 1/100, Training Loss: -0.5505\n",
      "Epoch 2/100, Training Loss: -0.5554\n",
      "Epoch 3/100, Training Loss: -0.5448\n",
      "Epoch 4/100, Training Loss: -0.5612\n",
      "Epoch 5/100, Training Loss: -0.5504\n",
      "Epoch 6/100, Training Loss: -0.5593\n",
      "Epoch 7/100, Training Loss: -0.5664\n",
      "Epoch 8/100, Training Loss: -0.5610\n",
      "Epoch 9/100, Training Loss: -0.5769\n",
      "Epoch 10/100, Training Loss: -0.5863\n",
      "Epoch 11/100, Training Loss: -0.5819\n",
      "Epoch 12/100, Training Loss: -0.5533\n",
      "Epoch 13/100, Training Loss: -0.5491\n",
      "Epoch 14/100, Training Loss: -0.5603\n",
      "Epoch 15/100, Training Loss: -0.5674\n",
      "Epoch 16/100, Training Loss: -0.5780\n",
      "Epoch 17/100, Training Loss: -0.5436\n",
      "Epoch 18/100, Training Loss: -0.5450\n",
      "Epoch 19/100, Training Loss: -0.5494\n",
      "Epoch 20/100, Training Loss: -0.5578\n",
      "Epoch 21/100, Training Loss: -0.5607\n",
      "Epoch 22/100, Training Loss: -0.5823\n",
      "Epoch 23/100, Training Loss: -0.5612\n",
      "Epoch 24/100, Training Loss: -0.5532\n",
      "Epoch 25/100, Training Loss: -0.5629\n",
      "Epoch 26/100, Training Loss: -0.5812\n",
      "Epoch 27/100, Training Loss: -0.5537\n",
      "Epoch 28/100, Training Loss: -0.5576\n",
      "Epoch 29/100, Training Loss: -0.5444\n",
      "Epoch 30/100, Training Loss: -0.5845\n",
      "Epoch 31/100, Training Loss: -0.5661\n",
      "Epoch 32/100, Training Loss: -0.5575\n",
      "Epoch 33/100, Training Loss: -0.5705\n",
      "Epoch 34/100, Training Loss: -0.5637\n",
      "Epoch 35/100, Training Loss: -0.5816\n",
      "Epoch 36/100, Training Loss: -0.5565\n",
      "Epoch 37/100, Training Loss: -0.5670\n",
      "Epoch 38/100, Training Loss: -0.5521\n",
      "Epoch 39/100, Training Loss: -0.5654\n",
      "Epoch 40/100, Training Loss: -0.5414\n",
      "Epoch 41/100, Training Loss: -0.5616\n",
      "Epoch 42/100, Training Loss: -0.5473\n",
      "Epoch 43/100, Training Loss: -0.5636\n",
      "Epoch 44/100, Training Loss: -0.5745\n",
      "Epoch 45/100, Training Loss: -0.5484\n",
      "Epoch 46/100, Training Loss: -0.5662\n",
      "Epoch 47/100, Training Loss: -0.5709\n",
      "Epoch 48/100, Training Loss: -0.5461\n",
      "Epoch 49/100, Training Loss: -0.5482\n",
      "Epoch 50/100, Training Loss: -0.5683\n",
      "Epoch 51/100, Training Loss: -0.5569\n",
      "Epoch 52/100, Training Loss: -0.5586\n",
      "Epoch 53/100, Training Loss: -0.5774\n",
      "Epoch 54/100, Training Loss: -0.5760\n",
      "Epoch 55/100, Training Loss: -0.5620\n",
      "Epoch 56/100, Training Loss: -0.5740\n",
      "Epoch 57/100, Training Loss: -0.5757\n",
      "Epoch 58/100, Training Loss: -0.5702\n",
      "Epoch 59/100, Training Loss: -0.5653\n",
      "Epoch 60/100, Training Loss: -0.5597\n",
      "Epoch 61/100, Training Loss: -0.5657\n",
      "Epoch 62/100, Training Loss: -0.5673\n",
      "Epoch 63/100, Training Loss: -0.5847\n",
      "Epoch 64/100, Training Loss: -0.5684\n",
      "Epoch 65/100, Training Loss: -0.5428\n",
      "Epoch 66/100, Training Loss: -0.5743\n",
      "Epoch 67/100, Training Loss: -0.5517\n",
      "Epoch 68/100, Training Loss: -0.5694\n",
      "Epoch 69/100, Training Loss: -0.5756\n",
      "Epoch 70/100, Training Loss: -0.5449\n",
      "Epoch 71/100, Training Loss: -0.5607\n",
      "Epoch 72/100, Training Loss: -0.5688\n",
      "Epoch 73/100, Training Loss: -0.5468\n",
      "Epoch 74/100, Training Loss: -0.5389\n",
      "Epoch 75/100, Training Loss: -0.5481\n",
      "Epoch 76/100, Training Loss: -0.5468\n",
      "Epoch 77/100, Training Loss: -0.5669\n",
      "Epoch 78/100, Training Loss: -0.5550\n",
      "Epoch 79/100, Training Loss: -0.5648\n",
      "Epoch 80/100, Training Loss: -0.5603\n",
      "Epoch 81/100, Training Loss: -0.5508\n",
      "Epoch 82/100, Training Loss: -0.5544\n",
      "Epoch 83/100, Training Loss: -0.5572\n",
      "Epoch 84/100, Training Loss: -0.5556\n",
      "Epoch 85/100, Training Loss: -0.5830\n",
      "Epoch 86/100, Training Loss: -0.5554\n",
      "Epoch 87/100, Training Loss: -0.5404\n",
      "Epoch 88/100, Training Loss: -0.5581\n",
      "Epoch 89/100, Training Loss: -0.5694\n",
      "Epoch 90/100, Training Loss: -0.5457\n",
      "Epoch 91/100, Training Loss: -0.5697\n",
      "Epoch 92/100, Training Loss: -0.5584\n",
      "Epoch 93/100, Training Loss: -0.5663\n",
      "Epoch 94/100, Training Loss: -0.5586\n",
      "Epoch 95/100, Training Loss: -0.5556\n",
      "Epoch 96/100, Training Loss: -0.5742\n",
      "Epoch 97/100, Training Loss: -0.5627\n",
      "Epoch 98/100, Training Loss: -0.5540\n",
      "Epoch 99/100, Training Loss: -0.5635\n",
      "Epoch 100/100, Training Loss: -0.5817\n",
      "Validation Loss: -0.5484\n",
      "Model 0, Fold 1: Validation Loss = -0.5484\n",
      "Epoch 1/100, Training Loss: -0.4783\n",
      "Epoch 2/100, Training Loss: -0.5315\n",
      "Epoch 3/100, Training Loss: -0.5517\n",
      "Epoch 4/100, Training Loss: -0.5452\n",
      "Epoch 5/100, Training Loss: -0.5611\n",
      "Epoch 6/100, Training Loss: -0.5422\n",
      "Epoch 7/100, Training Loss: -0.5328\n",
      "Epoch 8/100, Training Loss: -0.5478\n",
      "Epoch 9/100, Training Loss: -0.5499\n",
      "Epoch 10/100, Training Loss: -0.5456\n",
      "Epoch 11/100, Training Loss: -0.5503\n",
      "Epoch 12/100, Training Loss: -0.5550\n",
      "Epoch 13/100, Training Loss: -0.5560\n",
      "Epoch 14/100, Training Loss: -0.5458\n",
      "Epoch 15/100, Training Loss: -0.5601\n",
      "Epoch 16/100, Training Loss: -0.5601\n",
      "Epoch 17/100, Training Loss: -0.5430\n",
      "Epoch 18/100, Training Loss: -0.5416\n",
      "Epoch 19/100, Training Loss: -0.5644\n",
      "Epoch 20/100, Training Loss: -0.5473\n",
      "Epoch 21/100, Training Loss: -0.5526\n",
      "Epoch 22/100, Training Loss: -0.5688\n",
      "Epoch 23/100, Training Loss: -0.5641\n",
      "Epoch 24/100, Training Loss: -0.5474\n",
      "Epoch 25/100, Training Loss: -0.5410\n",
      "Epoch 26/100, Training Loss: -0.5468\n",
      "Epoch 27/100, Training Loss: -0.5704\n",
      "Epoch 28/100, Training Loss: -0.5567\n",
      "Epoch 29/100, Training Loss: -0.5707\n",
      "Epoch 30/100, Training Loss: -0.5540\n",
      "Epoch 31/100, Training Loss: -0.5653\n",
      "Epoch 32/100, Training Loss: -0.5622\n",
      "Epoch 33/100, Training Loss: -0.5660\n",
      "Epoch 34/100, Training Loss: -0.5421\n",
      "Epoch 35/100, Training Loss: -0.5480\n",
      "Epoch 36/100, Training Loss: -0.5550\n",
      "Epoch 37/100, Training Loss: -0.5446\n",
      "Epoch 38/100, Training Loss: -0.5367\n",
      "Epoch 39/100, Training Loss: -0.5694\n",
      "Epoch 40/100, Training Loss: -0.5643\n",
      "Epoch 41/100, Training Loss: -0.5613\n",
      "Epoch 42/100, Training Loss: -0.5526\n",
      "Epoch 43/100, Training Loss: -0.5409\n",
      "Epoch 44/100, Training Loss: -0.5670\n",
      "Epoch 45/100, Training Loss: -0.5506\n",
      "Epoch 46/100, Training Loss: -0.5281\n",
      "Epoch 47/100, Training Loss: -0.5603\n",
      "Epoch 48/100, Training Loss: -0.5411\n",
      "Epoch 49/100, Training Loss: -0.5555\n",
      "Epoch 50/100, Training Loss: -0.5696\n",
      "Epoch 51/100, Training Loss: -0.5693\n",
      "Epoch 52/100, Training Loss: -0.5545\n",
      "Epoch 53/100, Training Loss: -0.5433\n",
      "Epoch 54/100, Training Loss: -0.5434\n",
      "Epoch 55/100, Training Loss: -0.5536\n",
      "Epoch 56/100, Training Loss: -0.5492\n",
      "Epoch 57/100, Training Loss: -0.5408\n",
      "Epoch 58/100, Training Loss: -0.5189\n",
      "Epoch 59/100, Training Loss: -0.5773\n",
      "Epoch 60/100, Training Loss: -0.5528\n",
      "Epoch 61/100, Training Loss: -0.5522\n",
      "Epoch 62/100, Training Loss: -0.5381\n",
      "Epoch 63/100, Training Loss: -0.5518\n",
      "Epoch 64/100, Training Loss: -0.5461\n",
      "Epoch 65/100, Training Loss: -0.5497\n",
      "Epoch 66/100, Training Loss: -0.5642\n",
      "Epoch 67/100, Training Loss: -0.5516\n",
      "Epoch 68/100, Training Loss: -0.5647\n",
      "Epoch 69/100, Training Loss: -0.5275\n",
      "Epoch 70/100, Training Loss: -0.5242\n",
      "Epoch 71/100, Training Loss: -0.5401\n",
      "Epoch 72/100, Training Loss: -0.5390\n",
      "Epoch 73/100, Training Loss: -0.5676\n",
      "Epoch 74/100, Training Loss: -0.5723\n",
      "Epoch 75/100, Training Loss: -0.5459\n",
      "Epoch 76/100, Training Loss: -0.5508\n",
      "Epoch 77/100, Training Loss: -0.5459\n",
      "Epoch 78/100, Training Loss: -0.5481\n",
      "Epoch 79/100, Training Loss: -0.5656\n",
      "Epoch 80/100, Training Loss: -0.5570\n",
      "Epoch 81/100, Training Loss: -0.5602\n",
      "Epoch 82/100, Training Loss: -0.5545\n",
      "Epoch 83/100, Training Loss: -0.5702\n",
      "Epoch 84/100, Training Loss: -0.5860\n",
      "Epoch 85/100, Training Loss: -0.5521\n",
      "Epoch 86/100, Training Loss: -0.5287\n",
      "Epoch 87/100, Training Loss: -0.5539\n",
      "Epoch 88/100, Training Loss: -0.5668\n",
      "Epoch 89/100, Training Loss: -0.5403\n",
      "Epoch 90/100, Training Loss: -0.5360\n",
      "Epoch 91/100, Training Loss: -0.5520\n",
      "Epoch 92/100, Training Loss: -0.5504\n",
      "Epoch 93/100, Training Loss: -0.5612\n",
      "Epoch 94/100, Training Loss: -0.5403\n",
      "Epoch 95/100, Training Loss: -0.5398\n",
      "Epoch 96/100, Training Loss: -0.5395\n",
      "Epoch 97/100, Training Loss: -0.5344\n",
      "Epoch 98/100, Training Loss: -0.5837\n",
      "Epoch 99/100, Training Loss: -0.5313\n",
      "Epoch 100/100, Training Loss: -0.5664\n",
      "Validation Loss: -0.5568\n",
      "Model 1, Fold 0: Validation Loss = -0.5568\n",
      "Epoch 1/100, Training Loss: -0.5024\n",
      "Epoch 2/100, Training Loss: -0.5544\n",
      "Epoch 3/100, Training Loss: -0.5753\n",
      "Epoch 4/100, Training Loss: -0.5567\n",
      "Epoch 5/100, Training Loss: -0.5642\n",
      "Epoch 6/100, Training Loss: -0.5481\n",
      "Epoch 7/100, Training Loss: -0.5229\n",
      "Epoch 8/100, Training Loss: -0.5533\n",
      "Epoch 9/100, Training Loss: -0.5547\n",
      "Epoch 10/100, Training Loss: -0.5604\n",
      "Epoch 11/100, Training Loss: -0.5427\n",
      "Epoch 12/100, Training Loss: -0.5538\n",
      "Epoch 13/100, Training Loss: -0.5826\n",
      "Epoch 14/100, Training Loss: -0.5644\n",
      "Epoch 15/100, Training Loss: -0.5657\n",
      "Epoch 16/100, Training Loss: -0.5793\n",
      "Epoch 17/100, Training Loss: -0.5778\n",
      "Epoch 18/100, Training Loss: -0.5569\n",
      "Epoch 19/100, Training Loss: -0.5609\n",
      "Epoch 20/100, Training Loss: -0.5372\n",
      "Epoch 21/100, Training Loss: -0.5650\n",
      "Epoch 22/100, Training Loss: -0.5507\n",
      "Epoch 23/100, Training Loss: -0.5676\n",
      "Epoch 24/100, Training Loss: -0.5750\n",
      "Epoch 25/100, Training Loss: -0.5694\n",
      "Epoch 26/100, Training Loss: -0.5589\n",
      "Epoch 27/100, Training Loss: -0.5452\n",
      "Epoch 28/100, Training Loss: -0.5704\n",
      "Epoch 29/100, Training Loss: -0.5556\n",
      "Epoch 30/100, Training Loss: -0.5797\n",
      "Epoch 31/100, Training Loss: -0.5658\n",
      "Epoch 32/100, Training Loss: -0.5604\n",
      "Epoch 33/100, Training Loss: -0.5653\n",
      "Epoch 34/100, Training Loss: -0.5693\n",
      "Epoch 35/100, Training Loss: -0.5705\n",
      "Epoch 36/100, Training Loss: -0.5479\n",
      "Epoch 37/100, Training Loss: -0.5630\n",
      "Epoch 38/100, Training Loss: -0.5578\n",
      "Epoch 39/100, Training Loss: -0.5574\n",
      "Epoch 40/100, Training Loss: -0.5775\n",
      "Epoch 41/100, Training Loss: -0.5716\n",
      "Epoch 42/100, Training Loss: -0.5707\n",
      "Epoch 43/100, Training Loss: -0.5627\n",
      "Epoch 44/100, Training Loss: -0.5616\n",
      "Epoch 45/100, Training Loss: -0.5580\n",
      "Epoch 46/100, Training Loss: -0.5340\n",
      "Epoch 47/100, Training Loss: -0.5808\n",
      "Epoch 48/100, Training Loss: -0.6039\n",
      "Epoch 49/100, Training Loss: -0.5314\n",
      "Epoch 50/100, Training Loss: -0.5669\n",
      "Epoch 51/100, Training Loss: -0.5457\n",
      "Epoch 52/100, Training Loss: -0.5621\n",
      "Epoch 53/100, Training Loss: -0.5638\n",
      "Epoch 54/100, Training Loss: -0.5531\n",
      "Epoch 55/100, Training Loss: -0.5601\n",
      "Epoch 56/100, Training Loss: -0.5599\n",
      "Epoch 57/100, Training Loss: -0.5655\n",
      "Epoch 58/100, Training Loss: -0.5462\n",
      "Epoch 59/100, Training Loss: -0.5610\n",
      "Epoch 60/100, Training Loss: -0.5821\n",
      "Epoch 61/100, Training Loss: -0.5560\n",
      "Epoch 62/100, Training Loss: -0.5665\n",
      "Epoch 63/100, Training Loss: -0.5609\n",
      "Epoch 64/100, Training Loss: -0.5672\n",
      "Epoch 65/100, Training Loss: -0.5828\n",
      "Epoch 66/100, Training Loss: -0.5725\n",
      "Epoch 67/100, Training Loss: -0.5781\n",
      "Epoch 68/100, Training Loss: -0.5529\n",
      "Epoch 69/100, Training Loss: -0.5681\n",
      "Epoch 70/100, Training Loss: -0.5530\n",
      "Epoch 71/100, Training Loss: -0.5676\n",
      "Epoch 72/100, Training Loss: -0.5511\n",
      "Epoch 73/100, Training Loss: -0.5590\n",
      "Epoch 74/100, Training Loss: -0.5590\n",
      "Epoch 75/100, Training Loss: -0.5667\n",
      "Epoch 76/100, Training Loss: -0.5620\n",
      "Epoch 77/100, Training Loss: -0.5626\n",
      "Epoch 78/100, Training Loss: -0.5512\n",
      "Epoch 79/100, Training Loss: -0.5714\n",
      "Epoch 80/100, Training Loss: -0.5584\n",
      "Epoch 81/100, Training Loss: -0.5717\n",
      "Epoch 82/100, Training Loss: -0.5558\n",
      "Epoch 83/100, Training Loss: -0.5543\n",
      "Epoch 84/100, Training Loss: -0.5518\n",
      "Epoch 85/100, Training Loss: -0.5725\n",
      "Epoch 86/100, Training Loss: -0.5484\n",
      "Epoch 87/100, Training Loss: -0.5602\n",
      "Epoch 88/100, Training Loss: -0.5604\n",
      "Epoch 89/100, Training Loss: -0.5677\n",
      "Epoch 90/100, Training Loss: -0.5571\n",
      "Epoch 91/100, Training Loss: -0.5432\n",
      "Epoch 92/100, Training Loss: -0.5680\n",
      "Epoch 93/100, Training Loss: -0.5695\n",
      "Epoch 94/100, Training Loss: -0.5606\n",
      "Epoch 95/100, Training Loss: -0.5777\n",
      "Epoch 96/100, Training Loss: -0.5593\n",
      "Epoch 97/100, Training Loss: -0.5729\n",
      "Epoch 98/100, Training Loss: -0.5595\n",
      "Epoch 99/100, Training Loss: -0.5707\n",
      "Epoch 100/100, Training Loss: -0.5624\n",
      "Validation Loss: -0.5456\n",
      "Model 1, Fold 1: Validation Loss = -0.5456\n",
      "Epoch 1/100, Training Loss: -0.4649\n",
      "Epoch 2/100, Training Loss: -0.5389\n",
      "Epoch 3/100, Training Loss: -0.5314\n",
      "Epoch 4/100, Training Loss: -0.5310\n",
      "Epoch 5/100, Training Loss: -0.5271\n",
      "Epoch 6/100, Training Loss: -0.5415\n",
      "Epoch 7/100, Training Loss: -0.5686\n",
      "Epoch 8/100, Training Loss: -0.5485\n",
      "Epoch 9/100, Training Loss: -0.5617\n",
      "Epoch 10/100, Training Loss: -0.5485\n",
      "Epoch 11/100, Training Loss: -0.5488\n",
      "Epoch 12/100, Training Loss: -0.5755\n",
      "Epoch 13/100, Training Loss: -0.5270\n",
      "Epoch 14/100, Training Loss: -0.5505\n",
      "Epoch 15/100, Training Loss: -0.5441\n",
      "Epoch 16/100, Training Loss: -0.5407\n",
      "Epoch 17/100, Training Loss: -0.5471\n",
      "Epoch 18/100, Training Loss: -0.5658\n",
      "Epoch 19/100, Training Loss: -0.5429\n",
      "Epoch 20/100, Training Loss: -0.5453\n",
      "Epoch 21/100, Training Loss: -0.5338\n",
      "Epoch 22/100, Training Loss: -0.5456\n",
      "Epoch 23/100, Training Loss: -0.5463\n",
      "Epoch 24/100, Training Loss: -0.5674\n",
      "Epoch 25/100, Training Loss: -0.5523\n",
      "Epoch 26/100, Training Loss: -0.5349\n",
      "Epoch 27/100, Training Loss: -0.5478\n",
      "Epoch 28/100, Training Loss: -0.5410\n",
      "Epoch 29/100, Training Loss: -0.5277\n",
      "Epoch 30/100, Training Loss: -0.5639\n",
      "Epoch 31/100, Training Loss: -0.5614\n",
      "Epoch 32/100, Training Loss: -0.5695\n",
      "Epoch 33/100, Training Loss: -0.5485\n",
      "Epoch 34/100, Training Loss: -0.5561\n",
      "Epoch 35/100, Training Loss: -0.5573\n",
      "Epoch 36/100, Training Loss: -0.5542\n",
      "Epoch 37/100, Training Loss: -0.5503\n",
      "Epoch 38/100, Training Loss: -0.5442\n",
      "Epoch 39/100, Training Loss: -0.5537\n",
      "Epoch 40/100, Training Loss: -0.5667\n",
      "Epoch 41/100, Training Loss: -0.5459\n",
      "Epoch 42/100, Training Loss: -0.5313\n",
      "Epoch 43/100, Training Loss: -0.5576\n",
      "Epoch 44/100, Training Loss: -0.5434\n",
      "Epoch 45/100, Training Loss: -0.5473\n",
      "Epoch 46/100, Training Loss: -0.5677\n",
      "Epoch 47/100, Training Loss: -0.5551\n",
      "Epoch 48/100, Training Loss: -0.5683\n",
      "Epoch 49/100, Training Loss: -0.5460\n",
      "Epoch 50/100, Training Loss: -0.5531\n",
      "Epoch 51/100, Training Loss: -0.5271\n",
      "Epoch 52/100, Training Loss: -0.5645\n",
      "Epoch 53/100, Training Loss: -0.5479\n",
      "Epoch 54/100, Training Loss: -0.5597\n",
      "Epoch 55/100, Training Loss: -0.5441\n",
      "Epoch 56/100, Training Loss: -0.5489\n",
      "Epoch 57/100, Training Loss: -0.5458\n",
      "Epoch 58/100, Training Loss: -0.5582\n",
      "Epoch 59/100, Training Loss: -0.5454\n",
      "Epoch 60/100, Training Loss: -0.5547\n",
      "Epoch 61/100, Training Loss: -0.5308\n",
      "Epoch 62/100, Training Loss: -0.5757\n",
      "Epoch 63/100, Training Loss: -0.5627\n",
      "Epoch 64/100, Training Loss: -0.5563\n",
      "Epoch 65/100, Training Loss: -0.5580\n",
      "Epoch 66/100, Training Loss: -0.5598\n",
      "Epoch 67/100, Training Loss: -0.5500\n",
      "Epoch 68/100, Training Loss: -0.5746\n",
      "Epoch 69/100, Training Loss: -0.5659\n",
      "Epoch 70/100, Training Loss: -0.5462\n",
      "Epoch 71/100, Training Loss: -0.5332\n",
      "Epoch 72/100, Training Loss: -0.5584\n",
      "Epoch 73/100, Training Loss: -0.5550\n",
      "Epoch 74/100, Training Loss: -0.5413\n",
      "Epoch 75/100, Training Loss: -0.5510\n",
      "Epoch 76/100, Training Loss: -0.5630\n",
      "Epoch 77/100, Training Loss: -0.5573\n",
      "Epoch 78/100, Training Loss: -0.5324\n",
      "Epoch 79/100, Training Loss: -0.5615\n",
      "Epoch 80/100, Training Loss: -0.5623\n",
      "Epoch 81/100, Training Loss: -0.5581\n",
      "Epoch 82/100, Training Loss: -0.5540\n",
      "Epoch 83/100, Training Loss: -0.5248\n",
      "Epoch 84/100, Training Loss: -0.5680\n",
      "Epoch 85/100, Training Loss: -0.5572\n",
      "Epoch 86/100, Training Loss: -0.5571\n",
      "Epoch 87/100, Training Loss: -0.5671\n",
      "Epoch 88/100, Training Loss: -0.5519\n",
      "Epoch 89/100, Training Loss: -0.5678\n",
      "Epoch 90/100, Training Loss: -0.5468\n",
      "Epoch 91/100, Training Loss: -0.5601\n",
      "Epoch 92/100, Training Loss: -0.5731\n",
      "Epoch 93/100, Training Loss: -0.5492\n",
      "Epoch 94/100, Training Loss: -0.5448\n",
      "Epoch 95/100, Training Loss: -0.5709\n",
      "Epoch 96/100, Training Loss: -0.5698\n",
      "Epoch 97/100, Training Loss: -0.5608\n",
      "Epoch 98/100, Training Loss: -0.5642\n",
      "Epoch 99/100, Training Loss: -0.5440\n",
      "Epoch 100/100, Training Loss: -0.5719\n",
      "Validation Loss: -0.5547\n",
      "Model 2, Fold 0: Validation Loss = -0.5547\n",
      "Epoch 1/100, Training Loss: -0.4799\n",
      "Epoch 2/100, Training Loss: -0.5195\n",
      "Epoch 3/100, Training Loss: -0.5260\n",
      "Epoch 4/100, Training Loss: -0.5489\n",
      "Epoch 5/100, Training Loss: -0.5633\n",
      "Epoch 6/100, Training Loss: -0.5377\n",
      "Epoch 7/100, Training Loss: -0.5663\n",
      "Epoch 8/100, Training Loss: -0.5541\n",
      "Epoch 9/100, Training Loss: -0.5632\n",
      "Epoch 10/100, Training Loss: -0.5644\n",
      "Epoch 11/100, Training Loss: -0.5657\n",
      "Epoch 12/100, Training Loss: -0.5672\n",
      "Epoch 13/100, Training Loss: -0.5444\n",
      "Epoch 14/100, Training Loss: -0.5643\n",
      "Epoch 15/100, Training Loss: -0.5626\n",
      "Epoch 16/100, Training Loss: -0.5693\n",
      "Epoch 17/100, Training Loss: -0.5508\n",
      "Epoch 18/100, Training Loss: -0.5560\n",
      "Epoch 19/100, Training Loss: -0.5530\n",
      "Epoch 20/100, Training Loss: -0.5408\n",
      "Epoch 21/100, Training Loss: -0.5541\n",
      "Epoch 22/100, Training Loss: -0.5719\n",
      "Epoch 23/100, Training Loss: -0.5664\n",
      "Epoch 24/100, Training Loss: -0.5411\n",
      "Epoch 25/100, Training Loss: -0.5399\n",
      "Epoch 26/100, Training Loss: -0.5705\n",
      "Epoch 27/100, Training Loss: -0.5688\n",
      "Epoch 28/100, Training Loss: -0.5539\n",
      "Epoch 29/100, Training Loss: -0.5443\n",
      "Epoch 30/100, Training Loss: -0.5579\n",
      "Epoch 31/100, Training Loss: -0.5502\n",
      "Epoch 32/100, Training Loss: -0.5639\n",
      "Epoch 33/100, Training Loss: -0.5302\n",
      "Epoch 34/100, Training Loss: -0.5553\n",
      "Epoch 35/100, Training Loss: -0.5459\n",
      "Epoch 36/100, Training Loss: -0.5648\n",
      "Epoch 37/100, Training Loss: -0.5504\n",
      "Epoch 38/100, Training Loss: -0.5671\n",
      "Epoch 39/100, Training Loss: -0.5822\n",
      "Epoch 40/100, Training Loss: -0.5624\n",
      "Epoch 41/100, Training Loss: -0.5659\n",
      "Epoch 42/100, Training Loss: -0.5736\n",
      "Epoch 43/100, Training Loss: -0.5584\n",
      "Epoch 44/100, Training Loss: -0.5463\n",
      "Epoch 45/100, Training Loss: -0.5701\n",
      "Epoch 46/100, Training Loss: -0.5667\n",
      "Epoch 47/100, Training Loss: -0.5694\n",
      "Epoch 48/100, Training Loss: -0.5541\n",
      "Epoch 49/100, Training Loss: -0.5687\n",
      "Epoch 50/100, Training Loss: -0.5659\n",
      "Epoch 51/100, Training Loss: -0.5673\n",
      "Epoch 52/100, Training Loss: -0.5562\n",
      "Epoch 53/100, Training Loss: -0.5499\n",
      "Epoch 54/100, Training Loss: -0.5717\n",
      "Epoch 55/100, Training Loss: -0.5713\n",
      "Epoch 56/100, Training Loss: -0.5643\n",
      "Epoch 57/100, Training Loss: -0.5689\n",
      "Epoch 58/100, Training Loss: -0.5470\n",
      "Epoch 59/100, Training Loss: -0.5381\n",
      "Epoch 60/100, Training Loss: -0.5671\n",
      "Epoch 61/100, Training Loss: -0.5603\n",
      "Epoch 62/100, Training Loss: -0.5547\n",
      "Epoch 63/100, Training Loss: -0.5692\n",
      "Epoch 64/100, Training Loss: -0.5486\n",
      "Epoch 65/100, Training Loss: -0.5634\n",
      "Epoch 66/100, Training Loss: -0.5566\n",
      "Epoch 67/100, Training Loss: -0.5671\n",
      "Epoch 68/100, Training Loss: -0.5589\n",
      "Epoch 69/100, Training Loss: -0.5686\n",
      "Epoch 70/100, Training Loss: -0.5507\n",
      "Epoch 71/100, Training Loss: -0.5490\n",
      "Epoch 72/100, Training Loss: -0.5581\n",
      "Epoch 73/100, Training Loss: -0.5604\n",
      "Epoch 74/100, Training Loss: -0.5678\n",
      "Epoch 75/100, Training Loss: -0.5658\n",
      "Epoch 76/100, Training Loss: -0.5733\n",
      "Epoch 77/100, Training Loss: -0.5831\n",
      "Epoch 78/100, Training Loss: -0.5706\n",
      "Epoch 79/100, Training Loss: -0.5726\n",
      "Epoch 80/100, Training Loss: -0.5322\n",
      "Epoch 81/100, Training Loss: -0.5588\n",
      "Epoch 82/100, Training Loss: -0.5694\n",
      "Epoch 83/100, Training Loss: -0.5602\n",
      "Epoch 84/100, Training Loss: -0.5589\n",
      "Epoch 85/100, Training Loss: -0.5615\n",
      "Epoch 86/100, Training Loss: -0.5836\n",
      "Epoch 87/100, Training Loss: -0.5616\n",
      "Epoch 88/100, Training Loss: -0.5862\n",
      "Epoch 89/100, Training Loss: -0.5548\n",
      "Epoch 90/100, Training Loss: -0.5637\n",
      "Epoch 91/100, Training Loss: -0.5847\n",
      "Epoch 92/100, Training Loss: -0.5464\n",
      "Epoch 93/100, Training Loss: -0.5677\n",
      "Epoch 94/100, Training Loss: -0.5626\n",
      "Epoch 95/100, Training Loss: -0.5691\n",
      "Epoch 96/100, Training Loss: -0.5371\n",
      "Epoch 97/100, Training Loss: -0.5647\n",
      "Epoch 98/100, Training Loss: -0.5545\n",
      "Epoch 99/100, Training Loss: -0.5747\n",
      "Epoch 100/100, Training Loss: -0.5635\n",
      "Validation Loss: -0.5469\n",
      "Model 2, Fold 1: Validation Loss = -0.5469\n",
      "Epoch 1/100, Training Loss: -0.4781\n",
      "Epoch 2/100, Training Loss: -0.5346\n",
      "Epoch 3/100, Training Loss: -0.5293\n",
      "Epoch 4/100, Training Loss: -0.5410\n",
      "Epoch 5/100, Training Loss: -0.5606\n",
      "Epoch 6/100, Training Loss: -0.5375\n",
      "Epoch 7/100, Training Loss: -0.5682\n",
      "Epoch 8/100, Training Loss: -0.5394\n",
      "Epoch 9/100, Training Loss: -0.5313\n",
      "Epoch 10/100, Training Loss: -0.5587\n",
      "Epoch 11/100, Training Loss: -0.5686\n",
      "Epoch 12/100, Training Loss: -0.5417\n",
      "Epoch 13/100, Training Loss: -0.5517\n",
      "Epoch 14/100, Training Loss: -0.5530\n",
      "Epoch 15/100, Training Loss: -0.5472\n",
      "Epoch 16/100, Training Loss: -0.5504\n",
      "Epoch 17/100, Training Loss: -0.5476\n",
      "Epoch 18/100, Training Loss: -0.5562\n",
      "Epoch 19/100, Training Loss: -0.5461\n",
      "Epoch 20/100, Training Loss: -0.5606\n",
      "Epoch 21/100, Training Loss: -0.5493\n",
      "Epoch 22/100, Training Loss: -0.5491\n",
      "Epoch 23/100, Training Loss: -0.5621\n",
      "Epoch 24/100, Training Loss: -0.5700\n",
      "Epoch 25/100, Training Loss: -0.5562\n",
      "Epoch 26/100, Training Loss: -0.5444\n",
      "Epoch 27/100, Training Loss: -0.5532\n",
      "Epoch 28/100, Training Loss: -0.5489\n",
      "Epoch 29/100, Training Loss: -0.5763\n",
      "Epoch 30/100, Training Loss: -0.5502\n",
      "Epoch 31/100, Training Loss: -0.5692\n",
      "Epoch 32/100, Training Loss: -0.5346\n",
      "Epoch 33/100, Training Loss: -0.5496\n",
      "Epoch 34/100, Training Loss: -0.5370\n",
      "Epoch 35/100, Training Loss: -0.5440\n",
      "Epoch 36/100, Training Loss: -0.5371\n",
      "Epoch 37/100, Training Loss: -0.5463\n",
      "Epoch 38/100, Training Loss: -0.5376\n",
      "Epoch 39/100, Training Loss: -0.5676\n",
      "Epoch 40/100, Training Loss: -0.5368\n",
      "Epoch 41/100, Training Loss: -0.5528\n",
      "Epoch 42/100, Training Loss: -0.5642\n",
      "Epoch 43/100, Training Loss: -0.5355\n",
      "Epoch 44/100, Training Loss: -0.5517\n",
      "Epoch 45/100, Training Loss: -0.5660\n",
      "Epoch 46/100, Training Loss: -0.5656\n",
      "Epoch 47/100, Training Loss: -0.5511\n",
      "Epoch 48/100, Training Loss: -0.5602\n",
      "Epoch 49/100, Training Loss: -0.5404\n",
      "Epoch 50/100, Training Loss: -0.5557\n",
      "Epoch 51/100, Training Loss: -0.5517\n",
      "Epoch 52/100, Training Loss: -0.5520\n",
      "Epoch 53/100, Training Loss: -0.5354\n",
      "Epoch 54/100, Training Loss: -0.5323\n",
      "Epoch 55/100, Training Loss: -0.5369\n",
      "Epoch 56/100, Training Loss: -0.5630\n",
      "Epoch 57/100, Training Loss: -0.5619\n",
      "Epoch 58/100, Training Loss: -0.5446\n",
      "Epoch 59/100, Training Loss: -0.5651\n",
      "Epoch 60/100, Training Loss: -0.5354\n",
      "Epoch 61/100, Training Loss: -0.5265\n",
      "Epoch 62/100, Training Loss: -0.5575\n",
      "Epoch 63/100, Training Loss: -0.5423\n",
      "Epoch 64/100, Training Loss: -0.5548\n",
      "Epoch 65/100, Training Loss: -0.5639\n",
      "Epoch 66/100, Training Loss: -0.5597\n",
      "Epoch 67/100, Training Loss: -0.5570\n",
      "Epoch 68/100, Training Loss: -0.5552\n",
      "Epoch 69/100, Training Loss: -0.5575\n",
      "Epoch 70/100, Training Loss: -0.5809\n",
      "Epoch 71/100, Training Loss: -0.5614\n",
      "Epoch 72/100, Training Loss: -0.5875\n",
      "Epoch 73/100, Training Loss: -0.5354\n",
      "Epoch 74/100, Training Loss: -0.5466\n",
      "Epoch 75/100, Training Loss: -0.5473\n",
      "Epoch 76/100, Training Loss: -0.5589\n",
      "Epoch 77/100, Training Loss: -0.5318\n",
      "Epoch 78/100, Training Loss: -0.5687\n",
      "Epoch 79/100, Training Loss: -0.5557\n",
      "Epoch 80/100, Training Loss: -0.5955\n",
      "Epoch 81/100, Training Loss: -0.5801\n",
      "Epoch 82/100, Training Loss: -0.5597\n",
      "Epoch 83/100, Training Loss: -0.5867\n",
      "Epoch 84/100, Training Loss: -0.5661\n",
      "Epoch 85/100, Training Loss: -0.5719\n",
      "Epoch 86/100, Training Loss: -0.5375\n",
      "Epoch 87/100, Training Loss: -0.5305\n",
      "Epoch 88/100, Training Loss: -0.5609\n",
      "Epoch 89/100, Training Loss: -0.5481\n",
      "Epoch 90/100, Training Loss: -0.5342\n",
      "Epoch 91/100, Training Loss: -0.5457\n",
      "Epoch 92/100, Training Loss: -0.5416\n",
      "Epoch 93/100, Training Loss: -0.5516\n",
      "Epoch 94/100, Training Loss: -0.5575\n",
      "Epoch 95/100, Training Loss: -0.5523\n",
      "Epoch 96/100, Training Loss: -0.5484\n",
      "Epoch 97/100, Training Loss: -0.5447\n",
      "Epoch 98/100, Training Loss: -0.5432\n",
      "Epoch 99/100, Training Loss: -0.5780\n",
      "Epoch 100/100, Training Loss: -0.5704\n",
      "Validation Loss: -0.5532\n",
      "Model 3, Fold 0: Validation Loss = -0.5532\n",
      "Epoch 1/100, Training Loss: -0.5179\n",
      "Epoch 2/100, Training Loss: -0.5268\n",
      "Epoch 3/100, Training Loss: -0.5345\n",
      "Epoch 4/100, Training Loss: -0.5613\n",
      "Epoch 5/100, Training Loss: -0.5439\n",
      "Epoch 6/100, Training Loss: -0.5635\n",
      "Epoch 7/100, Training Loss: -0.5644\n",
      "Epoch 8/100, Training Loss: -0.5634\n",
      "Epoch 9/100, Training Loss: -0.5553\n",
      "Epoch 10/100, Training Loss: -0.5561\n",
      "Epoch 11/100, Training Loss: -0.5587\n",
      "Epoch 12/100, Training Loss: -0.5730\n",
      "Epoch 13/100, Training Loss: -0.5709\n",
      "Epoch 14/100, Training Loss: -0.5271\n",
      "Epoch 15/100, Training Loss: -0.5629\n",
      "Epoch 16/100, Training Loss: -0.5630\n",
      "Epoch 17/100, Training Loss: -0.5608\n",
      "Epoch 18/100, Training Loss: -0.5723\n",
      "Epoch 19/100, Training Loss: -0.5816\n",
      "Epoch 20/100, Training Loss: -0.5734\n",
      "Epoch 21/100, Training Loss: -0.5697\n",
      "Epoch 22/100, Training Loss: -0.5920\n",
      "Epoch 23/100, Training Loss: -0.5590\n",
      "Epoch 24/100, Training Loss: -0.5482\n",
      "Epoch 25/100, Training Loss: -0.5663\n",
      "Epoch 26/100, Training Loss: -0.5723\n",
      "Epoch 27/100, Training Loss: -0.5700\n",
      "Epoch 28/100, Training Loss: -0.5752\n",
      "Epoch 29/100, Training Loss: -0.5739\n",
      "Epoch 30/100, Training Loss: -0.5549\n",
      "Epoch 31/100, Training Loss: -0.5581\n",
      "Epoch 32/100, Training Loss: -0.5589\n",
      "Epoch 33/100, Training Loss: -0.5634\n",
      "Epoch 34/100, Training Loss: -0.5739\n",
      "Epoch 35/100, Training Loss: -0.5681\n",
      "Epoch 36/100, Training Loss: -0.5658\n",
      "Epoch 37/100, Training Loss: -0.5581\n",
      "Epoch 38/100, Training Loss: -0.5829\n",
      "Epoch 39/100, Training Loss: -0.5538\n",
      "Epoch 40/100, Training Loss: -0.5515\n",
      "Epoch 41/100, Training Loss: -0.5462\n",
      "Epoch 42/100, Training Loss: -0.5411\n",
      "Epoch 43/100, Training Loss: -0.5650\n",
      "Epoch 44/100, Training Loss: -0.5427\n",
      "Epoch 45/100, Training Loss: -0.5736\n",
      "Epoch 46/100, Training Loss: -0.5560\n",
      "Epoch 47/100, Training Loss: -0.5651\n",
      "Epoch 48/100, Training Loss: -0.5586\n",
      "Epoch 49/100, Training Loss: -0.5574\n",
      "Epoch 50/100, Training Loss: -0.5409\n",
      "Epoch 51/100, Training Loss: -0.5694\n",
      "Epoch 52/100, Training Loss: -0.5904\n",
      "Epoch 53/100, Training Loss: -0.5620\n",
      "Epoch 54/100, Training Loss: -0.5311\n",
      "Epoch 55/100, Training Loss: -0.5573\n",
      "Epoch 56/100, Training Loss: -0.5503\n",
      "Epoch 57/100, Training Loss: -0.5618\n",
      "Epoch 58/100, Training Loss: -0.5380\n",
      "Epoch 59/100, Training Loss: -0.5513\n",
      "Epoch 60/100, Training Loss: -0.5577\n",
      "Epoch 61/100, Training Loss: -0.5882\n",
      "Epoch 62/100, Training Loss: -0.5708\n",
      "Epoch 63/100, Training Loss: -0.5573\n",
      "Epoch 64/100, Training Loss: -0.5541\n",
      "Epoch 65/100, Training Loss: -0.5645\n",
      "Epoch 66/100, Training Loss: -0.5486\n",
      "Epoch 67/100, Training Loss: -0.5539\n",
      "Epoch 68/100, Training Loss: -0.5574\n",
      "Epoch 69/100, Training Loss: -0.5506\n",
      "Epoch 70/100, Training Loss: -0.5605\n",
      "Epoch 71/100, Training Loss: -0.5568\n",
      "Epoch 72/100, Training Loss: -0.5349\n",
      "Epoch 73/100, Training Loss: -0.5414\n",
      "Epoch 74/100, Training Loss: -0.5611\n",
      "Epoch 75/100, Training Loss: -0.5659\n",
      "Epoch 76/100, Training Loss: -0.5611\n",
      "Epoch 77/100, Training Loss: -0.5561\n",
      "Epoch 78/100, Training Loss: -0.5643\n",
      "Epoch 79/100, Training Loss: -0.5592\n",
      "Epoch 80/100, Training Loss: -0.5800\n",
      "Epoch 81/100, Training Loss: -0.5650\n",
      "Epoch 82/100, Training Loss: -0.5499\n",
      "Epoch 83/100, Training Loss: -0.5605\n",
      "Epoch 84/100, Training Loss: -0.5629\n",
      "Epoch 85/100, Training Loss: -0.5619\n",
      "Epoch 86/100, Training Loss: -0.5478\n",
      "Epoch 87/100, Training Loss: -0.5759\n",
      "Epoch 88/100, Training Loss: -0.5712\n",
      "Epoch 89/100, Training Loss: -0.5570\n",
      "Epoch 90/100, Training Loss: -0.5798\n",
      "Epoch 91/100, Training Loss: -0.5636\n",
      "Epoch 92/100, Training Loss: -0.5731\n",
      "Epoch 93/100, Training Loss: -0.5683\n",
      "Epoch 94/100, Training Loss: -0.5670\n",
      "Epoch 95/100, Training Loss: -0.5821\n",
      "Epoch 96/100, Training Loss: -0.5727\n",
      "Epoch 97/100, Training Loss: -0.5800\n",
      "Epoch 98/100, Training Loss: -0.5760\n",
      "Epoch 99/100, Training Loss: -0.5443\n",
      "Epoch 100/100, Training Loss: -0.5733\n",
      "Validation Loss: -0.5418\n",
      "Model 3, Fold 1: Validation Loss = -0.5418\n",
      "Best overall model index: 0 with average validation loss -0.5539\n",
      "Retraining best model on full dataset...\n",
      "Epoch 1/100, Training Loss: -0.5358\n",
      "Epoch 2/100, Training Loss: -0.5637\n",
      "Epoch 3/100, Training Loss: -0.5484\n",
      "Epoch 4/100, Training Loss: -0.5714\n",
      "Epoch 5/100, Training Loss: -0.5465\n",
      "Epoch 6/100, Training Loss: -0.5408\n",
      "Epoch 7/100, Training Loss: -0.5451\n",
      "Epoch 8/100, Training Loss: -0.5673\n",
      "Epoch 9/100, Training Loss: -0.5543\n",
      "Epoch 10/100, Training Loss: -0.5540\n",
      "Epoch 11/100, Training Loss: -0.5535\n",
      "Epoch 12/100, Training Loss: -0.5499\n",
      "Epoch 13/100, Training Loss: -0.5632\n",
      "Epoch 14/100, Training Loss: -0.5526\n",
      "Epoch 15/100, Training Loss: -0.5548\n",
      "Epoch 16/100, Training Loss: -0.5618\n",
      "Epoch 17/100, Training Loss: -0.5494\n",
      "Epoch 18/100, Training Loss: -0.5765\n",
      "Epoch 19/100, Training Loss: -0.5531\n",
      "Epoch 20/100, Training Loss: -0.5570\n",
      "Epoch 21/100, Training Loss: -0.5526\n",
      "Epoch 22/100, Training Loss: -0.5478\n",
      "Epoch 23/100, Training Loss: -0.5573\n",
      "Epoch 24/100, Training Loss: -0.5628\n",
      "Epoch 25/100, Training Loss: -0.5533\n",
      "Epoch 26/100, Training Loss: -0.5669\n",
      "Epoch 27/100, Training Loss: -0.5536\n",
      "Epoch 28/100, Training Loss: -0.5511\n",
      "Epoch 29/100, Training Loss: -0.5533\n",
      "Epoch 30/100, Training Loss: -0.5655\n",
      "Epoch 31/100, Training Loss: -0.5553\n",
      "Epoch 32/100, Training Loss: -0.5626\n",
      "Epoch 33/100, Training Loss: -0.5720\n",
      "Epoch 34/100, Training Loss: -0.5455\n",
      "Epoch 35/100, Training Loss: -0.5654\n",
      "Epoch 36/100, Training Loss: -0.5390\n",
      "Epoch 37/100, Training Loss: -0.5620\n",
      "Epoch 38/100, Training Loss: -0.5577\n",
      "Epoch 39/100, Training Loss: -0.5614\n",
      "Epoch 40/100, Training Loss: -0.5532\n",
      "Epoch 41/100, Training Loss: -0.5614\n",
      "Epoch 42/100, Training Loss: -0.5359\n",
      "Epoch 43/100, Training Loss: -0.5607\n",
      "Epoch 44/100, Training Loss: -0.5569\n",
      "Epoch 45/100, Training Loss: -0.5564\n",
      "Epoch 46/100, Training Loss: -0.5647\n",
      "Epoch 47/100, Training Loss: -0.5719\n",
      "Epoch 48/100, Training Loss: -0.5625\n",
      "Epoch 49/100, Training Loss: -0.5635\n",
      "Epoch 50/100, Training Loss: -0.5684\n",
      "Epoch 51/100, Training Loss: -0.5512\n",
      "Epoch 52/100, Training Loss: -0.5545\n",
      "Epoch 53/100, Training Loss: -0.5734\n",
      "Epoch 54/100, Training Loss: -0.5620\n",
      "Epoch 55/100, Training Loss: -0.5581\n",
      "Epoch 56/100, Training Loss: -0.5421\n",
      "Epoch 57/100, Training Loss: -0.5497\n",
      "Epoch 58/100, Training Loss: -0.5533\n",
      "Epoch 59/100, Training Loss: -0.5532\n",
      "Epoch 60/100, Training Loss: -0.5591\n",
      "Epoch 61/100, Training Loss: -0.5578\n",
      "Epoch 62/100, Training Loss: -0.5559\n",
      "Epoch 63/100, Training Loss: -0.5638\n",
      "Epoch 64/100, Training Loss: -0.5330\n",
      "Epoch 65/100, Training Loss: -0.5506\n",
      "Epoch 66/100, Training Loss: -0.5537\n",
      "Epoch 67/100, Training Loss: -0.5615\n",
      "Epoch 68/100, Training Loss: -0.5529\n",
      "Epoch 69/100, Training Loss: -0.5557\n",
      "Epoch 70/100, Training Loss: -0.5636\n",
      "Epoch 71/100, Training Loss: -0.5411\n",
      "Epoch 72/100, Training Loss: -0.5546\n",
      "Epoch 73/100, Training Loss: -0.5459\n",
      "Epoch 74/100, Training Loss: -0.5597\n",
      "Epoch 75/100, Training Loss: -0.5333\n",
      "Epoch 76/100, Training Loss: -0.5476\n",
      "Epoch 77/100, Training Loss: -0.5615\n",
      "Epoch 78/100, Training Loss: -0.5455\n",
      "Epoch 79/100, Training Loss: -0.5463\n",
      "Epoch 80/100, Training Loss: -0.5644\n",
      "Epoch 81/100, Training Loss: -0.5548\n",
      "Epoch 82/100, Training Loss: -0.5550\n",
      "Epoch 83/100, Training Loss: -0.5613\n",
      "Epoch 84/100, Training Loss: -0.5577\n",
      "Epoch 85/100, Training Loss: -0.5415\n",
      "Epoch 86/100, Training Loss: -0.5603\n",
      "Epoch 87/100, Training Loss: -0.5454\n",
      "Epoch 88/100, Training Loss: -0.5513\n",
      "Epoch 89/100, Training Loss: -0.5673\n",
      "Epoch 90/100, Training Loss: -0.5445\n",
      "Epoch 91/100, Training Loss: -0.5620\n",
      "Epoch 92/100, Training Loss: -0.5699\n",
      "Epoch 93/100, Training Loss: -0.5506\n",
      "Epoch 94/100, Training Loss: -0.5710\n",
      "Epoch 95/100, Training Loss: -0.5655\n",
      "Epoch 96/100, Training Loss: -0.5432\n",
      "Epoch 97/100, Training Loss: -0.5489\n",
      "Epoch 98/100, Training Loss: -0.5600\n",
      "Epoch 99/100, Training Loss: -0.5528\n",
      "Epoch 100/100, Training Loss: -0.5555\n",
      "Best overall model index: 0 with average validation loss -0.5539\n"
     ]
    }
   ],
   "source": [
    "# Training with cocycles\n",
    "kernel = [gaussian_kernel()] * 2\n",
    "loss_factory = CocycleLossFactory(kernel)\n",
    "loss= loss_factory.build_loss(\"CMMD_U\", X, Y, subsamples=10**4)\n",
    "final_model_cmmdu, (best_index_cmmdu, val_loss_cmmdu) = validate(\n",
    "        models,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs=hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_cmmdu} with average validation loss {val_loss_cmmdu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f18d3233-7c40-43c4-a84c-8b7d41732330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7824)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.kernel[1].lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3acc2ce5-88f4-4c11-81ec-e032a54f0d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Training Loss: 0.0191\n",
      "Epoch 2/500, Training Loss: 0.0185\n",
      "Epoch 3/500, Training Loss: 0.0221\n",
      "Epoch 4/500, Training Loss: 0.0207\n",
      "Epoch 5/500, Training Loss: 0.0210\n",
      "Epoch 6/500, Training Loss: 0.0170\n",
      "Epoch 7/500, Training Loss: 0.0197\n",
      "Epoch 8/500, Training Loss: 0.0115\n",
      "Epoch 9/500, Training Loss: 0.0191\n",
      "Epoch 10/500, Training Loss: 0.0191\n",
      "Epoch 11/500, Training Loss: 0.0173\n",
      "Epoch 12/500, Training Loss: 0.0232\n",
      "Epoch 13/500, Training Loss: 0.0194\n",
      "Epoch 14/500, Training Loss: 0.0175\n",
      "Epoch 15/500, Training Loss: 0.0191\n",
      "Epoch 16/500, Training Loss: 0.0178\n",
      "Epoch 17/500, Training Loss: 0.0179\n",
      "Epoch 18/500, Training Loss: 0.0180\n",
      "Epoch 19/500, Training Loss: 0.0154\n",
      "Epoch 20/500, Training Loss: 0.0141\n",
      "Epoch 21/500, Training Loss: 0.0135\n",
      "Epoch 22/500, Training Loss: 0.0164\n",
      "Epoch 23/500, Training Loss: 0.0151\n",
      "Epoch 24/500, Training Loss: 0.0148\n",
      "Epoch 25/500, Training Loss: 0.0131\n",
      "Epoch 26/500, Training Loss: 0.0145\n",
      "Epoch 27/500, Training Loss: 0.0133\n",
      "Epoch 28/500, Training Loss: 0.0146\n",
      "Epoch 29/500, Training Loss: 0.0134\n",
      "Epoch 30/500, Training Loss: 0.0117\n",
      "Epoch 31/500, Training Loss: 0.0110\n",
      "Epoch 32/500, Training Loss: 0.0115\n",
      "Epoch 33/500, Training Loss: 0.0112\n",
      "Epoch 34/500, Training Loss: 0.0094\n",
      "Epoch 35/500, Training Loss: 0.0073\n",
      "Epoch 36/500, Training Loss: 0.0128\n",
      "Epoch 37/500, Training Loss: 0.0078\n",
      "Epoch 38/500, Training Loss: 0.0077\n",
      "Epoch 39/500, Training Loss: 0.0084\n",
      "Epoch 40/500, Training Loss: 0.0073\n",
      "Epoch 41/500, Training Loss: 0.0066\n",
      "Epoch 42/500, Training Loss: 0.0055\n",
      "Epoch 43/500, Training Loss: 0.0048\n",
      "Epoch 44/500, Training Loss: 0.0044\n",
      "Epoch 45/500, Training Loss: 0.0054\n",
      "Epoch 46/500, Training Loss: 0.0040\n",
      "Epoch 47/500, Training Loss: 0.0053\n",
      "Epoch 48/500, Training Loss: 0.0058\n",
      "Epoch 49/500, Training Loss: 0.0044\n",
      "Epoch 50/500, Training Loss: 0.0031\n",
      "Epoch 51/500, Training Loss: 0.0036\n",
      "Epoch 52/500, Training Loss: 0.0046\n",
      "Epoch 53/500, Training Loss: 0.0037\n",
      "Epoch 54/500, Training Loss: 0.0025\n",
      "Epoch 55/500, Training Loss: 0.0031\n",
      "Epoch 56/500, Training Loss: 0.0032\n",
      "Epoch 57/500, Training Loss: 0.0028\n",
      "Epoch 58/500, Training Loss: 0.0029\n",
      "Epoch 59/500, Training Loss: 0.0039\n",
      "Epoch 60/500, Training Loss: 0.0034\n",
      "Epoch 61/500, Training Loss: 0.0029\n",
      "Epoch 62/500, Training Loss: 0.0033\n",
      "Epoch 63/500, Training Loss: 0.0028\n",
      "Epoch 64/500, Training Loss: 0.0028\n",
      "Epoch 65/500, Training Loss: 0.0030\n",
      "Epoch 66/500, Training Loss: 0.0026\n",
      "Epoch 67/500, Training Loss: 0.0028\n",
      "Epoch 68/500, Training Loss: 0.0026\n",
      "Epoch 69/500, Training Loss: 0.0020\n",
      "Epoch 70/500, Training Loss: 0.0024\n",
      "Epoch 71/500, Training Loss: 0.0027\n",
      "Epoch 72/500, Training Loss: 0.0022\n",
      "Epoch 73/500, Training Loss: 0.0024\n",
      "Epoch 74/500, Training Loss: 0.0043\n",
      "Epoch 75/500, Training Loss: 0.0027\n",
      "Epoch 76/500, Training Loss: 0.0037\n",
      "Epoch 77/500, Training Loss: 0.0028\n",
      "Epoch 78/500, Training Loss: 0.0034\n",
      "Epoch 79/500, Training Loss: 0.0033\n",
      "Epoch 80/500, Training Loss: 0.0025\n",
      "Epoch 81/500, Training Loss: 0.0028\n",
      "Epoch 82/500, Training Loss: 0.0030\n",
      "Epoch 83/500, Training Loss: 0.0029\n",
      "Epoch 84/500, Training Loss: 0.0032\n",
      "Epoch 85/500, Training Loss: 0.0029\n",
      "Epoch 86/500, Training Loss: 0.0028\n",
      "Epoch 87/500, Training Loss: 0.0034\n",
      "Epoch 88/500, Training Loss: 0.0028\n",
      "Epoch 89/500, Training Loss: 0.0022\n",
      "Epoch 90/500, Training Loss: 0.0029\n",
      "Epoch 91/500, Training Loss: 0.0027\n",
      "Epoch 92/500, Training Loss: 0.0040\n",
      "Epoch 93/500, Training Loss: 0.0035\n",
      "Epoch 94/500, Training Loss: 0.0029\n",
      "Epoch 95/500, Training Loss: 0.0029\n",
      "Epoch 96/500, Training Loss: 0.0029\n",
      "Epoch 97/500, Training Loss: 0.0032\n",
      "Epoch 98/500, Training Loss: 0.0028\n",
      "Epoch 99/500, Training Loss: 0.0026\n",
      "Epoch 100/500, Training Loss: 0.0032\n",
      "Epoch 101/500, Training Loss: 0.0028\n",
      "Epoch 102/500, Training Loss: 0.0042\n",
      "Epoch 103/500, Training Loss: 0.0030\n",
      "Epoch 104/500, Training Loss: 0.0023\n",
      "Epoch 105/500, Training Loss: 0.0025\n",
      "Epoch 106/500, Training Loss: 0.0042\n",
      "Epoch 107/500, Training Loss: 0.0026\n",
      "Epoch 108/500, Training Loss: 0.0022\n",
      "Epoch 109/500, Training Loss: 0.0041\n",
      "Epoch 110/500, Training Loss: 0.0025\n",
      "Epoch 111/500, Training Loss: 0.0035\n",
      "Epoch 112/500, Training Loss: 0.0025\n",
      "Epoch 113/500, Training Loss: 0.0033\n",
      "Epoch 114/500, Training Loss: 0.0023\n",
      "Epoch 115/500, Training Loss: 0.0025\n",
      "Epoch 116/500, Training Loss: 0.0027\n",
      "Epoch 117/500, Training Loss: 0.0029\n",
      "Epoch 118/500, Training Loss: 0.0028\n",
      "Epoch 119/500, Training Loss: 0.0026\n",
      "Epoch 120/500, Training Loss: 0.0027\n",
      "Epoch 121/500, Training Loss: 0.0035\n",
      "Epoch 122/500, Training Loss: 0.0025\n",
      "Epoch 123/500, Training Loss: 0.0034\n",
      "Epoch 124/500, Training Loss: 0.0034\n",
      "Epoch 125/500, Training Loss: 0.0020\n",
      "Epoch 126/500, Training Loss: 0.0027\n",
      "Epoch 127/500, Training Loss: 0.0028\n",
      "Epoch 128/500, Training Loss: 0.0025\n",
      "Epoch 129/500, Training Loss: 0.0038\n",
      "Epoch 130/500, Training Loss: 0.0033\n",
      "Epoch 131/500, Training Loss: 0.0025\n",
      "Epoch 132/500, Training Loss: 0.0031\n",
      "Epoch 133/500, Training Loss: 0.0022\n",
      "Epoch 134/500, Training Loss: 0.0024\n",
      "Epoch 135/500, Training Loss: 0.0031\n",
      "Epoch 136/500, Training Loss: 0.0020\n",
      "Epoch 137/500, Training Loss: 0.0040\n",
      "Epoch 138/500, Training Loss: 0.0024\n",
      "Epoch 139/500, Training Loss: 0.0027\n",
      "Epoch 140/500, Training Loss: 0.0026\n",
      "Epoch 141/500, Training Loss: 0.0029\n",
      "Epoch 142/500, Training Loss: 0.0026\n",
      "Epoch 143/500, Training Loss: 0.0047\n",
      "Epoch 144/500, Training Loss: 0.0038\n",
      "Epoch 145/500, Training Loss: 0.0027\n",
      "Epoch 146/500, Training Loss: 0.0038\n",
      "Epoch 147/500, Training Loss: 0.0033\n",
      "Epoch 148/500, Training Loss: 0.0036\n",
      "Epoch 149/500, Training Loss: 0.0044\n",
      "Epoch 150/500, Training Loss: 0.0032\n",
      "Epoch 151/500, Training Loss: 0.0033\n",
      "Epoch 152/500, Training Loss: 0.0039\n",
      "Epoch 153/500, Training Loss: 0.0025\n",
      "Epoch 154/500, Training Loss: 0.0029\n",
      "Epoch 155/500, Training Loss: 0.0027\n",
      "Epoch 156/500, Training Loss: 0.0021\n",
      "Epoch 157/500, Training Loss: 0.0026\n",
      "Epoch 158/500, Training Loss: 0.0030\n",
      "Epoch 159/500, Training Loss: 0.0026\n",
      "Epoch 160/500, Training Loss: 0.0034\n",
      "Epoch 161/500, Training Loss: 0.0031\n",
      "Epoch 162/500, Training Loss: 0.0024\n",
      "Epoch 163/500, Training Loss: 0.0031\n",
      "Epoch 164/500, Training Loss: 0.0027\n",
      "Epoch 165/500, Training Loss: 0.0033\n",
      "Epoch 166/500, Training Loss: 0.0033\n",
      "Epoch 167/500, Training Loss: 0.0028\n",
      "Epoch 168/500, Training Loss: 0.0024\n",
      "Epoch 169/500, Training Loss: 0.0030\n",
      "Epoch 170/500, Training Loss: 0.0034\n",
      "Epoch 171/500, Training Loss: 0.0026\n",
      "Epoch 172/500, Training Loss: 0.0029\n",
      "Epoch 173/500, Training Loss: 0.0028\n",
      "Epoch 174/500, Training Loss: 0.0031\n",
      "Epoch 175/500, Training Loss: 0.0027\n",
      "Epoch 176/500, Training Loss: 0.0039\n",
      "Epoch 177/500, Training Loss: 0.0028\n",
      "Epoch 178/500, Training Loss: 0.0044\n",
      "Epoch 179/500, Training Loss: 0.0026\n",
      "Epoch 180/500, Training Loss: 0.0035\n",
      "Epoch 181/500, Training Loss: 0.0027\n",
      "Epoch 182/500, Training Loss: 0.0031\n",
      "Epoch 183/500, Training Loss: 0.0031\n",
      "Epoch 184/500, Training Loss: 0.0030\n",
      "Epoch 185/500, Training Loss: 0.0030\n",
      "Epoch 186/500, Training Loss: 0.0030\n",
      "Epoch 187/500, Training Loss: 0.0027\n",
      "Epoch 188/500, Training Loss: 0.0032\n",
      "Epoch 189/500, Training Loss: 0.0034\n",
      "Epoch 190/500, Training Loss: 0.0026\n",
      "Epoch 191/500, Training Loss: 0.0027\n",
      "Epoch 192/500, Training Loss: 0.0028\n",
      "Epoch 193/500, Training Loss: 0.0027\n",
      "Epoch 194/500, Training Loss: 0.0031\n",
      "Epoch 195/500, Training Loss: 0.0032\n",
      "Epoch 196/500, Training Loss: 0.0027\n",
      "Epoch 197/500, Training Loss: 0.0028\n",
      "Epoch 198/500, Training Loss: 0.0029\n",
      "Epoch 199/500, Training Loss: 0.0028\n",
      "Epoch 200/500, Training Loss: 0.0031\n",
      "Epoch 201/500, Training Loss: 0.0029\n",
      "Epoch 202/500, Training Loss: 0.0025\n",
      "Epoch 203/500, Training Loss: 0.0028\n",
      "Epoch 204/500, Training Loss: 0.0030\n",
      "Epoch 205/500, Training Loss: 0.0024\n",
      "Epoch 206/500, Training Loss: 0.0031\n",
      "Epoch 207/500, Training Loss: 0.0029\n",
      "Epoch 208/500, Training Loss: 0.0029\n",
      "Epoch 209/500, Training Loss: 0.0024\n",
      "Epoch 210/500, Training Loss: 0.0028\n",
      "Epoch 211/500, Training Loss: 0.0030\n",
      "Epoch 212/500, Training Loss: 0.0023\n",
      "Epoch 213/500, Training Loss: 0.0026\n",
      "Epoch 214/500, Training Loss: 0.0023\n",
      "Epoch 215/500, Training Loss: 0.0036\n",
      "Epoch 216/500, Training Loss: 0.0025\n",
      "Epoch 217/500, Training Loss: 0.0026\n",
      "Epoch 218/500, Training Loss: 0.0031\n",
      "Epoch 219/500, Training Loss: 0.0032\n",
      "Epoch 220/500, Training Loss: 0.0032\n",
      "Epoch 221/500, Training Loss: 0.0023\n",
      "Epoch 222/500, Training Loss: 0.0029\n",
      "Epoch 223/500, Training Loss: 0.0030\n",
      "Epoch 224/500, Training Loss: 0.0025\n",
      "Epoch 225/500, Training Loss: 0.0035\n",
      "Epoch 226/500, Training Loss: 0.0033\n",
      "Epoch 227/500, Training Loss: 0.0023\n",
      "Epoch 228/500, Training Loss: 0.0036\n",
      "Epoch 229/500, Training Loss: 0.0034\n",
      "Epoch 230/500, Training Loss: 0.0029\n",
      "Epoch 231/500, Training Loss: 0.0025\n",
      "Epoch 232/500, Training Loss: 0.0030\n",
      "Epoch 233/500, Training Loss: 0.0039\n",
      "Epoch 234/500, Training Loss: 0.0035\n",
      "Epoch 235/500, Training Loss: 0.0028\n",
      "Epoch 236/500, Training Loss: 0.0027\n",
      "Epoch 237/500, Training Loss: 0.0019\n",
      "Epoch 238/500, Training Loss: 0.0031\n",
      "Epoch 239/500, Training Loss: 0.0036\n",
      "Epoch 240/500, Training Loss: 0.0019\n",
      "Epoch 241/500, Training Loss: 0.0037\n",
      "Epoch 242/500, Training Loss: 0.0032\n",
      "Epoch 243/500, Training Loss: 0.0028\n",
      "Epoch 244/500, Training Loss: 0.0023\n",
      "Epoch 245/500, Training Loss: 0.0022\n",
      "Epoch 246/500, Training Loss: 0.0034\n",
      "Epoch 247/500, Training Loss: 0.0016\n",
      "Epoch 248/500, Training Loss: 0.0033\n",
      "Epoch 249/500, Training Loss: 0.0028\n",
      "Epoch 250/500, Training Loss: 0.0036\n",
      "Epoch 251/500, Training Loss: 0.0022\n",
      "Epoch 252/500, Training Loss: 0.0032\n",
      "Epoch 253/500, Training Loss: 0.0030\n",
      "Epoch 254/500, Training Loss: 0.0027\n",
      "Epoch 255/500, Training Loss: 0.0032\n",
      "Epoch 256/500, Training Loss: 0.0033\n",
      "Epoch 257/500, Training Loss: 0.0029\n",
      "Epoch 258/500, Training Loss: 0.0034\n",
      "Epoch 259/500, Training Loss: 0.0024\n",
      "Epoch 260/500, Training Loss: 0.0028\n",
      "Epoch 261/500, Training Loss: 0.0027\n",
      "Epoch 262/500, Training Loss: 0.0030\n",
      "Epoch 263/500, Training Loss: 0.0030\n",
      "Epoch 264/500, Training Loss: 0.0027\n",
      "Epoch 265/500, Training Loss: 0.0027\n",
      "Epoch 266/500, Training Loss: 0.0030\n",
      "Epoch 267/500, Training Loss: 0.0030\n",
      "Epoch 268/500, Training Loss: 0.0039\n",
      "Epoch 269/500, Training Loss: 0.0029\n",
      "Epoch 270/500, Training Loss: 0.0027\n",
      "Epoch 271/500, Training Loss: 0.0025\n",
      "Epoch 272/500, Training Loss: 0.0036\n",
      "Epoch 273/500, Training Loss: 0.0031\n",
      "Epoch 274/500, Training Loss: 0.0024\n",
      "Epoch 275/500, Training Loss: 0.0034\n",
      "Epoch 276/500, Training Loss: 0.0024\n",
      "Epoch 277/500, Training Loss: 0.0026\n",
      "Epoch 278/500, Training Loss: 0.0024\n",
      "Epoch 279/500, Training Loss: 0.0026\n",
      "Epoch 280/500, Training Loss: 0.0032\n",
      "Epoch 281/500, Training Loss: 0.0032\n",
      "Epoch 282/500, Training Loss: 0.0035\n",
      "Epoch 283/500, Training Loss: 0.0029\n",
      "Epoch 284/500, Training Loss: 0.0027\n",
      "Epoch 285/500, Training Loss: 0.0030\n",
      "Epoch 286/500, Training Loss: 0.0033\n",
      "Epoch 287/500, Training Loss: 0.0025\n",
      "Epoch 288/500, Training Loss: 0.0032\n",
      "Epoch 289/500, Training Loss: 0.0028\n",
      "Epoch 290/500, Training Loss: 0.0031\n",
      "Epoch 291/500, Training Loss: 0.0028\n",
      "Epoch 292/500, Training Loss: 0.0023\n",
      "Epoch 293/500, Training Loss: 0.0026\n",
      "Epoch 294/500, Training Loss: 0.0026\n",
      "Epoch 295/500, Training Loss: 0.0025\n",
      "Epoch 296/500, Training Loss: 0.0025\n",
      "Epoch 297/500, Training Loss: 0.0030\n",
      "Epoch 298/500, Training Loss: 0.0040\n",
      "Epoch 299/500, Training Loss: 0.0026\n",
      "Epoch 300/500, Training Loss: 0.0040\n",
      "Epoch 301/500, Training Loss: 0.0029\n",
      "Epoch 302/500, Training Loss: 0.0026\n",
      "Epoch 303/500, Training Loss: 0.0023\n",
      "Epoch 304/500, Training Loss: 0.0031\n",
      "Epoch 305/500, Training Loss: 0.0030\n",
      "Epoch 306/500, Training Loss: 0.0029\n",
      "Epoch 307/500, Training Loss: 0.0026\n",
      "Epoch 308/500, Training Loss: 0.0024\n",
      "Epoch 309/500, Training Loss: 0.0031\n",
      "Epoch 310/500, Training Loss: 0.0031\n",
      "Epoch 311/500, Training Loss: 0.0026\n",
      "Epoch 312/500, Training Loss: 0.0036\n",
      "Epoch 313/500, Training Loss: 0.0030\n",
      "Epoch 314/500, Training Loss: 0.0033\n",
      "Epoch 315/500, Training Loss: 0.0031\n",
      "Epoch 316/500, Training Loss: 0.0025\n",
      "Epoch 317/500, Training Loss: 0.0033\n",
      "Epoch 318/500, Training Loss: 0.0032\n",
      "Epoch 319/500, Training Loss: 0.0022\n",
      "Epoch 320/500, Training Loss: 0.0020\n",
      "Epoch 321/500, Training Loss: 0.0028\n",
      "Epoch 322/500, Training Loss: 0.0032\n",
      "Epoch 323/500, Training Loss: 0.0030\n",
      "Epoch 324/500, Training Loss: 0.0032\n",
      "Epoch 325/500, Training Loss: 0.0025\n",
      "Epoch 326/500, Training Loss: 0.0020\n",
      "Epoch 327/500, Training Loss: 0.0019\n",
      "Epoch 328/500, Training Loss: 0.0027\n",
      "Epoch 329/500, Training Loss: 0.0031\n",
      "Epoch 330/500, Training Loss: 0.0029\n",
      "Epoch 331/500, Training Loss: 0.0045\n",
      "Epoch 332/500, Training Loss: 0.0036\n",
      "Epoch 333/500, Training Loss: 0.0035\n",
      "Epoch 334/500, Training Loss: 0.0030\n",
      "Epoch 335/500, Training Loss: 0.0026\n",
      "Epoch 336/500, Training Loss: 0.0030\n",
      "Epoch 337/500, Training Loss: 0.0035\n",
      "Epoch 338/500, Training Loss: 0.0027\n",
      "Epoch 339/500, Training Loss: 0.0033\n",
      "Epoch 340/500, Training Loss: 0.0026\n",
      "Epoch 341/500, Training Loss: 0.0029\n",
      "Epoch 342/500, Training Loss: 0.0029\n",
      "Epoch 343/500, Training Loss: 0.0019\n",
      "Epoch 344/500, Training Loss: 0.0030\n",
      "Epoch 345/500, Training Loss: 0.0029\n",
      "Epoch 346/500, Training Loss: 0.0027\n",
      "Epoch 347/500, Training Loss: 0.0041\n",
      "Epoch 348/500, Training Loss: 0.0033\n",
      "Epoch 349/500, Training Loss: 0.0028\n",
      "Epoch 350/500, Training Loss: 0.0023\n",
      "Epoch 351/500, Training Loss: 0.0035\n",
      "Epoch 352/500, Training Loss: 0.0033\n",
      "Epoch 353/500, Training Loss: 0.0029\n",
      "Epoch 354/500, Training Loss: 0.0034\n",
      "Epoch 355/500, Training Loss: 0.0029\n",
      "Epoch 356/500, Training Loss: 0.0041\n",
      "Epoch 357/500, Training Loss: 0.0029\n",
      "Epoch 358/500, Training Loss: 0.0030\n",
      "Epoch 359/500, Training Loss: 0.0033\n",
      "Epoch 360/500, Training Loss: 0.0035\n",
      "Epoch 361/500, Training Loss: 0.0032\n",
      "Epoch 362/500, Training Loss: 0.0035\n",
      "Epoch 363/500, Training Loss: 0.0030\n",
      "Epoch 364/500, Training Loss: 0.0026\n",
      "Epoch 365/500, Training Loss: 0.0031\n",
      "Epoch 366/500, Training Loss: 0.0029\n",
      "Epoch 367/500, Training Loss: 0.0029\n",
      "Epoch 368/500, Training Loss: 0.0024\n",
      "Epoch 369/500, Training Loss: 0.0026\n",
      "Epoch 370/500, Training Loss: 0.0029\n",
      "Epoch 371/500, Training Loss: 0.0021\n",
      "Epoch 372/500, Training Loss: 0.0030\n",
      "Epoch 373/500, Training Loss: 0.0023\n",
      "Epoch 374/500, Training Loss: 0.0029\n",
      "Epoch 375/500, Training Loss: 0.0029\n",
      "Epoch 376/500, Training Loss: 0.0044\n",
      "Epoch 377/500, Training Loss: 0.0033\n",
      "Epoch 378/500, Training Loss: 0.0031\n",
      "Epoch 379/500, Training Loss: 0.0032\n",
      "Epoch 380/500, Training Loss: 0.0027\n",
      "Epoch 381/500, Training Loss: 0.0037\n",
      "Epoch 382/500, Training Loss: 0.0035\n",
      "Epoch 383/500, Training Loss: 0.0024\n",
      "Epoch 384/500, Training Loss: 0.0024\n",
      "Epoch 385/500, Training Loss: 0.0030\n",
      "Epoch 386/500, Training Loss: 0.0025\n",
      "Epoch 387/500, Training Loss: 0.0032\n",
      "Epoch 388/500, Training Loss: 0.0022\n",
      "Epoch 389/500, Training Loss: 0.0029\n",
      "Epoch 390/500, Training Loss: 0.0038\n",
      "Epoch 391/500, Training Loss: 0.0025\n",
      "Epoch 392/500, Training Loss: 0.0025\n",
      "Epoch 393/500, Training Loss: 0.0031\n",
      "Epoch 394/500, Training Loss: 0.0028\n",
      "Epoch 395/500, Training Loss: 0.0034\n",
      "Epoch 396/500, Training Loss: 0.0030\n",
      "Epoch 397/500, Training Loss: 0.0031\n",
      "Epoch 398/500, Training Loss: 0.0031\n",
      "Epoch 399/500, Training Loss: 0.0034\n",
      "Epoch 400/500, Training Loss: 0.0042\n",
      "Epoch 401/500, Training Loss: 0.0027\n",
      "Epoch 402/500, Training Loss: 0.0027\n",
      "Epoch 403/500, Training Loss: 0.0028\n",
      "Epoch 404/500, Training Loss: 0.0025\n",
      "Epoch 405/500, Training Loss: 0.0029\n",
      "Epoch 406/500, Training Loss: 0.0021\n",
      "Epoch 407/500, Training Loss: 0.0034\n",
      "Epoch 408/500, Training Loss: 0.0029\n",
      "Epoch 409/500, Training Loss: 0.0037\n",
      "Epoch 410/500, Training Loss: 0.0026\n",
      "Epoch 411/500, Training Loss: 0.0034\n",
      "Epoch 412/500, Training Loss: 0.0030\n",
      "Epoch 413/500, Training Loss: 0.0027\n",
      "Epoch 414/500, Training Loss: 0.0026\n",
      "Epoch 415/500, Training Loss: 0.0037\n",
      "Epoch 416/500, Training Loss: 0.0030\n",
      "Epoch 417/500, Training Loss: 0.0036\n",
      "Epoch 418/500, Training Loss: 0.0027\n",
      "Epoch 419/500, Training Loss: 0.0026\n",
      "Epoch 420/500, Training Loss: 0.0024\n",
      "Epoch 421/500, Training Loss: 0.0034\n",
      "Epoch 422/500, Training Loss: 0.0030\n",
      "Epoch 423/500, Training Loss: 0.0027\n",
      "Epoch 424/500, Training Loss: 0.0021\n",
      "Epoch 425/500, Training Loss: 0.0032\n",
      "Epoch 426/500, Training Loss: 0.0033\n",
      "Epoch 427/500, Training Loss: 0.0030\n",
      "Epoch 428/500, Training Loss: 0.0031\n",
      "Epoch 429/500, Training Loss: 0.0028\n",
      "Epoch 430/500, Training Loss: 0.0028\n",
      "Epoch 431/500, Training Loss: 0.0027\n",
      "Epoch 432/500, Training Loss: 0.0031\n",
      "Epoch 433/500, Training Loss: 0.0028\n",
      "Epoch 434/500, Training Loss: 0.0025\n",
      "Epoch 435/500, Training Loss: 0.0032\n",
      "Epoch 436/500, Training Loss: 0.0036\n",
      "Epoch 437/500, Training Loss: 0.0024\n",
      "Epoch 438/500, Training Loss: 0.0038\n",
      "Epoch 439/500, Training Loss: 0.0029\n",
      "Epoch 440/500, Training Loss: 0.0030\n",
      "Epoch 441/500, Training Loss: 0.0023\n",
      "Epoch 442/500, Training Loss: 0.0027\n",
      "Epoch 443/500, Training Loss: 0.0028\n",
      "Epoch 444/500, Training Loss: 0.0029\n",
      "Epoch 445/500, Training Loss: 0.0028\n",
      "Epoch 446/500, Training Loss: 0.0029\n",
      "Epoch 447/500, Training Loss: 0.0029\n",
      "Epoch 448/500, Training Loss: 0.0036\n",
      "Epoch 449/500, Training Loss: 0.0023\n",
      "Epoch 450/500, Training Loss: 0.0026\n",
      "Epoch 451/500, Training Loss: 0.0023\n",
      "Epoch 452/500, Training Loss: 0.0024\n",
      "Epoch 453/500, Training Loss: 0.0032\n",
      "Epoch 454/500, Training Loss: 0.0035\n",
      "Epoch 455/500, Training Loss: 0.0031\n",
      "Epoch 456/500, Training Loss: 0.0029\n",
      "Epoch 457/500, Training Loss: 0.0022\n",
      "Epoch 458/500, Training Loss: 0.0032\n",
      "Epoch 459/500, Training Loss: 0.0022\n",
      "Epoch 460/500, Training Loss: 0.0030\n",
      "Epoch 461/500, Training Loss: 0.0028\n",
      "Epoch 462/500, Training Loss: 0.0027\n",
      "Epoch 463/500, Training Loss: 0.0046\n",
      "Epoch 464/500, Training Loss: 0.0041\n",
      "Epoch 465/500, Training Loss: 0.0029\n",
      "Epoch 466/500, Training Loss: 0.0025\n",
      "Epoch 467/500, Training Loss: 0.0034\n",
      "Epoch 468/500, Training Loss: 0.0029\n",
      "Epoch 469/500, Training Loss: 0.0035\n",
      "Epoch 470/500, Training Loss: 0.0033\n",
      "Epoch 471/500, Training Loss: 0.0032\n",
      "Epoch 472/500, Training Loss: 0.0030\n",
      "Epoch 473/500, Training Loss: 0.0036\n",
      "Epoch 474/500, Training Loss: 0.0028\n",
      "Epoch 475/500, Training Loss: 0.0033\n",
      "Epoch 476/500, Training Loss: 0.0022\n",
      "Epoch 477/500, Training Loss: 0.0031\n",
      "Epoch 478/500, Training Loss: 0.0027\n",
      "Epoch 479/500, Training Loss: 0.0033\n",
      "Epoch 480/500, Training Loss: 0.0034\n",
      "Epoch 481/500, Training Loss: 0.0030\n",
      "Epoch 482/500, Training Loss: 0.0028\n",
      "Epoch 483/500, Training Loss: 0.0029\n",
      "Epoch 484/500, Training Loss: 0.0031\n",
      "Epoch 485/500, Training Loss: 0.0035\n",
      "Epoch 486/500, Training Loss: 0.0034\n",
      "Epoch 487/500, Training Loss: 0.0037\n",
      "Epoch 488/500, Training Loss: 0.0028\n",
      "Epoch 489/500, Training Loss: 0.0033\n",
      "Epoch 490/500, Training Loss: 0.0023\n",
      "Epoch 491/500, Training Loss: 0.0027\n",
      "Epoch 492/500, Training Loss: 0.0023\n",
      "Epoch 493/500, Training Loss: 0.0029\n",
      "Epoch 494/500, Training Loss: 0.0023\n",
      "Epoch 495/500, Training Loss: 0.0016\n",
      "Epoch 496/500, Training Loss: 0.0023\n",
      "Epoch 497/500, Training Loss: 0.0028\n",
      "Epoch 498/500, Training Loss: 0.0029\n",
      "Epoch 499/500, Training Loss: 0.0024\n",
      "Epoch 500/500, Training Loss: 0.0034\n",
      "Validation Loss: 0.0012\n",
      "Model 0, Fold 0: Validation Loss = 0.0012\n",
      "Epoch 1/500, Training Loss: 0.0195\n",
      "Epoch 2/500, Training Loss: 0.0203\n",
      "Epoch 3/500, Training Loss: 0.0224\n",
      "Epoch 4/500, Training Loss: 0.0225\n",
      "Epoch 5/500, Training Loss: 0.0258\n",
      "Epoch 6/500, Training Loss: 0.0184\n",
      "Epoch 7/500, Training Loss: 0.0179\n",
      "Epoch 8/500, Training Loss: 0.0195\n",
      "Epoch 9/500, Training Loss: 0.0254\n",
      "Epoch 10/500, Training Loss: 0.0208\n",
      "Epoch 11/500, Training Loss: 0.0191\n",
      "Epoch 12/500, Training Loss: 0.0194\n",
      "Epoch 13/500, Training Loss: 0.0213\n",
      "Epoch 14/500, Training Loss: 0.0225\n",
      "Epoch 15/500, Training Loss: 0.0202\n",
      "Epoch 16/500, Training Loss: 0.0182\n",
      "Epoch 17/500, Training Loss: 0.0156\n",
      "Epoch 18/500, Training Loss: 0.0178\n",
      "Epoch 19/500, Training Loss: 0.0156\n",
      "Epoch 20/500, Training Loss: 0.0173\n",
      "Epoch 21/500, Training Loss: 0.0161\n",
      "Epoch 22/500, Training Loss: 0.0150\n",
      "Epoch 23/500, Training Loss: 0.0156\n",
      "Epoch 24/500, Training Loss: 0.0164\n",
      "Epoch 25/500, Training Loss: 0.0169\n",
      "Epoch 26/500, Training Loss: 0.0138\n",
      "Epoch 27/500, Training Loss: 0.0161\n",
      "Epoch 28/500, Training Loss: 0.0139\n",
      "Epoch 29/500, Training Loss: 0.0097\n",
      "Epoch 30/500, Training Loss: 0.0110\n",
      "Epoch 31/500, Training Loss: 0.0129\n",
      "Epoch 32/500, Training Loss: 0.0089\n",
      "Epoch 33/500, Training Loss: 0.0098\n",
      "Epoch 34/500, Training Loss: 0.0084\n",
      "Epoch 35/500, Training Loss: 0.0092\n",
      "Epoch 36/500, Training Loss: 0.0092\n",
      "Epoch 37/500, Training Loss: 0.0101\n",
      "Epoch 38/500, Training Loss: 0.0081\n",
      "Epoch 39/500, Training Loss: 0.0083\n",
      "Epoch 40/500, Training Loss: 0.0070\n",
      "Epoch 41/500, Training Loss: 0.0070\n",
      "Epoch 42/500, Training Loss: 0.0074\n",
      "Epoch 43/500, Training Loss: 0.0061\n",
      "Epoch 44/500, Training Loss: 0.0040\n",
      "Epoch 45/500, Training Loss: 0.0060\n",
      "Epoch 46/500, Training Loss: 0.0056\n",
      "Epoch 47/500, Training Loss: 0.0037\n",
      "Epoch 48/500, Training Loss: 0.0043\n",
      "Epoch 49/500, Training Loss: 0.0042\n",
      "Epoch 50/500, Training Loss: 0.0038\n",
      "Epoch 51/500, Training Loss: 0.0047\n",
      "Epoch 52/500, Training Loss: 0.0033\n",
      "Epoch 53/500, Training Loss: 0.0029\n",
      "Epoch 54/500, Training Loss: 0.0029\n",
      "Epoch 55/500, Training Loss: 0.0031\n",
      "Epoch 56/500, Training Loss: 0.0034\n",
      "Epoch 57/500, Training Loss: 0.0031\n",
      "Epoch 58/500, Training Loss: 0.0032\n",
      "Epoch 59/500, Training Loss: 0.0031\n",
      "Epoch 60/500, Training Loss: 0.0039\n",
      "Epoch 61/500, Training Loss: 0.0030\n",
      "Epoch 62/500, Training Loss: 0.0031\n",
      "Epoch 63/500, Training Loss: 0.0045\n",
      "Epoch 64/500, Training Loss: 0.0034\n",
      "Epoch 65/500, Training Loss: 0.0030\n",
      "Epoch 66/500, Training Loss: 0.0038\n",
      "Epoch 67/500, Training Loss: 0.0042\n",
      "Epoch 68/500, Training Loss: 0.0027\n",
      "Epoch 69/500, Training Loss: 0.0035\n",
      "Epoch 70/500, Training Loss: 0.0034\n",
      "Epoch 71/500, Training Loss: 0.0032\n",
      "Epoch 72/500, Training Loss: 0.0033\n",
      "Epoch 73/500, Training Loss: 0.0030\n",
      "Epoch 74/500, Training Loss: 0.0025\n",
      "Epoch 75/500, Training Loss: 0.0041\n",
      "Epoch 76/500, Training Loss: 0.0023\n",
      "Epoch 77/500, Training Loss: 0.0038\n",
      "Epoch 78/500, Training Loss: 0.0042\n",
      "Epoch 79/500, Training Loss: 0.0034\n",
      "Epoch 80/500, Training Loss: 0.0034\n",
      "Epoch 81/500, Training Loss: 0.0039\n",
      "Epoch 82/500, Training Loss: 0.0034\n",
      "Epoch 83/500, Training Loss: 0.0049\n",
      "Epoch 84/500, Training Loss: 0.0032\n",
      "Epoch 85/500, Training Loss: 0.0045\n",
      "Epoch 86/500, Training Loss: 0.0035\n",
      "Epoch 87/500, Training Loss: 0.0035\n",
      "Epoch 88/500, Training Loss: 0.0035\n",
      "Epoch 89/500, Training Loss: 0.0032\n",
      "Epoch 90/500, Training Loss: 0.0036\n",
      "Epoch 91/500, Training Loss: 0.0034\n",
      "Epoch 92/500, Training Loss: 0.0034\n",
      "Epoch 93/500, Training Loss: 0.0027\n",
      "Epoch 94/500, Training Loss: 0.0029\n",
      "Epoch 95/500, Training Loss: 0.0037\n",
      "Epoch 96/500, Training Loss: 0.0040\n",
      "Epoch 97/500, Training Loss: 0.0028\n",
      "Epoch 98/500, Training Loss: 0.0033\n",
      "Epoch 99/500, Training Loss: 0.0024\n",
      "Epoch 100/500, Training Loss: 0.0044\n",
      "Epoch 101/500, Training Loss: 0.0034\n",
      "Epoch 102/500, Training Loss: 0.0035\n",
      "Epoch 103/500, Training Loss: 0.0047\n",
      "Epoch 104/500, Training Loss: 0.0026\n",
      "Epoch 105/500, Training Loss: 0.0031\n",
      "Epoch 106/500, Training Loss: 0.0036\n",
      "Epoch 107/500, Training Loss: 0.0031\n",
      "Epoch 108/500, Training Loss: 0.0028\n",
      "Epoch 109/500, Training Loss: 0.0031\n",
      "Epoch 110/500, Training Loss: 0.0021\n",
      "Epoch 111/500, Training Loss: 0.0028\n",
      "Epoch 112/500, Training Loss: 0.0038\n",
      "Epoch 113/500, Training Loss: 0.0026\n",
      "Epoch 114/500, Training Loss: 0.0036\n",
      "Epoch 115/500, Training Loss: 0.0031\n",
      "Epoch 116/500, Training Loss: 0.0031\n",
      "Epoch 117/500, Training Loss: 0.0041\n",
      "Epoch 118/500, Training Loss: 0.0033\n",
      "Epoch 119/500, Training Loss: 0.0033\n",
      "Epoch 120/500, Training Loss: 0.0028\n",
      "Epoch 121/500, Training Loss: 0.0040\n",
      "Epoch 122/500, Training Loss: 0.0042\n",
      "Epoch 123/500, Training Loss: 0.0031\n",
      "Epoch 124/500, Training Loss: 0.0046\n",
      "Epoch 125/500, Training Loss: 0.0032\n",
      "Epoch 126/500, Training Loss: 0.0040\n",
      "Epoch 127/500, Training Loss: 0.0027\n",
      "Epoch 128/500, Training Loss: 0.0037\n",
      "Epoch 129/500, Training Loss: 0.0037\n",
      "Epoch 130/500, Training Loss: 0.0034\n",
      "Epoch 131/500, Training Loss: 0.0029\n",
      "Epoch 132/500, Training Loss: 0.0032\n",
      "Epoch 133/500, Training Loss: 0.0046\n",
      "Epoch 134/500, Training Loss: 0.0034\n",
      "Epoch 135/500, Training Loss: 0.0031\n",
      "Epoch 136/500, Training Loss: 0.0045\n",
      "Epoch 137/500, Training Loss: 0.0034\n",
      "Epoch 138/500, Training Loss: 0.0026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m Uhat \u001b[38;5;241m=\u001b[39m final_model_l2\u001b[38;5;241m.\u001b[39minverse_transformation(X,Y)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      5\u001b[0m loss\u001b[38;5;241m=\u001b[39m loss_factory\u001b[38;5;241m.\u001b[39mbuild_loss(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHSIC\u001b[39m\u001b[38;5;124m\"\u001b[39m, X, Uhat, subsamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m final_model_hsic, (best_index_hsic, val_loss_hsic) \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_val_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyper_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchoose_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest overall model index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_index_hsic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with average validation loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss_hsic\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Cocycles/causal_cocycle/optimise_new.py:339\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(models, loss, inputs, outputs, loss_val, method, train_val_split, opt_kwargs, hyper_kwargs, choose_best_model, retrain)\u001b[0m\n\u001b[1;32m    334\u001b[0m current_opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs_val\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Call the optimise function.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Note: We assume optimise has the following signature:\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m#   optimise(model, loss_tr, inputs, outputs, **current_opts)\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m train_losses, curr_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcurrent_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m val_losses[m, k] \u001b[38;5;241m=\u001b[39m curr_val_loss \u001b[38;5;28;01mif\u001b[39;00m curr_val_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    342\u001b[0m fold_models\u001b[38;5;241m.\u001b[39mappend(model_copy)\n",
      "File \u001b[0;32m~/Cocycles/causal_cocycle/optimise_new.py:143\u001b[0m, in \u001b[0;36moptimise\u001b[0;34m(model, loss_tr, inputs, outputs, inputs_val, outputs_val, learn_rate, epochs, weight_decay, batch_size, val_batch_size, scheduler, schedule_milestone, lr_mult, print_, plot, likelihood_param_opt, likelihood_param_lr, loss_val)\u001b[0m\n\u001b[1;32m    140\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m outputs[indices]\n\u001b[1;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 143\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_tr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m loss_value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Cocycles/causal_cocycle/loss.py:15\u001b[0m, in \u001b[0;36mCocycleLoss.__call__\u001b[0;34m(self, model, inputs, outputs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, outputs):\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Cocycles/causal_cocycle/loss.py:12\u001b[0m, in \u001b[0;36mCocycleLoss.forward\u001b[0;34m(self, model, inputs, outputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, outputs):\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Cocycles/causal_cocycle/loss_factory.py:62\u001b[0m, in \u001b[0;36mCocycleLossFactory._hsic\u001b[0;34m(self, model, inputs, outputs)\u001b[0m\n\u001b[1;32m     60\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs)\n\u001b[1;32m     61\u001b[0m K_xx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_gram(inputs, inputs)\n\u001b[0;32m---> 62\u001b[0m K_uu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(m, device\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mm)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones((m, m), device\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtrace(K_xx \u001b[38;5;241m@\u001b[39m H \u001b[38;5;241m@\u001b[39m K_uu \u001b[38;5;241m@\u001b[39m H) \u001b[38;5;241m/\u001b[39m (m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Cocycles/causal_cocycle/kernels.py:22\u001b[0m, in \u001b[0;36mgaussian_kernel.get_gram\u001b[0;34m(self, X, Z)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gram\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Z):\n\u001b[0;32m---> 22\u001b[0m     K_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlengthscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlengthscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)            \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m K_xx\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "File \u001b[0;32m~/.local/mambaforge/envs/cocycles/lib/python3.12/site-packages/torch/functional.py:1315\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1313\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training with hsic\n",
    "kernel = [gaussian_kernel()] * 2\n",
    "loss_factory = CocycleLossFactory(kernel)\n",
    "Uhat = final_model_l2.inverse_transformation(X,Y)[0].detach()\n",
    "loss= loss_factory.build_loss(\"HSIC\", X, Uhat, subsamples=10**4)\n",
    "final_model_hsic, (best_index_hsic, val_loss_hsic) = validate(\n",
    "        models,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs=hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_hsic} with average validation loss {val_loss_hsic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7686d3a-94d1-4633-82d1-8ca64ff9b68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Training Loss: 0.0719\n",
      "Epoch 2/500, Training Loss: -0.0094\n",
      "Epoch 3/500, Training Loss: 0.0100\n",
      "Epoch 4/500, Training Loss: -0.0226\n",
      "Epoch 5/500, Training Loss: -0.0157\n",
      "Epoch 6/500, Training Loss: -0.0751\n",
      "Epoch 7/500, Training Loss: -0.0490\n",
      "Epoch 8/500, Training Loss: -0.0620\n",
      "Epoch 9/500, Training Loss: -0.1245\n",
      "Epoch 10/500, Training Loss: -0.0488\n",
      "Epoch 11/500, Training Loss: -0.1238\n",
      "Epoch 12/500, Training Loss: -0.1351\n",
      "Epoch 13/500, Training Loss: -0.1149\n",
      "Epoch 14/500, Training Loss: -0.1577\n",
      "Epoch 15/500, Training Loss: -0.1238\n",
      "Epoch 16/500, Training Loss: -0.1746\n",
      "Epoch 17/500, Training Loss: -0.2348\n",
      "Epoch 18/500, Training Loss: -0.2857\n",
      "Epoch 19/500, Training Loss: -0.2371\n",
      "Epoch 20/500, Training Loss: -0.2513\n",
      "Epoch 21/500, Training Loss: -0.2302\n",
      "Epoch 22/500, Training Loss: -0.3016\n",
      "Epoch 23/500, Training Loss: -0.3102\n",
      "Epoch 24/500, Training Loss: -0.2946\n",
      "Epoch 25/500, Training Loss: -0.3664\n",
      "Epoch 26/500, Training Loss: -0.4091\n",
      "Epoch 27/500, Training Loss: -0.3874\n",
      "Epoch 28/500, Training Loss: -0.4653\n",
      "Epoch 29/500, Training Loss: -0.4303\n",
      "Epoch 30/500, Training Loss: -0.5024\n",
      "Epoch 31/500, Training Loss: -0.4346\n",
      "Epoch 32/500, Training Loss: -0.4856\n",
      "Epoch 33/500, Training Loss: -0.4941\n",
      "Epoch 34/500, Training Loss: -0.4844\n",
      "Epoch 35/500, Training Loss: -0.4809\n",
      "Epoch 36/500, Training Loss: -0.5485\n",
      "Epoch 37/500, Training Loss: -0.5125\n",
      "Epoch 38/500, Training Loss: -0.5573\n",
      "Epoch 39/500, Training Loss: -0.5221\n",
      "Epoch 40/500, Training Loss: -0.4980\n",
      "Epoch 41/500, Training Loss: -0.5697\n",
      "Epoch 42/500, Training Loss: -0.5271\n",
      "Epoch 43/500, Training Loss: -0.5034\n",
      "Epoch 44/500, Training Loss: -0.5474\n",
      "Epoch 45/500, Training Loss: -0.5787\n",
      "Epoch 46/500, Training Loss: -0.5720\n",
      "Epoch 47/500, Training Loss: -0.5942\n",
      "Epoch 48/500, Training Loss: -0.5098\n",
      "Epoch 49/500, Training Loss: -0.5724\n",
      "Epoch 50/500, Training Loss: -0.5555\n",
      "Epoch 51/500, Training Loss: -0.5560\n",
      "Epoch 52/500, Training Loss: -0.5303\n",
      "Epoch 53/500, Training Loss: -0.5492\n",
      "Epoch 54/500, Training Loss: -0.5650\n",
      "Epoch 55/500, Training Loss: -0.5676\n",
      "Epoch 56/500, Training Loss: -0.5245\n",
      "Epoch 57/500, Training Loss: -0.5465\n",
      "Epoch 58/500, Training Loss: -0.5256\n",
      "Epoch 59/500, Training Loss: -0.5504\n",
      "Epoch 60/500, Training Loss: -0.5467\n",
      "Epoch 61/500, Training Loss: -0.5745\n",
      "Epoch 62/500, Training Loss: -0.5688\n",
      "Epoch 63/500, Training Loss: -0.5406\n",
      "Epoch 64/500, Training Loss: -0.5950\n",
      "Epoch 65/500, Training Loss: -0.5703\n",
      "Epoch 66/500, Training Loss: -0.5832\n",
      "Epoch 67/500, Training Loss: -0.5555\n",
      "Epoch 68/500, Training Loss: -0.5626\n",
      "Epoch 69/500, Training Loss: -0.5738\n",
      "Epoch 70/500, Training Loss: -0.5439\n",
      "Epoch 71/500, Training Loss: -0.5201\n",
      "Epoch 72/500, Training Loss: -0.5595\n",
      "Epoch 73/500, Training Loss: -0.5704\n",
      "Epoch 74/500, Training Loss: -0.5351\n",
      "Epoch 75/500, Training Loss: -0.5746\n",
      "Epoch 76/500, Training Loss: -0.5998\n",
      "Epoch 77/500, Training Loss: -0.5734\n",
      "Epoch 78/500, Training Loss: -0.5917\n",
      "Epoch 79/500, Training Loss: -0.5586\n",
      "Epoch 80/500, Training Loss: -0.5676\n",
      "Epoch 81/500, Training Loss: -0.6079\n",
      "Epoch 82/500, Training Loss: -0.5698\n",
      "Epoch 83/500, Training Loss: -0.5562\n",
      "Epoch 84/500, Training Loss: -0.5712\n",
      "Epoch 85/500, Training Loss: -0.5751\n",
      "Epoch 86/500, Training Loss: -0.5477\n",
      "Epoch 87/500, Training Loss: -0.5573\n",
      "Epoch 88/500, Training Loss: -0.5544\n",
      "Epoch 89/500, Training Loss: -0.6009\n",
      "Epoch 90/500, Training Loss: -0.5780\n",
      "Epoch 91/500, Training Loss: -0.5537\n",
      "Epoch 92/500, Training Loss: -0.5296\n",
      "Epoch 93/500, Training Loss: -0.6058\n",
      "Epoch 94/500, Training Loss: -0.5696\n",
      "Epoch 95/500, Training Loss: -0.6177\n",
      "Epoch 96/500, Training Loss: -0.5907\n",
      "Epoch 97/500, Training Loss: -0.5628\n",
      "Epoch 98/500, Training Loss: -0.5644\n",
      "Epoch 99/500, Training Loss: -0.5560\n",
      "Epoch 100/500, Training Loss: -0.5916\n",
      "Epoch 101/500, Training Loss: -0.5402\n",
      "Epoch 102/500, Training Loss: -0.5695\n",
      "Epoch 103/500, Training Loss: -0.5628\n",
      "Epoch 104/500, Training Loss: -0.5769\n",
      "Epoch 105/500, Training Loss: -0.5910\n",
      "Epoch 106/500, Training Loss: -0.5602\n",
      "Epoch 107/500, Training Loss: -0.5449\n",
      "Epoch 108/500, Training Loss: -0.5543\n",
      "Epoch 109/500, Training Loss: -0.5572\n",
      "Epoch 110/500, Training Loss: -0.5712\n",
      "Epoch 111/500, Training Loss: -0.5382\n",
      "Epoch 112/500, Training Loss: -0.6020\n",
      "Epoch 113/500, Training Loss: -0.5752\n",
      "Epoch 114/500, Training Loss: -0.5428\n",
      "Epoch 115/500, Training Loss: -0.5840\n",
      "Epoch 116/500, Training Loss: -0.6088\n",
      "Epoch 117/500, Training Loss: -0.5679\n",
      "Epoch 118/500, Training Loss: -0.5986\n",
      "Epoch 119/500, Training Loss: -0.5313\n",
      "Epoch 120/500, Training Loss: -0.5610\n",
      "Epoch 121/500, Training Loss: -0.5680\n",
      "Epoch 122/500, Training Loss: -0.5712\n",
      "Epoch 123/500, Training Loss: -0.5512\n",
      "Epoch 124/500, Training Loss: -0.5131\n",
      "Epoch 125/500, Training Loss: -0.5795\n",
      "Epoch 126/500, Training Loss: -0.5582\n",
      "Epoch 127/500, Training Loss: -0.5518\n",
      "Epoch 128/500, Training Loss: -0.5406\n",
      "Epoch 129/500, Training Loss: -0.5713\n",
      "Epoch 130/500, Training Loss: -0.5436\n",
      "Epoch 131/500, Training Loss: -0.5726\n",
      "Epoch 132/500, Training Loss: -0.6045\n",
      "Epoch 133/500, Training Loss: -0.5529\n",
      "Epoch 134/500, Training Loss: -0.5453\n",
      "Epoch 135/500, Training Loss: -0.6137\n",
      "Epoch 136/500, Training Loss: -0.5971\n",
      "Epoch 137/500, Training Loss: -0.5138\n",
      "Epoch 138/500, Training Loss: -0.5549\n",
      "Epoch 139/500, Training Loss: -0.5401\n",
      "Epoch 140/500, Training Loss: -0.5661\n",
      "Epoch 141/500, Training Loss: -0.5550\n",
      "Epoch 142/500, Training Loss: -0.5306\n",
      "Epoch 143/500, Training Loss: -0.5610\n",
      "Epoch 144/500, Training Loss: -0.5320\n",
      "Epoch 145/500, Training Loss: -0.5331\n",
      "Epoch 146/500, Training Loss: -0.5989\n",
      "Epoch 147/500, Training Loss: -0.5702\n",
      "Epoch 148/500, Training Loss: -0.5739\n",
      "Epoch 149/500, Training Loss: -0.5998\n",
      "Epoch 150/500, Training Loss: -0.5455\n",
      "Epoch 151/500, Training Loss: -0.5349\n",
      "Epoch 152/500, Training Loss: -0.5791\n",
      "Epoch 153/500, Training Loss: -0.5236\n",
      "Epoch 154/500, Training Loss: -0.5486\n",
      "Epoch 155/500, Training Loss: -0.5452\n",
      "Epoch 156/500, Training Loss: -0.5439\n",
      "Epoch 157/500, Training Loss: -0.5838\n",
      "Epoch 158/500, Training Loss: -0.5639\n",
      "Epoch 159/500, Training Loss: -0.5557\n",
      "Epoch 160/500, Training Loss: -0.6195\n",
      "Epoch 161/500, Training Loss: -0.5787\n",
      "Epoch 162/500, Training Loss: -0.5350\n",
      "Epoch 163/500, Training Loss: -0.5114\n",
      "Epoch 164/500, Training Loss: -0.5450\n",
      "Epoch 165/500, Training Loss: -0.5649\n",
      "Epoch 166/500, Training Loss: -0.5477\n",
      "Epoch 167/500, Training Loss: -0.5730\n",
      "Epoch 168/500, Training Loss: -0.5776\n",
      "Epoch 169/500, Training Loss: -0.5550\n",
      "Epoch 170/500, Training Loss: -0.5930\n",
      "Epoch 171/500, Training Loss: -0.5404\n",
      "Epoch 172/500, Training Loss: -0.5676\n",
      "Epoch 173/500, Training Loss: -0.5557\n",
      "Epoch 174/500, Training Loss: -0.5747\n",
      "Epoch 175/500, Training Loss: -0.5666\n",
      "Epoch 176/500, Training Loss: -0.5678\n",
      "Epoch 177/500, Training Loss: -0.5370\n",
      "Epoch 178/500, Training Loss: -0.6026\n",
      "Epoch 179/500, Training Loss: -0.5586\n",
      "Epoch 180/500, Training Loss: -0.5634\n",
      "Epoch 181/500, Training Loss: -0.5564\n",
      "Epoch 182/500, Training Loss: -0.5601\n",
      "Epoch 183/500, Training Loss: -0.5548\n",
      "Epoch 184/500, Training Loss: -0.5490\n",
      "Epoch 185/500, Training Loss: -0.5577\n",
      "Epoch 186/500, Training Loss: -0.5280\n",
      "Epoch 187/500, Training Loss: -0.5666\n",
      "Epoch 188/500, Training Loss: -0.5344\n",
      "Epoch 189/500, Training Loss: -0.5643\n",
      "Epoch 190/500, Training Loss: -0.5690\n",
      "Epoch 191/500, Training Loss: -0.5345\n",
      "Epoch 192/500, Training Loss: -0.5831\n",
      "Epoch 193/500, Training Loss: -0.5863\n",
      "Epoch 194/500, Training Loss: -0.5607\n",
      "Epoch 195/500, Training Loss: -0.5812\n",
      "Epoch 196/500, Training Loss: -0.5732\n",
      "Epoch 197/500, Training Loss: -0.5755\n",
      "Epoch 198/500, Training Loss: -0.5610\n",
      "Epoch 199/500, Training Loss: -0.5730\n",
      "Epoch 200/500, Training Loss: -0.5683\n",
      "Epoch 201/500, Training Loss: -0.5920\n",
      "Epoch 202/500, Training Loss: -0.5844\n",
      "Epoch 203/500, Training Loss: -0.5408\n",
      "Epoch 204/500, Training Loss: -0.5922\n",
      "Epoch 205/500, Training Loss: -0.5442\n",
      "Epoch 206/500, Training Loss: -0.5931\n",
      "Epoch 207/500, Training Loss: -0.5605\n",
      "Epoch 208/500, Training Loss: -0.5520\n",
      "Epoch 209/500, Training Loss: -0.5595\n",
      "Epoch 210/500, Training Loss: -0.5361\n",
      "Epoch 211/500, Training Loss: -0.5549\n",
      "Epoch 212/500, Training Loss: -0.5463\n",
      "Epoch 213/500, Training Loss: -0.5594\n",
      "Epoch 214/500, Training Loss: -0.5445\n",
      "Epoch 215/500, Training Loss: -0.5583\n",
      "Epoch 216/500, Training Loss: -0.5200\n",
      "Epoch 217/500, Training Loss: -0.6015\n",
      "Epoch 218/500, Training Loss: -0.5764\n",
      "Epoch 219/500, Training Loss: -0.5673\n",
      "Epoch 220/500, Training Loss: -0.5542\n",
      "Epoch 221/500, Training Loss: -0.5447\n",
      "Epoch 222/500, Training Loss: -0.5177\n",
      "Epoch 223/500, Training Loss: -0.5633\n",
      "Epoch 224/500, Training Loss: -0.5721\n",
      "Epoch 225/500, Training Loss: -0.5588\n",
      "Epoch 226/500, Training Loss: -0.5154\n",
      "Epoch 227/500, Training Loss: -0.5291\n",
      "Epoch 228/500, Training Loss: -0.6036\n",
      "Epoch 229/500, Training Loss: -0.5565\n",
      "Epoch 230/500, Training Loss: -0.5559\n",
      "Epoch 231/500, Training Loss: -0.5286\n",
      "Epoch 232/500, Training Loss: -0.5269\n",
      "Epoch 233/500, Training Loss: -0.5857\n",
      "Epoch 234/500, Training Loss: -0.5676\n",
      "Epoch 235/500, Training Loss: -0.5621\n",
      "Epoch 236/500, Training Loss: -0.5521\n",
      "Epoch 237/500, Training Loss: -0.6020\n",
      "Epoch 238/500, Training Loss: -0.5427\n",
      "Epoch 239/500, Training Loss: -0.5818\n",
      "Epoch 240/500, Training Loss: -0.5930\n",
      "Epoch 241/500, Training Loss: -0.5873\n",
      "Epoch 242/500, Training Loss: -0.5614\n",
      "Epoch 243/500, Training Loss: -0.5777\n",
      "Epoch 244/500, Training Loss: -0.5581\n",
      "Epoch 245/500, Training Loss: -0.5705\n",
      "Epoch 246/500, Training Loss: -0.5834\n",
      "Epoch 247/500, Training Loss: -0.5516\n",
      "Epoch 248/500, Training Loss: -0.5422\n",
      "Epoch 249/500, Training Loss: -0.5850\n",
      "Epoch 250/500, Training Loss: -0.5821\n",
      "Epoch 251/500, Training Loss: -0.5645\n",
      "Epoch 252/500, Training Loss: -0.5352\n",
      "Epoch 253/500, Training Loss: -0.5643\n",
      "Epoch 254/500, Training Loss: -0.5546\n",
      "Epoch 255/500, Training Loss: -0.5890\n",
      "Epoch 256/500, Training Loss: -0.5549\n",
      "Epoch 257/500, Training Loss: -0.5571\n",
      "Epoch 258/500, Training Loss: -0.5371\n",
      "Epoch 259/500, Training Loss: -0.6246\n",
      "Epoch 260/500, Training Loss: -0.6010\n",
      "Epoch 261/500, Training Loss: -0.5812\n",
      "Epoch 262/500, Training Loss: -0.5509\n",
      "Epoch 263/500, Training Loss: -0.5874\n",
      "Epoch 264/500, Training Loss: -0.5582\n",
      "Epoch 265/500, Training Loss: -0.6132\n",
      "Epoch 266/500, Training Loss: -0.5950\n",
      "Epoch 267/500, Training Loss: -0.5419\n",
      "Epoch 268/500, Training Loss: -0.5372\n",
      "Epoch 269/500, Training Loss: -0.5762\n",
      "Epoch 270/500, Training Loss: -0.5328\n",
      "Epoch 271/500, Training Loss: -0.6045\n",
      "Epoch 272/500, Training Loss: -0.5570\n",
      "Epoch 273/500, Training Loss: -0.5560\n",
      "Epoch 274/500, Training Loss: -0.5526\n",
      "Epoch 275/500, Training Loss: -0.5723\n",
      "Epoch 276/500, Training Loss: -0.5527\n",
      "Epoch 277/500, Training Loss: -0.5835\n",
      "Epoch 278/500, Training Loss: -0.5503\n",
      "Epoch 279/500, Training Loss: -0.5549\n",
      "Epoch 280/500, Training Loss: -0.5600\n",
      "Epoch 281/500, Training Loss: -0.5921\n",
      "Epoch 282/500, Training Loss: -0.5422\n",
      "Epoch 283/500, Training Loss: -0.5666\n",
      "Epoch 284/500, Training Loss: -0.5513\n",
      "Epoch 285/500, Training Loss: -0.5712\n",
      "Epoch 286/500, Training Loss: -0.5857\n",
      "Epoch 287/500, Training Loss: -0.5253\n",
      "Epoch 288/500, Training Loss: -0.5514\n",
      "Epoch 289/500, Training Loss: -0.5777\n",
      "Epoch 290/500, Training Loss: -0.5915\n",
      "Epoch 291/500, Training Loss: -0.5837\n",
      "Epoch 292/500, Training Loss: -0.5595\n",
      "Epoch 293/500, Training Loss: -0.5651\n",
      "Epoch 294/500, Training Loss: -0.5704\n",
      "Epoch 295/500, Training Loss: -0.5516\n",
      "Epoch 296/500, Training Loss: -0.5638\n",
      "Epoch 297/500, Training Loss: -0.5959\n",
      "Epoch 298/500, Training Loss: -0.5681\n",
      "Epoch 299/500, Training Loss: -0.5583\n",
      "Epoch 300/500, Training Loss: -0.5636\n",
      "Epoch 301/500, Training Loss: -0.5418\n",
      "Epoch 302/500, Training Loss: -0.5475\n",
      "Epoch 303/500, Training Loss: -0.5515\n",
      "Epoch 304/500, Training Loss: -0.5572\n",
      "Epoch 305/500, Training Loss: -0.5764\n",
      "Epoch 306/500, Training Loss: -0.5825\n",
      "Epoch 307/500, Training Loss: -0.5592\n",
      "Epoch 308/500, Training Loss: -0.5627\n",
      "Epoch 309/500, Training Loss: -0.5670\n",
      "Epoch 310/500, Training Loss: -0.5645\n",
      "Epoch 311/500, Training Loss: -0.5894\n",
      "Epoch 312/500, Training Loss: -0.5265\n",
      "Epoch 313/500, Training Loss: -0.5759\n",
      "Epoch 314/500, Training Loss: -0.5654\n",
      "Epoch 315/500, Training Loss: -0.5370\n",
      "Epoch 316/500, Training Loss: -0.5653\n",
      "Epoch 317/500, Training Loss: -0.5774\n",
      "Epoch 318/500, Training Loss: -0.5839\n",
      "Epoch 319/500, Training Loss: -0.5451\n",
      "Epoch 320/500, Training Loss: -0.5592\n",
      "Epoch 321/500, Training Loss: -0.5525\n",
      "Epoch 322/500, Training Loss: -0.5720\n",
      "Epoch 323/500, Training Loss: -0.5675\n",
      "Epoch 324/500, Training Loss: -0.5493\n",
      "Epoch 325/500, Training Loss: -0.5844\n",
      "Epoch 326/500, Training Loss: -0.5955\n",
      "Epoch 327/500, Training Loss: -0.5948\n",
      "Epoch 328/500, Training Loss: -0.5301\n",
      "Epoch 329/500, Training Loss: -0.5257\n",
      "Epoch 330/500, Training Loss: -0.5771\n",
      "Epoch 331/500, Training Loss: -0.5661\n",
      "Epoch 332/500, Training Loss: -0.5497\n",
      "Epoch 333/500, Training Loss: -0.5890\n",
      "Epoch 334/500, Training Loss: -0.5135\n",
      "Epoch 335/500, Training Loss: -0.5687\n",
      "Epoch 336/500, Training Loss: -0.6024\n",
      "Epoch 337/500, Training Loss: -0.5824\n",
      "Epoch 338/500, Training Loss: -0.5492\n",
      "Epoch 339/500, Training Loss: -0.5607\n",
      "Epoch 340/500, Training Loss: -0.6090\n",
      "Epoch 341/500, Training Loss: -0.5314\n",
      "Epoch 342/500, Training Loss: -0.5911\n",
      "Epoch 343/500, Training Loss: -0.5931\n",
      "Epoch 344/500, Training Loss: -0.5573\n",
      "Epoch 345/500, Training Loss: -0.5718\n",
      "Epoch 346/500, Training Loss: -0.5668\n",
      "Epoch 347/500, Training Loss: -0.5429\n",
      "Epoch 348/500, Training Loss: -0.5903\n",
      "Epoch 349/500, Training Loss: -0.5886\n",
      "Epoch 350/500, Training Loss: -0.5721\n",
      "Epoch 351/500, Training Loss: -0.5702\n",
      "Epoch 352/500, Training Loss: -0.5481\n",
      "Epoch 353/500, Training Loss: -0.5733\n",
      "Epoch 354/500, Training Loss: -0.5205\n",
      "Epoch 355/500, Training Loss: -0.5474\n",
      "Epoch 356/500, Training Loss: -0.5398\n",
      "Epoch 357/500, Training Loss: -0.5378\n",
      "Epoch 358/500, Training Loss: -0.5544\n",
      "Epoch 359/500, Training Loss: -0.5552\n",
      "Epoch 360/500, Training Loss: -0.5768\n",
      "Epoch 361/500, Training Loss: -0.5162\n",
      "Epoch 362/500, Training Loss: -0.5966\n",
      "Epoch 363/500, Training Loss: -0.5287\n",
      "Epoch 364/500, Training Loss: -0.5740\n",
      "Epoch 365/500, Training Loss: -0.5692\n",
      "Epoch 366/500, Training Loss: -0.5732\n",
      "Epoch 367/500, Training Loss: -0.5974\n",
      "Epoch 368/500, Training Loss: -0.5862\n",
      "Epoch 369/500, Training Loss: -0.5565\n",
      "Epoch 370/500, Training Loss: -0.5337\n",
      "Epoch 371/500, Training Loss: -0.5313\n",
      "Epoch 372/500, Training Loss: -0.5814\n",
      "Epoch 373/500, Training Loss: -0.5460\n",
      "Epoch 374/500, Training Loss: -0.5459\n",
      "Epoch 375/500, Training Loss: -0.5640\n",
      "Epoch 376/500, Training Loss: -0.5668\n",
      "Epoch 377/500, Training Loss: -0.5415\n",
      "Epoch 378/500, Training Loss: -0.5591\n",
      "Epoch 379/500, Training Loss: -0.5928\n",
      "Epoch 380/500, Training Loss: -0.5123\n",
      "Epoch 381/500, Training Loss: -0.5577\n",
      "Epoch 382/500, Training Loss: -0.5477\n",
      "Epoch 383/500, Training Loss: -0.5602\n",
      "Epoch 384/500, Training Loss: -0.5710\n",
      "Epoch 385/500, Training Loss: -0.5719\n",
      "Epoch 386/500, Training Loss: -0.5814\n",
      "Epoch 387/500, Training Loss: -0.5819\n",
      "Epoch 388/500, Training Loss: -0.5652\n",
      "Epoch 389/500, Training Loss: -0.5554\n",
      "Epoch 390/500, Training Loss: -0.5625\n",
      "Epoch 391/500, Training Loss: -0.5677\n",
      "Epoch 392/500, Training Loss: -0.5603\n",
      "Epoch 393/500, Training Loss: -0.5735\n",
      "Epoch 394/500, Training Loss: -0.5833\n",
      "Epoch 395/500, Training Loss: -0.5769\n",
      "Epoch 396/500, Training Loss: -0.5879\n",
      "Epoch 397/500, Training Loss: -0.5853\n",
      "Epoch 398/500, Training Loss: -0.5355\n",
      "Epoch 399/500, Training Loss: -0.5533\n",
      "Epoch 400/500, Training Loss: -0.5679\n",
      "Epoch 401/500, Training Loss: -0.5082\n",
      "Epoch 402/500, Training Loss: -0.5603\n",
      "Epoch 403/500, Training Loss: -0.5347\n",
      "Epoch 404/500, Training Loss: -0.6148\n",
      "Epoch 405/500, Training Loss: -0.5844\n",
      "Epoch 406/500, Training Loss: -0.5728\n",
      "Epoch 407/500, Training Loss: -0.5619\n",
      "Epoch 408/500, Training Loss: -0.5444\n",
      "Epoch 409/500, Training Loss: -0.5583\n",
      "Epoch 410/500, Training Loss: -0.5569\n",
      "Epoch 411/500, Training Loss: -0.5484\n",
      "Epoch 412/500, Training Loss: -0.5567\n",
      "Epoch 413/500, Training Loss: -0.5719\n",
      "Epoch 414/500, Training Loss: -0.5757\n",
      "Epoch 415/500, Training Loss: -0.5317\n",
      "Epoch 416/500, Training Loss: -0.5524\n",
      "Epoch 417/500, Training Loss: -0.5804\n",
      "Epoch 418/500, Training Loss: -0.5288\n",
      "Epoch 419/500, Training Loss: -0.5188\n",
      "Epoch 420/500, Training Loss: -0.5844\n",
      "Epoch 421/500, Training Loss: -0.5735\n",
      "Epoch 422/500, Training Loss: -0.5722\n",
      "Epoch 423/500, Training Loss: -0.6006\n",
      "Epoch 424/500, Training Loss: -0.5517\n",
      "Epoch 425/500, Training Loss: -0.5444\n",
      "Epoch 426/500, Training Loss: -0.5928\n",
      "Epoch 427/500, Training Loss: -0.5547\n",
      "Epoch 428/500, Training Loss: -0.5829\n",
      "Epoch 429/500, Training Loss: -0.5591\n",
      "Epoch 430/500, Training Loss: -0.5848\n",
      "Epoch 431/500, Training Loss: -0.5344\n",
      "Epoch 432/500, Training Loss: -0.5210\n",
      "Epoch 433/500, Training Loss: -0.5530\n",
      "Epoch 434/500, Training Loss: -0.5965\n",
      "Epoch 435/500, Training Loss: -0.5580\n",
      "Epoch 436/500, Training Loss: -0.5252\n",
      "Epoch 437/500, Training Loss: -0.6134\n",
      "Epoch 438/500, Training Loss: -0.5649\n",
      "Epoch 439/500, Training Loss: -0.5490\n",
      "Epoch 440/500, Training Loss: -0.5343\n",
      "Epoch 441/500, Training Loss: -0.5681\n",
      "Epoch 442/500, Training Loss: -0.5354\n",
      "Epoch 443/500, Training Loss: -0.5558\n",
      "Epoch 444/500, Training Loss: -0.5556\n",
      "Epoch 445/500, Training Loss: -0.5328\n",
      "Epoch 446/500, Training Loss: -0.5434\n",
      "Epoch 447/500, Training Loss: -0.5620\n",
      "Epoch 448/500, Training Loss: -0.5808\n",
      "Epoch 449/500, Training Loss: -0.5672\n",
      "Epoch 450/500, Training Loss: -0.5542\n",
      "Epoch 451/500, Training Loss: -0.5474\n",
      "Epoch 452/500, Training Loss: -0.5768\n",
      "Epoch 453/500, Training Loss: -0.5656\n",
      "Epoch 454/500, Training Loss: -0.5472\n",
      "Epoch 455/500, Training Loss: -0.5464\n",
      "Epoch 456/500, Training Loss: -0.5568\n",
      "Epoch 457/500, Training Loss: -0.5548\n",
      "Epoch 458/500, Training Loss: -0.5672\n",
      "Epoch 459/500, Training Loss: -0.5638\n",
      "Epoch 460/500, Training Loss: -0.5753\n",
      "Epoch 461/500, Training Loss: -0.5584\n",
      "Epoch 462/500, Training Loss: -0.5932\n",
      "Epoch 463/500, Training Loss: -0.5885\n",
      "Epoch 464/500, Training Loss: -0.5036\n",
      "Epoch 465/500, Training Loss: -0.5633\n",
      "Epoch 466/500, Training Loss: -0.5819\n",
      "Epoch 467/500, Training Loss: -0.5535\n",
      "Epoch 468/500, Training Loss: -0.5596\n",
      "Epoch 469/500, Training Loss: -0.5845\n",
      "Epoch 470/500, Training Loss: -0.5475\n",
      "Epoch 471/500, Training Loss: -0.5466\n",
      "Epoch 472/500, Training Loss: -0.5656\n",
      "Epoch 473/500, Training Loss: -0.5527\n",
      "Epoch 474/500, Training Loss: -0.5714\n",
      "Epoch 475/500, Training Loss: -0.5649\n",
      "Epoch 476/500, Training Loss: -0.5708\n",
      "Epoch 477/500, Training Loss: -0.5524\n",
      "Epoch 478/500, Training Loss: -0.5557\n",
      "Epoch 479/500, Training Loss: -0.5615\n",
      "Epoch 480/500, Training Loss: -0.5285\n",
      "Epoch 481/500, Training Loss: -0.5353\n",
      "Epoch 482/500, Training Loss: -0.5800\n",
      "Epoch 483/500, Training Loss: -0.5762\n",
      "Epoch 484/500, Training Loss: -0.5700\n",
      "Epoch 485/500, Training Loss: -0.5443\n",
      "Epoch 486/500, Training Loss: -0.5771\n",
      "Epoch 487/500, Training Loss: -0.5341\n",
      "Epoch 488/500, Training Loss: -0.5478\n",
      "Epoch 489/500, Training Loss: -0.5560\n",
      "Epoch 490/500, Training Loss: -0.5319\n",
      "Epoch 491/500, Training Loss: -0.5354\n",
      "Epoch 492/500, Training Loss: -0.5812\n",
      "Epoch 493/500, Training Loss: -0.5358\n",
      "Epoch 494/500, Training Loss: -0.5944\n",
      "Epoch 495/500, Training Loss: -0.5499\n",
      "Epoch 496/500, Training Loss: -0.5483\n",
      "Epoch 497/500, Training Loss: -0.5779\n",
      "Epoch 498/500, Training Loss: -0.5748\n",
      "Epoch 499/500, Training Loss: -0.5569\n",
      "Epoch 500/500, Training Loss: -0.5824\n",
      "Validation Loss: -0.5474\n",
      "Model 0, Fold 0: Validation Loss = -0.5474\n",
      "Epoch 1/500, Training Loss: 0.0559\n",
      "Epoch 2/500, Training Loss: 0.0679\n",
      "Epoch 3/500, Training Loss: 0.0408\n",
      "Epoch 4/500, Training Loss: -0.0298\n",
      "Epoch 5/500, Training Loss: -0.0084\n",
      "Epoch 6/500, Training Loss: 0.0171\n",
      "Epoch 7/500, Training Loss: -0.0320\n",
      "Epoch 8/500, Training Loss: -0.0330\n",
      "Epoch 9/500, Training Loss: -0.0522\n",
      "Epoch 10/500, Training Loss: -0.0797\n",
      "Epoch 11/500, Training Loss: -0.1158\n",
      "Epoch 12/500, Training Loss: -0.0956\n",
      "Epoch 13/500, Training Loss: -0.1378\n",
      "Epoch 14/500, Training Loss: -0.1379\n",
      "Epoch 15/500, Training Loss: -0.1303\n",
      "Epoch 16/500, Training Loss: -0.1619\n",
      "Epoch 17/500, Training Loss: -0.1810\n",
      "Epoch 18/500, Training Loss: -0.1389\n",
      "Epoch 19/500, Training Loss: -0.2589\n",
      "Epoch 20/500, Training Loss: -0.2781\n",
      "Epoch 21/500, Training Loss: -0.2886\n",
      "Epoch 22/500, Training Loss: -0.2725\n",
      "Epoch 23/500, Training Loss: -0.2771\n",
      "Epoch 24/500, Training Loss: -0.3570\n",
      "Epoch 25/500, Training Loss: -0.3618\n",
      "Epoch 26/500, Training Loss: -0.3797\n",
      "Epoch 27/500, Training Loss: -0.4189\n",
      "Epoch 28/500, Training Loss: -0.4278\n",
      "Epoch 29/500, Training Loss: -0.4748\n",
      "Epoch 30/500, Training Loss: -0.4835\n",
      "Epoch 31/500, Training Loss: -0.4671\n",
      "Epoch 32/500, Training Loss: -0.4753\n",
      "Epoch 33/500, Training Loss: -0.5231\n",
      "Epoch 34/500, Training Loss: -0.5265\n",
      "Epoch 35/500, Training Loss: -0.4907\n",
      "Epoch 36/500, Training Loss: -0.5374\n",
      "Epoch 37/500, Training Loss: -0.5170\n",
      "Epoch 38/500, Training Loss: -0.5451\n",
      "Epoch 39/500, Training Loss: -0.5357\n",
      "Epoch 40/500, Training Loss: -0.5197\n",
      "Epoch 41/500, Training Loss: -0.5215\n",
      "Epoch 42/500, Training Loss: -0.5589\n",
      "Epoch 43/500, Training Loss: -0.5546\n",
      "Epoch 44/500, Training Loss: -0.5288\n",
      "Epoch 45/500, Training Loss: -0.5593\n",
      "Epoch 46/500, Training Loss: -0.5559\n",
      "Epoch 47/500, Training Loss: -0.5761\n",
      "Epoch 48/500, Training Loss: -0.5576\n",
      "Epoch 49/500, Training Loss: -0.5395\n",
      "Epoch 50/500, Training Loss: -0.5829\n",
      "Epoch 51/500, Training Loss: -0.5670\n",
      "Epoch 52/500, Training Loss: -0.5285\n",
      "Epoch 53/500, Training Loss: -0.5474\n",
      "Epoch 54/500, Training Loss: -0.5379\n",
      "Epoch 55/500, Training Loss: -0.5127\n",
      "Epoch 56/500, Training Loss: -0.5597\n",
      "Epoch 57/500, Training Loss: -0.4783\n",
      "Epoch 58/500, Training Loss: -0.5837\n",
      "Epoch 59/500, Training Loss: -0.5673\n",
      "Epoch 60/500, Training Loss: -0.5880\n",
      "Epoch 61/500, Training Loss: -0.5646\n",
      "Epoch 62/500, Training Loss: -0.5371\n",
      "Epoch 63/500, Training Loss: -0.5229\n",
      "Epoch 64/500, Training Loss: -0.5449\n",
      "Epoch 65/500, Training Loss: -0.5239\n",
      "Epoch 66/500, Training Loss: -0.5336\n",
      "Epoch 67/500, Training Loss: -0.5438\n",
      "Epoch 68/500, Training Loss: -0.5872\n",
      "Epoch 69/500, Training Loss: -0.5399\n",
      "Epoch 70/500, Training Loss: -0.5618\n",
      "Epoch 71/500, Training Loss: -0.5561\n",
      "Epoch 72/500, Training Loss: -0.5478\n",
      "Epoch 73/500, Training Loss: -0.5210\n",
      "Epoch 74/500, Training Loss: -0.5568\n",
      "Epoch 75/500, Training Loss: -0.5297\n",
      "Epoch 76/500, Training Loss: -0.5545\n",
      "Epoch 77/500, Training Loss: -0.5483\n",
      "Epoch 78/500, Training Loss: -0.5881\n",
      "Epoch 79/500, Training Loss: -0.5629\n",
      "Epoch 80/500, Training Loss: -0.5658\n",
      "Epoch 81/500, Training Loss: -0.5065\n",
      "Epoch 82/500, Training Loss: -0.5649\n",
      "Epoch 83/500, Training Loss: -0.5608\n",
      "Epoch 84/500, Training Loss: -0.5575\n",
      "Epoch 85/500, Training Loss: -0.5790\n",
      "Epoch 86/500, Training Loss: -0.5350\n",
      "Epoch 87/500, Training Loss: -0.5618\n",
      "Epoch 88/500, Training Loss: -0.5384\n",
      "Epoch 89/500, Training Loss: -0.5143\n",
      "Epoch 90/500, Training Loss: -0.5444\n",
      "Epoch 91/500, Training Loss: -0.5455\n",
      "Epoch 92/500, Training Loss: -0.5171\n",
      "Epoch 93/500, Training Loss: -0.5251\n",
      "Epoch 94/500, Training Loss: -0.5402\n",
      "Epoch 95/500, Training Loss: -0.5322\n",
      "Epoch 96/500, Training Loss: -0.5335\n",
      "Epoch 97/500, Training Loss: -0.5310\n",
      "Epoch 98/500, Training Loss: -0.5579\n",
      "Epoch 99/500, Training Loss: -0.5257\n",
      "Epoch 100/500, Training Loss: -0.5512\n",
      "Epoch 101/500, Training Loss: -0.5278\n",
      "Epoch 102/500, Training Loss: -0.5436\n",
      "Epoch 103/500, Training Loss: -0.5493\n",
      "Epoch 104/500, Training Loss: -0.5641\n",
      "Epoch 105/500, Training Loss: -0.5534\n",
      "Epoch 106/500, Training Loss: -0.5602\n",
      "Epoch 107/500, Training Loss: -0.5269\n",
      "Epoch 108/500, Training Loss: -0.5454\n",
      "Epoch 109/500, Training Loss: -0.5519\n",
      "Epoch 110/500, Training Loss: -0.5188\n",
      "Epoch 111/500, Training Loss: -0.5492\n",
      "Epoch 112/500, Training Loss: -0.5388\n",
      "Epoch 113/500, Training Loss: -0.5356\n",
      "Epoch 114/500, Training Loss: -0.5544\n",
      "Epoch 115/500, Training Loss: -0.5211\n",
      "Epoch 116/500, Training Loss: -0.5399\n",
      "Epoch 117/500, Training Loss: -0.5581\n",
      "Epoch 118/500, Training Loss: -0.5018\n",
      "Epoch 119/500, Training Loss: -0.5529\n",
      "Epoch 120/500, Training Loss: -0.5299\n",
      "Epoch 121/500, Training Loss: -0.5677\n",
      "Epoch 122/500, Training Loss: -0.5400\n",
      "Epoch 123/500, Training Loss: -0.5898\n",
      "Epoch 124/500, Training Loss: -0.5492\n",
      "Epoch 125/500, Training Loss: -0.5642\n",
      "Epoch 126/500, Training Loss: -0.5180\n",
      "Epoch 127/500, Training Loss: -0.5562\n",
      "Epoch 128/500, Training Loss: -0.5568\n",
      "Epoch 129/500, Training Loss: -0.5101\n",
      "Epoch 130/500, Training Loss: -0.5634\n",
      "Epoch 131/500, Training Loss: -0.5179\n",
      "Epoch 132/500, Training Loss: -0.5375\n",
      "Epoch 133/500, Training Loss: -0.5223\n",
      "Epoch 134/500, Training Loss: -0.5476\n",
      "Epoch 135/500, Training Loss: -0.5401\n",
      "Epoch 136/500, Training Loss: -0.5448\n",
      "Epoch 137/500, Training Loss: -0.5665\n",
      "Epoch 138/500, Training Loss: -0.5886\n",
      "Epoch 139/500, Training Loss: -0.5518\n",
      "Epoch 140/500, Training Loss: -0.5256\n",
      "Epoch 141/500, Training Loss: -0.5576\n",
      "Epoch 142/500, Training Loss: -0.5021\n",
      "Epoch 143/500, Training Loss: -0.5457\n",
      "Epoch 144/500, Training Loss: -0.5474\n",
      "Epoch 145/500, Training Loss: -0.5353\n",
      "Epoch 146/500, Training Loss: -0.5482\n",
      "Epoch 147/500, Training Loss: -0.5435\n",
      "Epoch 148/500, Training Loss: -0.5403\n",
      "Epoch 149/500, Training Loss: -0.5795\n",
      "Epoch 150/500, Training Loss: -0.5524\n",
      "Epoch 151/500, Training Loss: -0.5746\n",
      "Epoch 152/500, Training Loss: -0.5423\n",
      "Epoch 153/500, Training Loss: -0.5346\n",
      "Epoch 154/500, Training Loss: -0.5288\n",
      "Epoch 155/500, Training Loss: -0.5783\n",
      "Epoch 156/500, Training Loss: -0.5618\n",
      "Epoch 157/500, Training Loss: -0.5652\n",
      "Epoch 158/500, Training Loss: -0.5720\n",
      "Epoch 159/500, Training Loss: -0.5803\n",
      "Epoch 160/500, Training Loss: -0.5187\n",
      "Epoch 161/500, Training Loss: -0.6047\n",
      "Epoch 162/500, Training Loss: -0.5547\n",
      "Epoch 163/500, Training Loss: -0.5228\n",
      "Epoch 164/500, Training Loss: -0.5256\n",
      "Epoch 165/500, Training Loss: -0.5260\n",
      "Epoch 166/500, Training Loss: -0.5313\n",
      "Epoch 167/500, Training Loss: -0.5186\n",
      "Epoch 168/500, Training Loss: -0.5938\n",
      "Epoch 169/500, Training Loss: -0.5259\n",
      "Epoch 170/500, Training Loss: -0.5558\n",
      "Epoch 171/500, Training Loss: -0.5290\n",
      "Epoch 172/500, Training Loss: -0.5331\n",
      "Epoch 173/500, Training Loss: -0.5583\n",
      "Epoch 174/500, Training Loss: -0.5085\n",
      "Epoch 175/500, Training Loss: -0.5701\n",
      "Epoch 176/500, Training Loss: -0.5338\n",
      "Epoch 177/500, Training Loss: -0.5619\n",
      "Epoch 178/500, Training Loss: -0.5489\n",
      "Epoch 179/500, Training Loss: -0.5545\n",
      "Epoch 180/500, Training Loss: -0.5183\n",
      "Epoch 181/500, Training Loss: -0.5542\n",
      "Epoch 182/500, Training Loss: -0.5260\n",
      "Epoch 183/500, Training Loss: -0.5607\n",
      "Epoch 184/500, Training Loss: -0.5686\n",
      "Epoch 185/500, Training Loss: -0.5396\n",
      "Epoch 186/500, Training Loss: -0.5561\n",
      "Epoch 187/500, Training Loss: -0.5350\n",
      "Epoch 188/500, Training Loss: -0.5600\n",
      "Epoch 189/500, Training Loss: -0.5549\n",
      "Epoch 190/500, Training Loss: -0.5424\n",
      "Epoch 191/500, Training Loss: -0.5686\n",
      "Epoch 192/500, Training Loss: -0.5285\n",
      "Epoch 193/500, Training Loss: -0.5670\n",
      "Epoch 194/500, Training Loss: -0.5273\n",
      "Epoch 195/500, Training Loss: -0.5565\n",
      "Epoch 196/500, Training Loss: -0.5388\n",
      "Epoch 197/500, Training Loss: -0.5495\n",
      "Epoch 198/500, Training Loss: -0.5366\n",
      "Epoch 199/500, Training Loss: -0.5337\n",
      "Epoch 200/500, Training Loss: -0.5647\n",
      "Epoch 201/500, Training Loss: -0.5762\n",
      "Epoch 202/500, Training Loss: -0.5658\n",
      "Epoch 203/500, Training Loss: -0.5734\n",
      "Epoch 204/500, Training Loss: -0.5746\n",
      "Epoch 205/500, Training Loss: -0.5821\n",
      "Epoch 206/500, Training Loss: -0.5434\n",
      "Epoch 207/500, Training Loss: -0.5353\n",
      "Epoch 208/500, Training Loss: -0.5552\n",
      "Epoch 209/500, Training Loss: -0.5428\n",
      "Epoch 210/500, Training Loss: -0.5494\n",
      "Epoch 211/500, Training Loss: -0.5537\n",
      "Epoch 212/500, Training Loss: -0.5698\n",
      "Epoch 213/500, Training Loss: -0.5308\n",
      "Epoch 214/500, Training Loss: -0.5578\n",
      "Epoch 215/500, Training Loss: -0.5323\n",
      "Epoch 216/500, Training Loss: -0.5864\n",
      "Epoch 217/500, Training Loss: -0.5327\n",
      "Epoch 218/500, Training Loss: -0.5547\n",
      "Epoch 219/500, Training Loss: -0.5540\n",
      "Epoch 220/500, Training Loss: -0.5267\n",
      "Epoch 221/500, Training Loss: -0.5642\n",
      "Epoch 222/500, Training Loss: -0.5127\n",
      "Epoch 223/500, Training Loss: -0.5394\n",
      "Epoch 224/500, Training Loss: -0.5725\n",
      "Epoch 225/500, Training Loss: -0.5348\n",
      "Epoch 226/500, Training Loss: -0.5655\n",
      "Epoch 227/500, Training Loss: -0.5540\n",
      "Epoch 228/500, Training Loss: -0.5437\n",
      "Epoch 229/500, Training Loss: -0.5607\n",
      "Epoch 230/500, Training Loss: -0.5501\n",
      "Epoch 231/500, Training Loss: -0.5598\n",
      "Epoch 232/500, Training Loss: -0.5295\n",
      "Epoch 233/500, Training Loss: -0.4991\n",
      "Epoch 234/500, Training Loss: -0.5460\n",
      "Epoch 235/500, Training Loss: -0.5496\n",
      "Epoch 236/500, Training Loss: -0.5730\n",
      "Epoch 237/500, Training Loss: -0.5672\n",
      "Epoch 238/500, Training Loss: -0.5409\n",
      "Epoch 239/500, Training Loss: -0.5560\n",
      "Epoch 240/500, Training Loss: -0.5449\n",
      "Epoch 241/500, Training Loss: -0.5761\n",
      "Epoch 242/500, Training Loss: -0.5490\n",
      "Epoch 243/500, Training Loss: -0.5526\n",
      "Epoch 244/500, Training Loss: -0.5445\n",
      "Epoch 245/500, Training Loss: -0.5361\n",
      "Epoch 246/500, Training Loss: -0.5579\n",
      "Epoch 247/500, Training Loss: -0.5689\n",
      "Epoch 248/500, Training Loss: -0.5591\n",
      "Epoch 249/500, Training Loss: -0.5509\n",
      "Epoch 250/500, Training Loss: -0.5131\n",
      "Epoch 251/500, Training Loss: -0.4970\n",
      "Epoch 252/500, Training Loss: -0.5669\n",
      "Epoch 253/500, Training Loss: -0.5801\n",
      "Epoch 254/500, Training Loss: -0.5539\n",
      "Epoch 255/500, Training Loss: -0.5502\n",
      "Epoch 256/500, Training Loss: -0.5658\n",
      "Epoch 257/500, Training Loss: -0.5434\n",
      "Epoch 258/500, Training Loss: -0.5149\n",
      "Epoch 259/500, Training Loss: -0.5224\n",
      "Epoch 260/500, Training Loss: -0.5377\n",
      "Epoch 261/500, Training Loss: -0.5263\n",
      "Epoch 262/500, Training Loss: -0.5276\n",
      "Epoch 263/500, Training Loss: -0.5648\n",
      "Epoch 264/500, Training Loss: -0.5379\n",
      "Epoch 265/500, Training Loss: -0.5467\n",
      "Epoch 266/500, Training Loss: -0.5007\n",
      "Epoch 267/500, Training Loss: -0.5602\n",
      "Epoch 268/500, Training Loss: -0.5432\n",
      "Epoch 269/500, Training Loss: -0.5540\n",
      "Epoch 270/500, Training Loss: -0.5561\n",
      "Epoch 271/500, Training Loss: -0.5706\n",
      "Epoch 272/500, Training Loss: -0.5763\n",
      "Epoch 273/500, Training Loss: -0.5534\n",
      "Epoch 274/500, Training Loss: -0.5795\n",
      "Epoch 275/500, Training Loss: -0.5500\n",
      "Epoch 276/500, Training Loss: -0.5219\n",
      "Epoch 277/500, Training Loss: -0.5552\n",
      "Epoch 278/500, Training Loss: -0.5400\n",
      "Epoch 279/500, Training Loss: -0.5359\n",
      "Epoch 280/500, Training Loss: -0.5376\n",
      "Epoch 281/500, Training Loss: -0.5231\n",
      "Epoch 282/500, Training Loss: -0.5533\n",
      "Epoch 283/500, Training Loss: -0.5543\n",
      "Epoch 284/500, Training Loss: -0.5487\n",
      "Epoch 285/500, Training Loss: -0.5562\n",
      "Epoch 286/500, Training Loss: -0.5335\n",
      "Epoch 287/500, Training Loss: -0.5499\n",
      "Epoch 288/500, Training Loss: -0.5616\n",
      "Epoch 289/500, Training Loss: -0.5558\n",
      "Epoch 290/500, Training Loss: -0.5465\n",
      "Epoch 291/500, Training Loss: -0.5562\n",
      "Epoch 292/500, Training Loss: -0.5420\n",
      "Epoch 293/500, Training Loss: -0.5447\n",
      "Epoch 294/500, Training Loss: -0.5844\n",
      "Epoch 295/500, Training Loss: -0.5404\n",
      "Epoch 296/500, Training Loss: -0.5410\n",
      "Epoch 297/500, Training Loss: -0.5226\n",
      "Epoch 298/500, Training Loss: -0.5692\n",
      "Epoch 299/500, Training Loss: -0.5771\n",
      "Epoch 300/500, Training Loss: -0.5635\n",
      "Epoch 301/500, Training Loss: -0.5709\n",
      "Epoch 302/500, Training Loss: -0.5613\n",
      "Epoch 303/500, Training Loss: -0.5687\n",
      "Epoch 304/500, Training Loss: -0.5194\n",
      "Epoch 305/500, Training Loss: -0.5622\n",
      "Epoch 306/500, Training Loss: -0.5442\n",
      "Epoch 307/500, Training Loss: -0.5710\n",
      "Epoch 308/500, Training Loss: -0.5177\n",
      "Epoch 309/500, Training Loss: -0.5619\n",
      "Epoch 310/500, Training Loss: -0.5609\n",
      "Epoch 311/500, Training Loss: -0.5485\n",
      "Epoch 312/500, Training Loss: -0.5564\n",
      "Epoch 313/500, Training Loss: -0.5502\n",
      "Epoch 314/500, Training Loss: -0.5334\n",
      "Epoch 315/500, Training Loss: -0.5240\n",
      "Epoch 316/500, Training Loss: -0.5180\n",
      "Epoch 317/500, Training Loss: -0.5433\n",
      "Epoch 318/500, Training Loss: -0.5398\n",
      "Epoch 319/500, Training Loss: -0.5555\n",
      "Epoch 320/500, Training Loss: -0.5626\n",
      "Epoch 321/500, Training Loss: -0.5273\n",
      "Epoch 322/500, Training Loss: -0.5843\n",
      "Epoch 323/500, Training Loss: -0.5358\n",
      "Epoch 324/500, Training Loss: -0.5796\n",
      "Epoch 325/500, Training Loss: -0.5011\n",
      "Epoch 326/500, Training Loss: -0.5301\n",
      "Epoch 327/500, Training Loss: -0.5552\n",
      "Epoch 328/500, Training Loss: -0.5838\n",
      "Epoch 329/500, Training Loss: -0.5555\n",
      "Epoch 330/500, Training Loss: -0.5765\n",
      "Epoch 331/500, Training Loss: -0.5639\n",
      "Epoch 332/500, Training Loss: -0.5340\n",
      "Epoch 333/500, Training Loss: -0.5343\n",
      "Epoch 334/500, Training Loss: -0.5584\n",
      "Epoch 335/500, Training Loss: -0.5405\n",
      "Epoch 336/500, Training Loss: -0.5855\n",
      "Epoch 337/500, Training Loss: -0.5359\n",
      "Epoch 338/500, Training Loss: -0.5523\n",
      "Epoch 339/500, Training Loss: -0.5581\n",
      "Epoch 340/500, Training Loss: -0.5533\n",
      "Epoch 341/500, Training Loss: -0.5666\n",
      "Epoch 342/500, Training Loss: -0.5317\n",
      "Epoch 343/500, Training Loss: -0.5609\n",
      "Epoch 344/500, Training Loss: -0.5725\n",
      "Epoch 345/500, Training Loss: -0.5300\n",
      "Epoch 346/500, Training Loss: -0.5493\n",
      "Epoch 347/500, Training Loss: -0.5062\n",
      "Epoch 348/500, Training Loss: -0.5448\n",
      "Epoch 349/500, Training Loss: -0.5097\n",
      "Epoch 350/500, Training Loss: -0.6020\n",
      "Epoch 351/500, Training Loss: -0.5638\n",
      "Epoch 352/500, Training Loss: -0.5613\n",
      "Epoch 353/500, Training Loss: -0.5392\n",
      "Epoch 354/500, Training Loss: -0.5789\n",
      "Epoch 355/500, Training Loss: -0.5683\n",
      "Epoch 356/500, Training Loss: -0.5470\n",
      "Epoch 357/500, Training Loss: -0.5586\n",
      "Epoch 358/500, Training Loss: -0.5120\n",
      "Epoch 359/500, Training Loss: -0.5431\n",
      "Epoch 360/500, Training Loss: -0.5734\n",
      "Epoch 361/500, Training Loss: -0.5163\n",
      "Epoch 362/500, Training Loss: -0.5131\n",
      "Epoch 363/500, Training Loss: -0.5379\n",
      "Epoch 364/500, Training Loss: -0.5385\n",
      "Epoch 365/500, Training Loss: -0.5514\n",
      "Epoch 366/500, Training Loss: -0.5518\n",
      "Epoch 367/500, Training Loss: -0.5320\n",
      "Epoch 368/500, Training Loss: -0.5403\n",
      "Epoch 369/500, Training Loss: -0.5330\n",
      "Epoch 370/500, Training Loss: -0.5819\n",
      "Epoch 371/500, Training Loss: -0.5066\n",
      "Epoch 372/500, Training Loss: -0.5743\n",
      "Epoch 373/500, Training Loss: -0.5334\n",
      "Epoch 374/500, Training Loss: -0.5370\n",
      "Epoch 375/500, Training Loss: -0.5807\n",
      "Epoch 376/500, Training Loss: -0.5690\n",
      "Epoch 377/500, Training Loss: -0.5457\n",
      "Epoch 378/500, Training Loss: -0.5171\n",
      "Epoch 379/500, Training Loss: -0.5487\n",
      "Epoch 380/500, Training Loss: -0.5346\n",
      "Epoch 381/500, Training Loss: -0.5395\n",
      "Epoch 382/500, Training Loss: -0.5479\n",
      "Epoch 383/500, Training Loss: -0.5506\n",
      "Epoch 384/500, Training Loss: -0.4932\n",
      "Epoch 385/500, Training Loss: -0.5330\n",
      "Epoch 386/500, Training Loss: -0.5576\n",
      "Epoch 387/500, Training Loss: -0.5924\n",
      "Epoch 388/500, Training Loss: -0.5353\n",
      "Epoch 389/500, Training Loss: -0.5458\n",
      "Epoch 390/500, Training Loss: -0.5086\n",
      "Epoch 391/500, Training Loss: -0.5510\n",
      "Epoch 392/500, Training Loss: -0.5662\n",
      "Epoch 393/500, Training Loss: -0.5327\n",
      "Epoch 394/500, Training Loss: -0.5695\n",
      "Epoch 395/500, Training Loss: -0.5793\n",
      "Epoch 396/500, Training Loss: -0.5605\n",
      "Epoch 397/500, Training Loss: -0.5619\n",
      "Epoch 398/500, Training Loss: -0.5279\n",
      "Epoch 399/500, Training Loss: -0.5292\n",
      "Epoch 400/500, Training Loss: -0.5625\n",
      "Epoch 401/500, Training Loss: -0.5576\n",
      "Epoch 402/500, Training Loss: -0.5535\n",
      "Epoch 403/500, Training Loss: -0.5295\n",
      "Epoch 404/500, Training Loss: -0.5217\n",
      "Epoch 405/500, Training Loss: -0.5246\n",
      "Epoch 406/500, Training Loss: -0.5385\n",
      "Epoch 407/500, Training Loss: -0.5295\n",
      "Epoch 408/500, Training Loss: -0.5687\n",
      "Epoch 409/500, Training Loss: -0.5759\n",
      "Epoch 410/500, Training Loss: -0.5291\n",
      "Epoch 411/500, Training Loss: -0.5325\n",
      "Epoch 412/500, Training Loss: -0.5525\n",
      "Epoch 413/500, Training Loss: -0.5500\n",
      "Epoch 414/500, Training Loss: -0.5386\n",
      "Epoch 415/500, Training Loss: -0.5763\n",
      "Epoch 416/500, Training Loss: -0.5055\n",
      "Epoch 417/500, Training Loss: -0.5344\n",
      "Epoch 418/500, Training Loss: -0.5241\n",
      "Epoch 419/500, Training Loss: -0.5279\n",
      "Epoch 420/500, Training Loss: -0.5667\n",
      "Epoch 421/500, Training Loss: -0.5439\n",
      "Epoch 422/500, Training Loss: -0.5355\n",
      "Epoch 423/500, Training Loss: -0.5352\n",
      "Epoch 424/500, Training Loss: -0.5689\n",
      "Epoch 425/500, Training Loss: -0.5334\n",
      "Epoch 426/500, Training Loss: -0.5618\n",
      "Epoch 427/500, Training Loss: -0.5342\n",
      "Epoch 428/500, Training Loss: -0.5534\n",
      "Epoch 429/500, Training Loss: -0.5522\n",
      "Epoch 430/500, Training Loss: -0.5254\n",
      "Epoch 431/500, Training Loss: -0.5493\n",
      "Epoch 432/500, Training Loss: -0.5402\n",
      "Epoch 433/500, Training Loss: -0.5144\n",
      "Epoch 434/500, Training Loss: -0.5529\n",
      "Epoch 435/500, Training Loss: -0.5592\n",
      "Epoch 436/500, Training Loss: -0.5667\n",
      "Epoch 437/500, Training Loss: -0.5629\n",
      "Epoch 438/500, Training Loss: -0.5453\n",
      "Epoch 439/500, Training Loss: -0.5754\n",
      "Epoch 440/500, Training Loss: -0.5451\n",
      "Epoch 441/500, Training Loss: -0.5613\n",
      "Epoch 442/500, Training Loss: -0.5550\n",
      "Epoch 443/500, Training Loss: -0.5331\n",
      "Epoch 444/500, Training Loss: -0.5623\n",
      "Epoch 445/500, Training Loss: -0.5771\n",
      "Epoch 446/500, Training Loss: -0.5606\n",
      "Epoch 447/500, Training Loss: -0.5427\n",
      "Epoch 448/500, Training Loss: -0.5597\n",
      "Epoch 449/500, Training Loss: -0.5717\n",
      "Epoch 450/500, Training Loss: -0.5679\n",
      "Epoch 451/500, Training Loss: -0.5153\n",
      "Epoch 452/500, Training Loss: -0.5660\n",
      "Epoch 453/500, Training Loss: -0.5609\n",
      "Epoch 454/500, Training Loss: -0.5101\n",
      "Epoch 455/500, Training Loss: -0.5497\n",
      "Epoch 456/500, Training Loss: -0.5466\n",
      "Epoch 457/500, Training Loss: -0.4996\n",
      "Epoch 458/500, Training Loss: -0.5624\n",
      "Epoch 459/500, Training Loss: -0.5738\n",
      "Epoch 460/500, Training Loss: -0.5766\n",
      "Epoch 461/500, Training Loss: -0.5335\n",
      "Epoch 462/500, Training Loss: -0.5778\n",
      "Epoch 463/500, Training Loss: -0.4890\n",
      "Epoch 464/500, Training Loss: -0.5414\n",
      "Epoch 465/500, Training Loss: -0.5108\n",
      "Epoch 466/500, Training Loss: -0.5282\n",
      "Epoch 467/500, Training Loss: -0.5251\n",
      "Epoch 468/500, Training Loss: -0.5321\n",
      "Epoch 469/500, Training Loss: -0.5483\n",
      "Epoch 470/500, Training Loss: -0.5180\n",
      "Epoch 471/500, Training Loss: -0.5772\n",
      "Epoch 472/500, Training Loss: -0.5451\n",
      "Epoch 473/500, Training Loss: -0.5498\n",
      "Epoch 474/500, Training Loss: -0.5341\n",
      "Epoch 475/500, Training Loss: -0.5270\n",
      "Epoch 476/500, Training Loss: -0.5885\n",
      "Epoch 477/500, Training Loss: -0.5733\n",
      "Epoch 478/500, Training Loss: -0.5097\n",
      "Epoch 479/500, Training Loss: -0.5536\n",
      "Epoch 480/500, Training Loss: -0.5286\n",
      "Epoch 481/500, Training Loss: -0.5685\n",
      "Epoch 482/500, Training Loss: -0.5972\n",
      "Epoch 483/500, Training Loss: -0.5690\n",
      "Epoch 484/500, Training Loss: -0.5690\n",
      "Epoch 485/500, Training Loss: -0.5167\n",
      "Epoch 486/500, Training Loss: -0.5481\n",
      "Epoch 487/500, Training Loss: -0.5650\n",
      "Epoch 488/500, Training Loss: -0.5190\n",
      "Epoch 489/500, Training Loss: -0.5441\n",
      "Epoch 490/500, Training Loss: -0.5504\n",
      "Epoch 491/500, Training Loss: -0.5849\n",
      "Epoch 492/500, Training Loss: -0.5322\n",
      "Epoch 493/500, Training Loss: -0.5610\n",
      "Epoch 494/500, Training Loss: -0.5374\n",
      "Epoch 495/500, Training Loss: -0.5655\n",
      "Epoch 496/500, Training Loss: -0.5772\n",
      "Epoch 497/500, Training Loss: -0.5434\n",
      "Epoch 498/500, Training Loss: -0.5548\n",
      "Epoch 499/500, Training Loss: -0.5578\n",
      "Epoch 500/500, Training Loss: -0.5513\n",
      "Validation Loss: -0.5602\n",
      "Model 0, Fold 1: Validation Loss = -0.5602\n",
      "Epoch 1/500, Training Loss: -0.3557\n",
      "Epoch 2/500, Training Loss: -0.5330\n",
      "Epoch 3/500, Training Loss: -0.5613\n",
      "Epoch 4/500, Training Loss: -0.5192\n",
      "Epoch 5/500, Training Loss: -0.5833\n",
      "Epoch 6/500, Training Loss: -0.5497\n",
      "Epoch 7/500, Training Loss: -0.5571\n",
      "Epoch 8/500, Training Loss: -0.6049\n",
      "Epoch 9/500, Training Loss: -0.5409\n",
      "Epoch 10/500, Training Loss: -0.5629\n",
      "Epoch 11/500, Training Loss: -0.5402\n",
      "Epoch 12/500, Training Loss: -0.5752\n",
      "Epoch 13/500, Training Loss: -0.5369\n",
      "Epoch 14/500, Training Loss: -0.5365\n",
      "Epoch 15/500, Training Loss: -0.5435\n",
      "Epoch 16/500, Training Loss: -0.5438\n",
      "Epoch 17/500, Training Loss: -0.5626\n",
      "Epoch 18/500, Training Loss: -0.5859\n",
      "Epoch 19/500, Training Loss: -0.6221\n",
      "Epoch 20/500, Training Loss: -0.5422\n",
      "Epoch 21/500, Training Loss: -0.5365\n",
      "Epoch 22/500, Training Loss: -0.5257\n",
      "Epoch 23/500, Training Loss: -0.5686\n",
      "Epoch 24/500, Training Loss: -0.5713\n",
      "Epoch 25/500, Training Loss: -0.5996\n",
      "Epoch 26/500, Training Loss: -0.5577\n",
      "Epoch 27/500, Training Loss: -0.5831\n",
      "Epoch 28/500, Training Loss: -0.5706\n",
      "Epoch 29/500, Training Loss: -0.6147\n",
      "Epoch 30/500, Training Loss: -0.6001\n",
      "Epoch 31/500, Training Loss: -0.5896\n",
      "Epoch 32/500, Training Loss: -0.5365\n",
      "Epoch 33/500, Training Loss: -0.5879\n",
      "Epoch 34/500, Training Loss: -0.5928\n",
      "Epoch 35/500, Training Loss: -0.5405\n",
      "Epoch 36/500, Training Loss: -0.5364\n",
      "Epoch 37/500, Training Loss: -0.5438\n",
      "Epoch 38/500, Training Loss: -0.6095\n",
      "Epoch 39/500, Training Loss: -0.5677\n",
      "Epoch 40/500, Training Loss: -0.5752\n",
      "Epoch 41/500, Training Loss: -0.5774\n",
      "Epoch 42/500, Training Loss: -0.5766\n",
      "Epoch 43/500, Training Loss: -0.5482\n",
      "Epoch 44/500, Training Loss: -0.5415\n",
      "Epoch 45/500, Training Loss: -0.5459\n",
      "Epoch 46/500, Training Loss: -0.5636\n",
      "Epoch 47/500, Training Loss: -0.5451\n",
      "Epoch 48/500, Training Loss: -0.5439\n",
      "Epoch 49/500, Training Loss: -0.6097\n",
      "Epoch 50/500, Training Loss: -0.5630\n",
      "Epoch 51/500, Training Loss: -0.5446\n",
      "Epoch 52/500, Training Loss: -0.5571\n",
      "Epoch 53/500, Training Loss: -0.5374\n",
      "Epoch 54/500, Training Loss: -0.5910\n",
      "Epoch 55/500, Training Loss: -0.5692\n",
      "Epoch 56/500, Training Loss: -0.5624\n",
      "Epoch 57/500, Training Loss: -0.5478\n",
      "Epoch 58/500, Training Loss: -0.5346\n",
      "Epoch 59/500, Training Loss: -0.5716\n",
      "Epoch 60/500, Training Loss: -0.5648\n",
      "Epoch 61/500, Training Loss: -0.5606\n",
      "Epoch 62/500, Training Loss: -0.5821\n",
      "Epoch 63/500, Training Loss: -0.5502\n",
      "Epoch 64/500, Training Loss: -0.5472\n",
      "Epoch 65/500, Training Loss: -0.5759\n",
      "Epoch 66/500, Training Loss: -0.5633\n",
      "Epoch 67/500, Training Loss: -0.5639\n",
      "Epoch 68/500, Training Loss: -0.5792\n",
      "Epoch 69/500, Training Loss: -0.5430\n",
      "Epoch 70/500, Training Loss: -0.5547\n",
      "Epoch 71/500, Training Loss: -0.6087\n",
      "Epoch 72/500, Training Loss: -0.5564\n",
      "Epoch 73/500, Training Loss: -0.5926\n",
      "Epoch 74/500, Training Loss: -0.5669\n",
      "Epoch 75/500, Training Loss: -0.5529\n",
      "Epoch 76/500, Training Loss: -0.5574\n",
      "Epoch 77/500, Training Loss: -0.5821\n",
      "Epoch 78/500, Training Loss: -0.5572\n",
      "Epoch 79/500, Training Loss: -0.5721\n",
      "Epoch 80/500, Training Loss: -0.5346\n",
      "Epoch 81/500, Training Loss: -0.5349\n",
      "Epoch 82/500, Training Loss: -0.5455\n",
      "Epoch 83/500, Training Loss: -0.5827\n",
      "Epoch 84/500, Training Loss: -0.5488\n",
      "Epoch 85/500, Training Loss: -0.5596\n",
      "Epoch 86/500, Training Loss: -0.5858\n",
      "Epoch 87/500, Training Loss: -0.5895\n",
      "Epoch 88/500, Training Loss: -0.5701\n",
      "Epoch 89/500, Training Loss: -0.5781\n",
      "Epoch 90/500, Training Loss: -0.5765\n",
      "Epoch 91/500, Training Loss: -0.5588\n",
      "Epoch 92/500, Training Loss: -0.5625\n",
      "Epoch 93/500, Training Loss: -0.5784\n",
      "Epoch 94/500, Training Loss: -0.5700\n",
      "Epoch 95/500, Training Loss: -0.5469\n",
      "Epoch 96/500, Training Loss: -0.5833\n",
      "Epoch 97/500, Training Loss: -0.5871\n",
      "Epoch 98/500, Training Loss: -0.5645\n",
      "Epoch 99/500, Training Loss: -0.6087\n",
      "Epoch 100/500, Training Loss: -0.5665\n",
      "Epoch 101/500, Training Loss: -0.5771\n",
      "Epoch 102/500, Training Loss: -0.5309\n",
      "Epoch 103/500, Training Loss: -0.5217\n",
      "Epoch 104/500, Training Loss: -0.5760\n",
      "Epoch 105/500, Training Loss: -0.5555\n",
      "Epoch 106/500, Training Loss: -0.5357\n",
      "Epoch 107/500, Training Loss: -0.6060\n",
      "Epoch 108/500, Training Loss: -0.5759\n",
      "Epoch 109/500, Training Loss: -0.5816\n",
      "Epoch 110/500, Training Loss: -0.5936\n",
      "Epoch 111/500, Training Loss: -0.5880\n",
      "Epoch 112/500, Training Loss: -0.5718\n",
      "Epoch 113/500, Training Loss: -0.5422\n",
      "Epoch 114/500, Training Loss: -0.6308\n",
      "Epoch 115/500, Training Loss: -0.5833\n",
      "Epoch 116/500, Training Loss: -0.5671\n",
      "Epoch 117/500, Training Loss: -0.5721\n",
      "Epoch 118/500, Training Loss: -0.5792\n",
      "Epoch 119/500, Training Loss: -0.5854\n",
      "Epoch 120/500, Training Loss: -0.5725\n",
      "Epoch 121/500, Training Loss: -0.5593\n",
      "Epoch 122/500, Training Loss: -0.5476\n",
      "Epoch 123/500, Training Loss: -0.5472\n",
      "Epoch 124/500, Training Loss: -0.5648\n",
      "Epoch 125/500, Training Loss: -0.5694\n",
      "Epoch 126/500, Training Loss: -0.5730\n",
      "Epoch 127/500, Training Loss: -0.6064\n",
      "Epoch 128/500, Training Loss: -0.5820\n",
      "Epoch 129/500, Training Loss: -0.5746\n",
      "Epoch 130/500, Training Loss: -0.5774\n",
      "Epoch 131/500, Training Loss: -0.5682\n",
      "Epoch 132/500, Training Loss: -0.5445\n",
      "Epoch 133/500, Training Loss: -0.5897\n",
      "Epoch 134/500, Training Loss: -0.5781\n",
      "Epoch 135/500, Training Loss: -0.6014\n",
      "Epoch 136/500, Training Loss: -0.5702\n",
      "Epoch 137/500, Training Loss: -0.5942\n",
      "Epoch 138/500, Training Loss: -0.5701\n",
      "Epoch 139/500, Training Loss: -0.5354\n",
      "Epoch 140/500, Training Loss: -0.5522\n",
      "Epoch 141/500, Training Loss: -0.5907\n",
      "Epoch 142/500, Training Loss: -0.5819\n",
      "Epoch 143/500, Training Loss: -0.5512\n",
      "Epoch 144/500, Training Loss: -0.5502\n",
      "Epoch 145/500, Training Loss: -0.5755\n",
      "Epoch 146/500, Training Loss: -0.5690\n",
      "Epoch 147/500, Training Loss: -0.5818\n",
      "Epoch 148/500, Training Loss: -0.5895\n",
      "Epoch 149/500, Training Loss: -0.5382\n",
      "Epoch 150/500, Training Loss: -0.5897\n",
      "Epoch 151/500, Training Loss: -0.5732\n",
      "Epoch 152/500, Training Loss: -0.5420\n",
      "Epoch 153/500, Training Loss: -0.5395\n",
      "Epoch 154/500, Training Loss: -0.5852\n",
      "Epoch 155/500, Training Loss: -0.5203\n",
      "Epoch 156/500, Training Loss: -0.5555\n",
      "Epoch 157/500, Training Loss: -0.5942\n",
      "Epoch 158/500, Training Loss: -0.5534\n",
      "Epoch 159/500, Training Loss: -0.5819\n",
      "Epoch 160/500, Training Loss: -0.5796\n",
      "Epoch 161/500, Training Loss: -0.5357\n",
      "Epoch 162/500, Training Loss: -0.5553\n",
      "Epoch 163/500, Training Loss: -0.6171\n",
      "Epoch 164/500, Training Loss: -0.5730\n",
      "Epoch 165/500, Training Loss: -0.6047\n",
      "Epoch 166/500, Training Loss: -0.5740\n",
      "Epoch 167/500, Training Loss: -0.5707\n",
      "Epoch 168/500, Training Loss: -0.5268\n",
      "Epoch 169/500, Training Loss: -0.5373\n",
      "Epoch 170/500, Training Loss: -0.5697\n",
      "Epoch 171/500, Training Loss: -0.5932\n",
      "Epoch 172/500, Training Loss: -0.5903\n",
      "Epoch 173/500, Training Loss: -0.5826\n",
      "Epoch 174/500, Training Loss: -0.5896\n",
      "Epoch 175/500, Training Loss: -0.5793\n",
      "Epoch 176/500, Training Loss: -0.5761\n",
      "Epoch 177/500, Training Loss: -0.5799\n",
      "Epoch 178/500, Training Loss: -0.5869\n",
      "Epoch 179/500, Training Loss: -0.5671\n",
      "Epoch 180/500, Training Loss: -0.5834\n",
      "Epoch 181/500, Training Loss: -0.5435\n",
      "Epoch 182/500, Training Loss: -0.5988\n",
      "Epoch 183/500, Training Loss: -0.5575\n",
      "Epoch 184/500, Training Loss: -0.5814\n",
      "Epoch 185/500, Training Loss: -0.5488\n",
      "Epoch 186/500, Training Loss: -0.5444\n",
      "Epoch 187/500, Training Loss: -0.5544\n",
      "Epoch 188/500, Training Loss: -0.5801\n",
      "Epoch 189/500, Training Loss: -0.5394\n",
      "Epoch 190/500, Training Loss: -0.5488\n",
      "Epoch 191/500, Training Loss: -0.5649\n",
      "Epoch 192/500, Training Loss: -0.5570\n",
      "Epoch 193/500, Training Loss: -0.5929\n",
      "Epoch 194/500, Training Loss: -0.5407\n",
      "Epoch 195/500, Training Loss: -0.5316\n",
      "Epoch 196/500, Training Loss: -0.5270\n",
      "Epoch 197/500, Training Loss: -0.5906\n",
      "Epoch 198/500, Training Loss: -0.6021\n",
      "Epoch 199/500, Training Loss: -0.6045\n",
      "Epoch 200/500, Training Loss: -0.5921\n",
      "Epoch 201/500, Training Loss: -0.6020\n",
      "Epoch 202/500, Training Loss: -0.5709\n",
      "Epoch 203/500, Training Loss: -0.5626\n",
      "Epoch 204/500, Training Loss: -0.5568\n",
      "Epoch 205/500, Training Loss: -0.5844\n",
      "Epoch 206/500, Training Loss: -0.5891\n",
      "Epoch 207/500, Training Loss: -0.5563\n",
      "Epoch 208/500, Training Loss: -0.5813\n",
      "Epoch 209/500, Training Loss: -0.5875\n",
      "Epoch 210/500, Training Loss: -0.6108\n",
      "Epoch 211/500, Training Loss: -0.5593\n",
      "Epoch 212/500, Training Loss: -0.5552\n",
      "Epoch 213/500, Training Loss: -0.5753\n",
      "Epoch 214/500, Training Loss: -0.5600\n",
      "Epoch 215/500, Training Loss: -0.6017\n",
      "Epoch 216/500, Training Loss: -0.5822\n",
      "Epoch 217/500, Training Loss: -0.5506\n",
      "Epoch 218/500, Training Loss: -0.5712\n",
      "Epoch 219/500, Training Loss: -0.5805\n",
      "Epoch 220/500, Training Loss: -0.5823\n",
      "Epoch 221/500, Training Loss: -0.6113\n",
      "Epoch 222/500, Training Loss: -0.5755\n",
      "Epoch 223/500, Training Loss: -0.5409\n",
      "Epoch 224/500, Training Loss: -0.5605\n",
      "Epoch 225/500, Training Loss: -0.5503\n",
      "Epoch 226/500, Training Loss: -0.5426\n",
      "Epoch 227/500, Training Loss: -0.5946\n",
      "Epoch 228/500, Training Loss: -0.5687\n",
      "Epoch 229/500, Training Loss: -0.5475\n",
      "Epoch 230/500, Training Loss: -0.5639\n",
      "Epoch 231/500, Training Loss: -0.5774\n",
      "Epoch 232/500, Training Loss: -0.5691\n",
      "Epoch 233/500, Training Loss: -0.5789\n",
      "Epoch 234/500, Training Loss: -0.5855\n",
      "Epoch 235/500, Training Loss: -0.5301\n",
      "Epoch 236/500, Training Loss: -0.5428\n",
      "Epoch 237/500, Training Loss: -0.5676\n",
      "Epoch 238/500, Training Loss: -0.5885\n",
      "Epoch 239/500, Training Loss: -0.5957\n",
      "Epoch 240/500, Training Loss: -0.5200\n",
      "Epoch 241/500, Training Loss: -0.5572\n",
      "Epoch 242/500, Training Loss: -0.5341\n",
      "Epoch 243/500, Training Loss: -0.5660\n",
      "Epoch 244/500, Training Loss: -0.5506\n",
      "Epoch 245/500, Training Loss: -0.5714\n",
      "Epoch 246/500, Training Loss: -0.5275\n",
      "Epoch 247/500, Training Loss: -0.5975\n",
      "Epoch 248/500, Training Loss: -0.5511\n",
      "Epoch 249/500, Training Loss: -0.5654\n",
      "Epoch 250/500, Training Loss: -0.5800\n",
      "Epoch 251/500, Training Loss: -0.6025\n",
      "Epoch 252/500, Training Loss: -0.5580\n",
      "Epoch 253/500, Training Loss: -0.5546\n",
      "Epoch 254/500, Training Loss: -0.5552\n",
      "Epoch 255/500, Training Loss: -0.5863\n",
      "Epoch 256/500, Training Loss: -0.5606\n",
      "Epoch 257/500, Training Loss: -0.5662\n",
      "Epoch 258/500, Training Loss: -0.5776\n",
      "Epoch 259/500, Training Loss: -0.5468\n",
      "Epoch 260/500, Training Loss: -0.5747\n",
      "Epoch 261/500, Training Loss: -0.5626\n",
      "Epoch 262/500, Training Loss: -0.5837\n",
      "Epoch 263/500, Training Loss: -0.5487\n",
      "Epoch 264/500, Training Loss: -0.6070\n",
      "Epoch 265/500, Training Loss: -0.5741\n",
      "Epoch 266/500, Training Loss: -0.5359\n",
      "Epoch 267/500, Training Loss: -0.5525\n",
      "Epoch 268/500, Training Loss: -0.6363\n",
      "Epoch 269/500, Training Loss: -0.5311\n",
      "Epoch 270/500, Training Loss: -0.5012\n",
      "Epoch 271/500, Training Loss: -0.6003\n",
      "Epoch 272/500, Training Loss: -0.5700\n",
      "Epoch 273/500, Training Loss: -0.5780\n",
      "Epoch 274/500, Training Loss: -0.5843\n",
      "Epoch 275/500, Training Loss: -0.5779\n",
      "Epoch 276/500, Training Loss: -0.5728\n",
      "Epoch 277/500, Training Loss: -0.5913\n",
      "Epoch 278/500, Training Loss: -0.5607\n",
      "Epoch 279/500, Training Loss: -0.5718\n",
      "Epoch 280/500, Training Loss: -0.5747\n",
      "Epoch 281/500, Training Loss: -0.5528\n",
      "Epoch 282/500, Training Loss: -0.5696\n",
      "Epoch 283/500, Training Loss: -0.5798\n",
      "Epoch 284/500, Training Loss: -0.5666\n",
      "Epoch 285/500, Training Loss: -0.5705\n",
      "Epoch 286/500, Training Loss: -0.6184\n",
      "Epoch 287/500, Training Loss: -0.5711\n",
      "Epoch 288/500, Training Loss: -0.5331\n",
      "Epoch 289/500, Training Loss: -0.5392\n",
      "Epoch 290/500, Training Loss: -0.5689\n",
      "Epoch 291/500, Training Loss: -0.5843\n",
      "Epoch 292/500, Training Loss: -0.5366\n",
      "Epoch 293/500, Training Loss: -0.5796\n",
      "Epoch 294/500, Training Loss: -0.5517\n",
      "Epoch 295/500, Training Loss: -0.5928\n",
      "Epoch 296/500, Training Loss: -0.5387\n",
      "Epoch 297/500, Training Loss: -0.5797\n",
      "Epoch 298/500, Training Loss: -0.5364\n",
      "Epoch 299/500, Training Loss: -0.5869\n",
      "Epoch 300/500, Training Loss: -0.5704\n",
      "Epoch 301/500, Training Loss: -0.5931\n",
      "Epoch 302/500, Training Loss: -0.5951\n",
      "Epoch 303/500, Training Loss: -0.5594\n",
      "Epoch 304/500, Training Loss: -0.5577\n",
      "Epoch 305/500, Training Loss: -0.5619\n",
      "Epoch 306/500, Training Loss: -0.5444\n",
      "Epoch 307/500, Training Loss: -0.5604\n",
      "Epoch 308/500, Training Loss: -0.5767\n",
      "Epoch 309/500, Training Loss: -0.5699\n",
      "Epoch 310/500, Training Loss: -0.6030\n",
      "Epoch 311/500, Training Loss: -0.5639\n",
      "Epoch 312/500, Training Loss: -0.5792\n",
      "Epoch 313/500, Training Loss: -0.5921\n",
      "Epoch 314/500, Training Loss: -0.5589\n",
      "Epoch 315/500, Training Loss: -0.5617\n",
      "Epoch 316/500, Training Loss: -0.5661\n",
      "Epoch 317/500, Training Loss: -0.5914\n",
      "Epoch 318/500, Training Loss: -0.5689\n",
      "Epoch 319/500, Training Loss: -0.5401\n",
      "Epoch 320/500, Training Loss: -0.5618\n",
      "Epoch 321/500, Training Loss: -0.5451\n",
      "Epoch 322/500, Training Loss: -0.5756\n",
      "Epoch 323/500, Training Loss: -0.5696\n",
      "Epoch 324/500, Training Loss: -0.5995\n",
      "Epoch 325/500, Training Loss: -0.5619\n",
      "Epoch 326/500, Training Loss: -0.6030\n",
      "Epoch 327/500, Training Loss: -0.5998\n",
      "Epoch 328/500, Training Loss: -0.5636\n",
      "Epoch 329/500, Training Loss: -0.5689\n",
      "Epoch 330/500, Training Loss: -0.5229\n",
      "Epoch 331/500, Training Loss: -0.5231\n",
      "Epoch 332/500, Training Loss: -0.5303\n",
      "Epoch 333/500, Training Loss: -0.5937\n",
      "Epoch 334/500, Training Loss: -0.5608\n",
      "Epoch 335/500, Training Loss: -0.5967\n",
      "Epoch 336/500, Training Loss: -0.5513\n",
      "Epoch 337/500, Training Loss: -0.5868\n",
      "Epoch 338/500, Training Loss: -0.5922\n",
      "Epoch 339/500, Training Loss: -0.5621\n",
      "Epoch 340/500, Training Loss: -0.6146\n",
      "Epoch 341/500, Training Loss: -0.5690\n",
      "Epoch 342/500, Training Loss: -0.5759\n",
      "Epoch 343/500, Training Loss: -0.5486\n",
      "Epoch 344/500, Training Loss: -0.5109\n",
      "Epoch 345/500, Training Loss: -0.5727\n",
      "Epoch 346/500, Training Loss: -0.5527\n",
      "Epoch 347/500, Training Loss: -0.5663\n",
      "Epoch 348/500, Training Loss: -0.5923\n",
      "Epoch 349/500, Training Loss: -0.5630\n",
      "Epoch 350/500, Training Loss: -0.5802\n",
      "Epoch 351/500, Training Loss: -0.5512\n",
      "Epoch 352/500, Training Loss: -0.5916\n",
      "Epoch 353/500, Training Loss: -0.6178\n",
      "Epoch 354/500, Training Loss: -0.5567\n",
      "Epoch 355/500, Training Loss: -0.5579\n",
      "Epoch 356/500, Training Loss: -0.5908\n",
      "Epoch 357/500, Training Loss: -0.5958\n",
      "Epoch 358/500, Training Loss: -0.5733\n",
      "Epoch 359/500, Training Loss: -0.5567\n",
      "Epoch 360/500, Training Loss: -0.5859\n",
      "Epoch 361/500, Training Loss: -0.5689\n",
      "Epoch 362/500, Training Loss: -0.5493\n",
      "Epoch 363/500, Training Loss: -0.5468\n",
      "Epoch 364/500, Training Loss: -0.5789\n",
      "Epoch 365/500, Training Loss: -0.6122\n",
      "Epoch 366/500, Training Loss: -0.6081\n",
      "Epoch 367/500, Training Loss: -0.5579\n",
      "Epoch 368/500, Training Loss: -0.5499\n",
      "Epoch 369/500, Training Loss: -0.5759\n",
      "Epoch 370/500, Training Loss: -0.5645\n",
      "Epoch 371/500, Training Loss: -0.5796\n",
      "Epoch 372/500, Training Loss: -0.5603\n",
      "Epoch 373/500, Training Loss: -0.5818\n",
      "Epoch 374/500, Training Loss: -0.5778\n",
      "Epoch 375/500, Training Loss: -0.6140\n",
      "Epoch 376/500, Training Loss: -0.5472\n",
      "Epoch 377/500, Training Loss: -0.5785\n",
      "Epoch 378/500, Training Loss: -0.5881\n",
      "Epoch 379/500, Training Loss: -0.5716\n",
      "Epoch 380/500, Training Loss: -0.5452\n",
      "Epoch 381/500, Training Loss: -0.5579\n",
      "Epoch 382/500, Training Loss: -0.6023\n",
      "Epoch 383/500, Training Loss: -0.5894\n",
      "Epoch 384/500, Training Loss: -0.5643\n",
      "Epoch 385/500, Training Loss: -0.5758\n",
      "Epoch 386/500, Training Loss: -0.5831\n",
      "Epoch 387/500, Training Loss: -0.5681\n",
      "Epoch 388/500, Training Loss: -0.5387\n",
      "Epoch 389/500, Training Loss: -0.5747\n",
      "Epoch 390/500, Training Loss: -0.5389\n",
      "Epoch 391/500, Training Loss: -0.5759\n",
      "Epoch 392/500, Training Loss: -0.5471\n",
      "Epoch 393/500, Training Loss: -0.5630\n",
      "Epoch 394/500, Training Loss: -0.5977\n",
      "Epoch 395/500, Training Loss: -0.6198\n",
      "Epoch 396/500, Training Loss: -0.5866\n",
      "Epoch 397/500, Training Loss: -0.5509\n",
      "Epoch 398/500, Training Loss: -0.5550\n",
      "Epoch 399/500, Training Loss: -0.5551\n",
      "Epoch 400/500, Training Loss: -0.5525\n",
      "Epoch 401/500, Training Loss: -0.5760\n",
      "Epoch 402/500, Training Loss: -0.5776\n",
      "Epoch 403/500, Training Loss: -0.5765\n",
      "Epoch 404/500, Training Loss: -0.5445\n",
      "Epoch 405/500, Training Loss: -0.5811\n",
      "Epoch 406/500, Training Loss: -0.5671\n",
      "Epoch 407/500, Training Loss: -0.5812\n",
      "Epoch 408/500, Training Loss: -0.5757\n",
      "Epoch 409/500, Training Loss: -0.5728\n",
      "Epoch 410/500, Training Loss: -0.5713\n",
      "Epoch 411/500, Training Loss: -0.5173\n",
      "Epoch 412/500, Training Loss: -0.5809\n",
      "Epoch 413/500, Training Loss: -0.5456\n",
      "Epoch 414/500, Training Loss: -0.5515\n",
      "Epoch 415/500, Training Loss: -0.5407\n",
      "Epoch 416/500, Training Loss: -0.5414\n",
      "Epoch 417/500, Training Loss: -0.6293\n",
      "Epoch 418/500, Training Loss: -0.5627\n",
      "Epoch 419/500, Training Loss: -0.5427\n",
      "Epoch 420/500, Training Loss: -0.6117\n",
      "Epoch 421/500, Training Loss: -0.5882\n",
      "Epoch 422/500, Training Loss: -0.5575\n",
      "Epoch 423/500, Training Loss: -0.5702\n",
      "Epoch 424/500, Training Loss: -0.5955\n",
      "Epoch 425/500, Training Loss: -0.5776\n",
      "Epoch 426/500, Training Loss: -0.5471\n",
      "Epoch 427/500, Training Loss: -0.5746\n",
      "Epoch 428/500, Training Loss: -0.5514\n",
      "Epoch 429/500, Training Loss: -0.5867\n",
      "Epoch 430/500, Training Loss: -0.5620\n",
      "Epoch 431/500, Training Loss: -0.5630\n",
      "Epoch 432/500, Training Loss: -0.5377\n",
      "Epoch 433/500, Training Loss: -0.5780\n",
      "Epoch 434/500, Training Loss: -0.5565\n",
      "Epoch 435/500, Training Loss: -0.5618\n",
      "Epoch 436/500, Training Loss: -0.5651\n",
      "Epoch 437/500, Training Loss: -0.5508\n",
      "Epoch 438/500, Training Loss: -0.5449\n",
      "Epoch 439/500, Training Loss: -0.5892\n",
      "Epoch 440/500, Training Loss: -0.5697\n",
      "Epoch 441/500, Training Loss: -0.5523\n",
      "Epoch 442/500, Training Loss: -0.6060\n",
      "Epoch 443/500, Training Loss: -0.5630\n",
      "Epoch 444/500, Training Loss: -0.6080\n",
      "Epoch 445/500, Training Loss: -0.5942\n",
      "Epoch 446/500, Training Loss: -0.5297\n",
      "Epoch 447/500, Training Loss: -0.5459\n",
      "Epoch 448/500, Training Loss: -0.5691\n",
      "Epoch 449/500, Training Loss: -0.5575\n",
      "Epoch 450/500, Training Loss: -0.5388\n",
      "Epoch 451/500, Training Loss: -0.5694\n",
      "Epoch 452/500, Training Loss: -0.5698\n",
      "Epoch 453/500, Training Loss: -0.5497\n",
      "Epoch 454/500, Training Loss: -0.5333\n",
      "Epoch 455/500, Training Loss: -0.5552\n",
      "Epoch 456/500, Training Loss: -0.5738\n",
      "Epoch 457/500, Training Loss: -0.5805\n",
      "Epoch 458/500, Training Loss: -0.5543\n",
      "Epoch 459/500, Training Loss: -0.6124\n",
      "Epoch 460/500, Training Loss: -0.5616\n",
      "Epoch 461/500, Training Loss: -0.5523\n",
      "Epoch 462/500, Training Loss: -0.6144\n",
      "Epoch 463/500, Training Loss: -0.5439\n",
      "Epoch 464/500, Training Loss: -0.5572\n",
      "Epoch 465/500, Training Loss: -0.5728\n",
      "Epoch 466/500, Training Loss: -0.5304\n",
      "Epoch 467/500, Training Loss: -0.5808\n",
      "Epoch 468/500, Training Loss: -0.5717\n",
      "Epoch 469/500, Training Loss: -0.5824\n",
      "Epoch 470/500, Training Loss: -0.5734\n",
      "Epoch 471/500, Training Loss: -0.5886\n",
      "Epoch 472/500, Training Loss: -0.5730\n",
      "Epoch 473/500, Training Loss: -0.5335\n",
      "Epoch 474/500, Training Loss: -0.5850\n",
      "Epoch 475/500, Training Loss: -0.5965\n",
      "Epoch 476/500, Training Loss: -0.6085\n",
      "Epoch 477/500, Training Loss: -0.5655\n",
      "Epoch 478/500, Training Loss: -0.5864\n",
      "Epoch 479/500, Training Loss: -0.5651\n",
      "Epoch 480/500, Training Loss: -0.5839\n",
      "Epoch 481/500, Training Loss: -0.6159\n",
      "Epoch 482/500, Training Loss: -0.5939\n",
      "Epoch 483/500, Training Loss: -0.6286\n",
      "Epoch 484/500, Training Loss: -0.5483\n",
      "Epoch 485/500, Training Loss: -0.5952\n",
      "Epoch 486/500, Training Loss: -0.5515\n",
      "Epoch 487/500, Training Loss: -0.6025\n",
      "Epoch 488/500, Training Loss: -0.5554\n",
      "Epoch 489/500, Training Loss: -0.5443\n",
      "Epoch 490/500, Training Loss: -0.5277\n",
      "Epoch 491/500, Training Loss: -0.5445\n",
      "Epoch 492/500, Training Loss: -0.5627\n",
      "Epoch 493/500, Training Loss: -0.5448\n",
      "Epoch 494/500, Training Loss: -0.5977\n",
      "Epoch 495/500, Training Loss: -0.5302\n",
      "Epoch 496/500, Training Loss: -0.5715\n",
      "Epoch 497/500, Training Loss: -0.5545\n",
      "Epoch 498/500, Training Loss: -0.5947\n",
      "Epoch 499/500, Training Loss: -0.5968\n",
      "Epoch 500/500, Training Loss: -0.5621\n",
      "Validation Loss: -0.5375\n",
      "Model 1, Fold 0: Validation Loss = -0.5375\n",
      "Epoch 1/500, Training Loss: -0.3986\n",
      "Epoch 2/500, Training Loss: -0.4826\n",
      "Epoch 3/500, Training Loss: -0.5300\n",
      "Epoch 4/500, Training Loss: -0.5218\n",
      "Epoch 5/500, Training Loss: -0.5338\n",
      "Epoch 6/500, Training Loss: -0.5164\n",
      "Epoch 7/500, Training Loss: -0.5207\n",
      "Epoch 8/500, Training Loss: -0.5373\n",
      "Epoch 9/500, Training Loss: -0.5284\n",
      "Epoch 10/500, Training Loss: -0.5453\n",
      "Epoch 11/500, Training Loss: -0.5791\n",
      "Epoch 12/500, Training Loss: -0.5079\n",
      "Epoch 13/500, Training Loss: -0.5340\n",
      "Epoch 14/500, Training Loss: -0.5604\n",
      "Epoch 15/500, Training Loss: -0.5351\n",
      "Epoch 16/500, Training Loss: -0.5339\n",
      "Epoch 17/500, Training Loss: -0.5776\n",
      "Epoch 18/500, Training Loss: -0.5483\n",
      "Epoch 19/500, Training Loss: -0.5283\n",
      "Epoch 20/500, Training Loss: -0.5398\n",
      "Epoch 21/500, Training Loss: -0.5434\n",
      "Epoch 22/500, Training Loss: -0.5663\n",
      "Epoch 23/500, Training Loss: -0.5619\n",
      "Epoch 24/500, Training Loss: -0.5437\n",
      "Epoch 25/500, Training Loss: -0.5359\n",
      "Epoch 26/500, Training Loss: -0.5505\n",
      "Epoch 27/500, Training Loss: -0.4827\n",
      "Epoch 28/500, Training Loss: -0.5949\n",
      "Epoch 29/500, Training Loss: -0.5128\n",
      "Epoch 30/500, Training Loss: -0.5370\n",
      "Epoch 31/500, Training Loss: -0.5623\n",
      "Epoch 32/500, Training Loss: -0.4930\n",
      "Epoch 33/500, Training Loss: -0.5841\n",
      "Epoch 34/500, Training Loss: -0.5242\n",
      "Epoch 35/500, Training Loss: -0.5535\n",
      "Epoch 36/500, Training Loss: -0.5799\n",
      "Epoch 37/500, Training Loss: -0.5669\n",
      "Epoch 38/500, Training Loss: -0.5216\n",
      "Epoch 39/500, Training Loss: -0.5393\n",
      "Epoch 40/500, Training Loss: -0.5630\n",
      "Epoch 41/500, Training Loss: -0.5661\n",
      "Epoch 42/500, Training Loss: -0.5157\n",
      "Epoch 43/500, Training Loss: -0.5244\n",
      "Epoch 44/500, Training Loss: -0.5221\n",
      "Epoch 45/500, Training Loss: -0.5536\n",
      "Epoch 46/500, Training Loss: -0.5364\n",
      "Epoch 47/500, Training Loss: -0.5584\n",
      "Epoch 48/500, Training Loss: -0.5379\n",
      "Epoch 49/500, Training Loss: -0.5611\n",
      "Epoch 50/500, Training Loss: -0.5982\n",
      "Epoch 51/500, Training Loss: -0.5751\n",
      "Epoch 52/500, Training Loss: -0.5598\n",
      "Epoch 53/500, Training Loss: -0.5860\n",
      "Epoch 54/500, Training Loss: -0.5569\n",
      "Epoch 55/500, Training Loss: -0.5307\n",
      "Epoch 56/500, Training Loss: -0.5504\n",
      "Epoch 57/500, Training Loss: -0.5466\n",
      "Epoch 58/500, Training Loss: -0.5446\n",
      "Epoch 59/500, Training Loss: -0.5351\n",
      "Epoch 60/500, Training Loss: -0.5810\n",
      "Epoch 61/500, Training Loss: -0.5500\n",
      "Epoch 62/500, Training Loss: -0.5522\n",
      "Epoch 63/500, Training Loss: -0.5593\n",
      "Epoch 64/500, Training Loss: -0.5365\n",
      "Epoch 65/500, Training Loss: -0.5625\n",
      "Epoch 66/500, Training Loss: -0.5622\n",
      "Epoch 67/500, Training Loss: -0.5250\n",
      "Epoch 68/500, Training Loss: -0.5571\n",
      "Epoch 69/500, Training Loss: -0.5598\n",
      "Epoch 70/500, Training Loss: -0.5687\n",
      "Epoch 71/500, Training Loss: -0.5142\n",
      "Epoch 72/500, Training Loss: -0.5938\n",
      "Epoch 73/500, Training Loss: -0.5349\n",
      "Epoch 74/500, Training Loss: -0.5380\n",
      "Epoch 75/500, Training Loss: -0.5197\n",
      "Epoch 76/500, Training Loss: -0.5819\n",
      "Epoch 77/500, Training Loss: -0.5357\n",
      "Epoch 78/500, Training Loss: -0.5661\n",
      "Epoch 79/500, Training Loss: -0.5013\n",
      "Epoch 80/500, Training Loss: -0.5711\n",
      "Epoch 81/500, Training Loss: -0.5716\n",
      "Epoch 82/500, Training Loss: -0.5353\n",
      "Epoch 83/500, Training Loss: -0.5340\n",
      "Epoch 84/500, Training Loss: -0.5198\n",
      "Epoch 85/500, Training Loss: -0.5306\n",
      "Epoch 86/500, Training Loss: -0.5369\n",
      "Epoch 87/500, Training Loss: -0.5283\n",
      "Epoch 88/500, Training Loss: -0.5506\n",
      "Epoch 89/500, Training Loss: -0.5731\n",
      "Epoch 90/500, Training Loss: -0.5561\n",
      "Epoch 91/500, Training Loss: -0.5589\n",
      "Epoch 92/500, Training Loss: -0.5406\n",
      "Epoch 93/500, Training Loss: -0.5439\n",
      "Epoch 94/500, Training Loss: -0.5850\n",
      "Epoch 95/500, Training Loss: -0.5371\n",
      "Epoch 96/500, Training Loss: -0.5304\n",
      "Epoch 97/500, Training Loss: -0.5259\n",
      "Epoch 98/500, Training Loss: -0.5384\n",
      "Epoch 99/500, Training Loss: -0.5293\n",
      "Epoch 100/500, Training Loss: -0.5877\n",
      "Epoch 101/500, Training Loss: -0.5561\n",
      "Epoch 102/500, Training Loss: -0.5350\n",
      "Epoch 103/500, Training Loss: -0.5581\n",
      "Epoch 104/500, Training Loss: -0.5209\n",
      "Epoch 105/500, Training Loss: -0.5503\n",
      "Epoch 106/500, Training Loss: -0.5267\n",
      "Epoch 107/500, Training Loss: -0.5593\n",
      "Epoch 108/500, Training Loss: -0.5235\n",
      "Epoch 109/500, Training Loss: -0.5222\n",
      "Epoch 110/500, Training Loss: -0.5491\n",
      "Epoch 111/500, Training Loss: -0.5698\n",
      "Epoch 112/500, Training Loss: -0.5491\n",
      "Epoch 113/500, Training Loss: -0.5590\n",
      "Epoch 114/500, Training Loss: -0.5249\n",
      "Epoch 115/500, Training Loss: -0.5689\n",
      "Epoch 116/500, Training Loss: -0.5393\n",
      "Epoch 117/500, Training Loss: -0.5306\n",
      "Epoch 118/500, Training Loss: -0.5161\n",
      "Epoch 119/500, Training Loss: -0.5366\n",
      "Epoch 120/500, Training Loss: -0.5687\n",
      "Epoch 121/500, Training Loss: -0.5694\n",
      "Epoch 122/500, Training Loss: -0.5660\n",
      "Epoch 123/500, Training Loss: -0.5048\n",
      "Epoch 124/500, Training Loss: -0.5461\n",
      "Epoch 125/500, Training Loss: -0.5592\n",
      "Epoch 126/500, Training Loss: -0.5758\n",
      "Epoch 127/500, Training Loss: -0.5316\n",
      "Epoch 128/500, Training Loss: -0.5350\n",
      "Epoch 129/500, Training Loss: -0.5321\n",
      "Epoch 130/500, Training Loss: -0.5619\n",
      "Epoch 131/500, Training Loss: -0.5711\n",
      "Epoch 132/500, Training Loss: -0.5427\n",
      "Epoch 133/500, Training Loss: -0.5454\n",
      "Epoch 134/500, Training Loss: -0.5779\n",
      "Epoch 135/500, Training Loss: -0.5653\n",
      "Epoch 136/500, Training Loss: -0.5413\n",
      "Epoch 137/500, Training Loss: -0.5060\n",
      "Epoch 138/500, Training Loss: -0.5317\n",
      "Epoch 139/500, Training Loss: -0.5710\n",
      "Epoch 140/500, Training Loss: -0.5460\n",
      "Epoch 141/500, Training Loss: -0.5440\n",
      "Epoch 142/500, Training Loss: -0.5891\n",
      "Epoch 143/500, Training Loss: -0.5952\n",
      "Epoch 144/500, Training Loss: -0.5244\n",
      "Epoch 145/500, Training Loss: -0.5484\n",
      "Epoch 146/500, Training Loss: -0.5136\n",
      "Epoch 147/500, Training Loss: -0.5743\n",
      "Epoch 148/500, Training Loss: -0.5703\n",
      "Epoch 149/500, Training Loss: -0.5564\n",
      "Epoch 150/500, Training Loss: -0.5297\n",
      "Epoch 151/500, Training Loss: -0.5638\n",
      "Epoch 152/500, Training Loss: -0.5733\n",
      "Epoch 153/500, Training Loss: -0.5511\n",
      "Epoch 154/500, Training Loss: -0.5252\n",
      "Epoch 155/500, Training Loss: -0.4843\n",
      "Epoch 156/500, Training Loss: -0.5199\n",
      "Epoch 157/500, Training Loss: -0.5449\n",
      "Epoch 158/500, Training Loss: -0.5605\n",
      "Epoch 159/500, Training Loss: -0.5625\n",
      "Epoch 160/500, Training Loss: -0.5529\n",
      "Epoch 161/500, Training Loss: -0.5768\n",
      "Epoch 162/500, Training Loss: -0.5496\n",
      "Epoch 163/500, Training Loss: -0.5411\n",
      "Epoch 164/500, Training Loss: -0.5178\n",
      "Epoch 165/500, Training Loss: -0.5446\n",
      "Epoch 166/500, Training Loss: -0.5403\n",
      "Epoch 167/500, Training Loss: -0.5739\n",
      "Epoch 168/500, Training Loss: -0.5291\n",
      "Epoch 169/500, Training Loss: -0.5207\n",
      "Epoch 170/500, Training Loss: -0.5509\n",
      "Epoch 171/500, Training Loss: -0.5579\n",
      "Epoch 172/500, Training Loss: -0.5474\n",
      "Epoch 173/500, Training Loss: -0.5433\n",
      "Epoch 174/500, Training Loss: -0.5528\n",
      "Epoch 175/500, Training Loss: -0.5464\n",
      "Epoch 176/500, Training Loss: -0.5861\n",
      "Epoch 177/500, Training Loss: -0.5663\n",
      "Epoch 178/500, Training Loss: -0.5374\n",
      "Epoch 179/500, Training Loss: -0.5820\n",
      "Epoch 180/500, Training Loss: -0.5676\n",
      "Epoch 181/500, Training Loss: -0.5390\n",
      "Epoch 182/500, Training Loss: -0.5485\n",
      "Epoch 183/500, Training Loss: -0.5521\n",
      "Epoch 184/500, Training Loss: -0.5043\n",
      "Epoch 185/500, Training Loss: -0.5420\n",
      "Epoch 186/500, Training Loss: -0.5563\n",
      "Epoch 187/500, Training Loss: -0.5627\n",
      "Epoch 188/500, Training Loss: -0.5472\n",
      "Epoch 189/500, Training Loss: -0.5407\n",
      "Epoch 190/500, Training Loss: -0.5690\n",
      "Epoch 191/500, Training Loss: -0.5244\n",
      "Epoch 192/500, Training Loss: -0.5906\n",
      "Epoch 193/500, Training Loss: -0.5481\n",
      "Epoch 194/500, Training Loss: -0.5346\n",
      "Epoch 195/500, Training Loss: -0.5334\n",
      "Epoch 196/500, Training Loss: -0.5829\n",
      "Epoch 197/500, Training Loss: -0.5648\n",
      "Epoch 198/500, Training Loss: -0.5333\n",
      "Epoch 199/500, Training Loss: -0.5841\n",
      "Epoch 200/500, Training Loss: -0.5275\n",
      "Epoch 201/500, Training Loss: -0.5764\n",
      "Epoch 202/500, Training Loss: -0.5608\n",
      "Epoch 203/500, Training Loss: -0.5507\n",
      "Epoch 204/500, Training Loss: -0.5767\n",
      "Epoch 205/500, Training Loss: -0.5627\n",
      "Epoch 206/500, Training Loss: -0.5520\n",
      "Epoch 207/500, Training Loss: -0.5071\n",
      "Epoch 208/500, Training Loss: -0.5350\n",
      "Epoch 209/500, Training Loss: -0.5456\n",
      "Epoch 210/500, Training Loss: -0.5449\n",
      "Epoch 211/500, Training Loss: -0.5678\n",
      "Epoch 212/500, Training Loss: -0.5421\n",
      "Epoch 213/500, Training Loss: -0.5508\n",
      "Epoch 214/500, Training Loss: -0.5567\n",
      "Epoch 215/500, Training Loss: -0.5141\n",
      "Epoch 216/500, Training Loss: -0.5314\n",
      "Epoch 217/500, Training Loss: -0.5853\n",
      "Epoch 218/500, Training Loss: -0.5217\n",
      "Epoch 219/500, Training Loss: -0.5704\n",
      "Epoch 220/500, Training Loss: -0.5189\n",
      "Epoch 221/500, Training Loss: -0.5462\n",
      "Epoch 222/500, Training Loss: -0.4902\n",
      "Epoch 223/500, Training Loss: -0.5503\n",
      "Epoch 224/500, Training Loss: -0.5316\n",
      "Epoch 225/500, Training Loss: -0.5493\n",
      "Epoch 226/500, Training Loss: -0.5813\n",
      "Epoch 227/500, Training Loss: -0.5051\n",
      "Epoch 228/500, Training Loss: -0.5386\n",
      "Epoch 229/500, Training Loss: -0.5679\n",
      "Epoch 230/500, Training Loss: -0.5605\n",
      "Epoch 231/500, Training Loss: -0.5353\n",
      "Epoch 232/500, Training Loss: -0.5930\n",
      "Epoch 233/500, Training Loss: -0.4986\n",
      "Epoch 234/500, Training Loss: -0.5519\n",
      "Epoch 235/500, Training Loss: -0.5522\n",
      "Epoch 236/500, Training Loss: -0.5369\n",
      "Epoch 237/500, Training Loss: -0.5471\n",
      "Epoch 238/500, Training Loss: -0.5737\n",
      "Epoch 239/500, Training Loss: -0.5393\n",
      "Epoch 240/500, Training Loss: -0.5364\n",
      "Epoch 241/500, Training Loss: -0.5405\n",
      "Epoch 242/500, Training Loss: -0.5456\n",
      "Epoch 243/500, Training Loss: -0.5549\n",
      "Epoch 244/500, Training Loss: -0.5475\n",
      "Epoch 245/500, Training Loss: -0.5768\n",
      "Epoch 246/500, Training Loss: -0.5605\n",
      "Epoch 247/500, Training Loss: -0.5650\n",
      "Epoch 248/500, Training Loss: -0.5328\n",
      "Epoch 249/500, Training Loss: -0.5656\n",
      "Epoch 250/500, Training Loss: -0.5447\n",
      "Epoch 251/500, Training Loss: -0.5940\n",
      "Epoch 252/500, Training Loss: -0.5745\n",
      "Epoch 253/500, Training Loss: -0.5613\n",
      "Epoch 254/500, Training Loss: -0.5155\n",
      "Epoch 255/500, Training Loss: -0.5850\n",
      "Epoch 256/500, Training Loss: -0.5278\n",
      "Epoch 257/500, Training Loss: -0.5401\n",
      "Epoch 258/500, Training Loss: -0.5444\n",
      "Epoch 259/500, Training Loss: -0.5544\n",
      "Epoch 260/500, Training Loss: -0.5326\n",
      "Epoch 261/500, Training Loss: -0.5407\n",
      "Epoch 262/500, Training Loss: -0.5388\n",
      "Epoch 263/500, Training Loss: -0.5240\n",
      "Epoch 264/500, Training Loss: -0.5537\n",
      "Epoch 265/500, Training Loss: -0.5473\n",
      "Epoch 266/500, Training Loss: -0.5514\n",
      "Epoch 267/500, Training Loss: -0.5592\n",
      "Epoch 268/500, Training Loss: -0.5128\n",
      "Epoch 269/500, Training Loss: -0.5295\n",
      "Epoch 270/500, Training Loss: -0.5657\n",
      "Epoch 271/500, Training Loss: -0.5313\n",
      "Epoch 272/500, Training Loss: -0.5642\n",
      "Epoch 273/500, Training Loss: -0.5407\n",
      "Epoch 274/500, Training Loss: -0.5495\n",
      "Epoch 275/500, Training Loss: -0.5417\n",
      "Epoch 276/500, Training Loss: -0.5654\n",
      "Epoch 277/500, Training Loss: -0.5884\n",
      "Epoch 278/500, Training Loss: -0.4986\n",
      "Epoch 279/500, Training Loss: -0.5846\n",
      "Epoch 280/500, Training Loss: -0.5261\n",
      "Epoch 281/500, Training Loss: -0.5871\n",
      "Epoch 282/500, Training Loss: -0.5676\n",
      "Epoch 283/500, Training Loss: -0.5570\n",
      "Epoch 284/500, Training Loss: -0.5704\n",
      "Epoch 285/500, Training Loss: -0.5351\n",
      "Epoch 286/500, Training Loss: -0.5775\n",
      "Epoch 287/500, Training Loss: -0.5288\n",
      "Epoch 288/500, Training Loss: -0.5434\n",
      "Epoch 289/500, Training Loss: -0.5339\n",
      "Epoch 290/500, Training Loss: -0.5418\n",
      "Epoch 291/500, Training Loss: -0.5838\n",
      "Epoch 292/500, Training Loss: -0.5183\n",
      "Epoch 293/500, Training Loss: -0.5473\n",
      "Epoch 294/500, Training Loss: -0.5515\n",
      "Epoch 295/500, Training Loss: -0.5648\n",
      "Epoch 296/500, Training Loss: -0.5545\n",
      "Epoch 297/500, Training Loss: -0.5352\n",
      "Epoch 298/500, Training Loss: -0.5528\n",
      "Epoch 299/500, Training Loss: -0.5766\n",
      "Epoch 300/500, Training Loss: -0.5151\n",
      "Epoch 301/500, Training Loss: -0.5734\n",
      "Epoch 302/500, Training Loss: -0.5158\n",
      "Epoch 303/500, Training Loss: -0.5786\n",
      "Epoch 304/500, Training Loss: -0.5580\n",
      "Epoch 305/500, Training Loss: -0.5637\n",
      "Epoch 306/500, Training Loss: -0.4947\n",
      "Epoch 307/500, Training Loss: -0.5445\n",
      "Epoch 308/500, Training Loss: -0.5573\n",
      "Epoch 309/500, Training Loss: -0.5385\n",
      "Epoch 310/500, Training Loss: -0.5413\n",
      "Epoch 311/500, Training Loss: -0.5242\n",
      "Epoch 312/500, Training Loss: -0.5505\n",
      "Epoch 313/500, Training Loss: -0.5643\n",
      "Epoch 314/500, Training Loss: -0.5418\n",
      "Epoch 315/500, Training Loss: -0.5581\n",
      "Epoch 316/500, Training Loss: -0.5756\n",
      "Epoch 317/500, Training Loss: -0.5482\n",
      "Epoch 318/500, Training Loss: -0.5372\n",
      "Epoch 319/500, Training Loss: -0.5128\n",
      "Epoch 320/500, Training Loss: -0.5241\n",
      "Epoch 321/500, Training Loss: -0.5381\n",
      "Epoch 322/500, Training Loss: -0.5724\n",
      "Epoch 323/500, Training Loss: -0.5408\n",
      "Epoch 324/500, Training Loss: -0.5688\n",
      "Epoch 325/500, Training Loss: -0.5804\n",
      "Epoch 326/500, Training Loss: -0.6110\n",
      "Epoch 327/500, Training Loss: -0.5638\n",
      "Epoch 328/500, Training Loss: -0.5349\n",
      "Epoch 329/500, Training Loss: -0.6032\n",
      "Epoch 330/500, Training Loss: -0.5498\n",
      "Epoch 331/500, Training Loss: -0.5630\n",
      "Epoch 332/500, Training Loss: -0.5410\n",
      "Epoch 333/500, Training Loss: -0.5554\n",
      "Epoch 334/500, Training Loss: -0.5123\n",
      "Epoch 335/500, Training Loss: -0.5931\n",
      "Epoch 336/500, Training Loss: -0.5966\n",
      "Epoch 337/500, Training Loss: -0.5277\n",
      "Epoch 338/500, Training Loss: -0.5556\n",
      "Epoch 339/500, Training Loss: -0.5392\n",
      "Epoch 340/500, Training Loss: -0.5401\n",
      "Epoch 341/500, Training Loss: -0.5697\n",
      "Epoch 342/500, Training Loss: -0.5544\n",
      "Epoch 343/500, Training Loss: -0.5379\n",
      "Epoch 344/500, Training Loss: -0.5490\n",
      "Epoch 345/500, Training Loss: -0.5582\n",
      "Epoch 346/500, Training Loss: -0.5221\n",
      "Epoch 347/500, Training Loss: -0.6068\n",
      "Epoch 348/500, Training Loss: -0.5676\n",
      "Epoch 349/500, Training Loss: -0.5569\n",
      "Epoch 350/500, Training Loss: -0.5590\n",
      "Epoch 351/500, Training Loss: -0.5659\n",
      "Epoch 352/500, Training Loss: -0.5613\n",
      "Epoch 353/500, Training Loss: -0.5467\n",
      "Epoch 354/500, Training Loss: -0.5514\n",
      "Epoch 355/500, Training Loss: -0.5179\n",
      "Epoch 356/500, Training Loss: -0.5397\n",
      "Epoch 357/500, Training Loss: -0.5806\n",
      "Epoch 358/500, Training Loss: -0.5619\n",
      "Epoch 359/500, Training Loss: -0.5544\n",
      "Epoch 360/500, Training Loss: -0.5721\n",
      "Epoch 361/500, Training Loss: -0.5024\n",
      "Epoch 362/500, Training Loss: -0.5585\n",
      "Epoch 363/500, Training Loss: -0.5773\n",
      "Epoch 364/500, Training Loss: -0.5479\n",
      "Epoch 365/500, Training Loss: -0.5316\n",
      "Epoch 366/500, Training Loss: -0.5800\n",
      "Epoch 367/500, Training Loss: -0.5903\n",
      "Epoch 368/500, Training Loss: -0.5596\n",
      "Epoch 369/500, Training Loss: -0.6059\n",
      "Epoch 370/500, Training Loss: -0.5609\n",
      "Epoch 371/500, Training Loss: -0.6096\n",
      "Epoch 372/500, Training Loss: -0.5191\n",
      "Epoch 373/500, Training Loss: -0.5726\n",
      "Epoch 374/500, Training Loss: -0.5150\n",
      "Epoch 375/500, Training Loss: -0.5787\n",
      "Epoch 376/500, Training Loss: -0.5411\n",
      "Epoch 377/500, Training Loss: -0.5245\n",
      "Epoch 378/500, Training Loss: -0.5697\n",
      "Epoch 379/500, Training Loss: -0.5605\n",
      "Epoch 380/500, Training Loss: -0.5625\n",
      "Epoch 381/500, Training Loss: -0.5299\n",
      "Epoch 382/500, Training Loss: -0.5544\n",
      "Epoch 383/500, Training Loss: -0.5315\n",
      "Epoch 384/500, Training Loss: -0.5802\n",
      "Epoch 385/500, Training Loss: -0.5522\n",
      "Epoch 386/500, Training Loss: -0.5225\n",
      "Epoch 387/500, Training Loss: -0.5431\n",
      "Epoch 388/500, Training Loss: -0.5333\n",
      "Epoch 389/500, Training Loss: -0.5236\n",
      "Epoch 390/500, Training Loss: -0.5588\n",
      "Epoch 391/500, Training Loss: -0.5446\n",
      "Epoch 392/500, Training Loss: -0.5357\n",
      "Epoch 393/500, Training Loss: -0.5369\n",
      "Epoch 394/500, Training Loss: -0.5580\n",
      "Epoch 395/500, Training Loss: -0.5778\n",
      "Epoch 396/500, Training Loss: -0.5455\n",
      "Epoch 397/500, Training Loss: -0.5474\n",
      "Epoch 398/500, Training Loss: -0.5449\n",
      "Epoch 399/500, Training Loss: -0.5550\n",
      "Epoch 400/500, Training Loss: -0.5735\n",
      "Epoch 401/500, Training Loss: -0.5830\n",
      "Epoch 402/500, Training Loss: -0.5591\n",
      "Epoch 403/500, Training Loss: -0.5371\n",
      "Epoch 404/500, Training Loss: -0.5562\n",
      "Epoch 405/500, Training Loss: -0.5986\n",
      "Epoch 406/500, Training Loss: -0.5299\n",
      "Epoch 407/500, Training Loss: -0.5330\n",
      "Epoch 408/500, Training Loss: -0.5223\n",
      "Epoch 409/500, Training Loss: -0.5869\n",
      "Epoch 410/500, Training Loss: -0.5655\n",
      "Epoch 411/500, Training Loss: -0.5764\n",
      "Epoch 412/500, Training Loss: -0.5243\n",
      "Epoch 413/500, Training Loss: -0.5604\n",
      "Epoch 414/500, Training Loss: -0.5441\n",
      "Epoch 415/500, Training Loss: -0.5709\n",
      "Epoch 416/500, Training Loss: -0.5492\n",
      "Epoch 417/500, Training Loss: -0.5766\n",
      "Epoch 418/500, Training Loss: -0.5673\n",
      "Epoch 419/500, Training Loss: -0.5552\n",
      "Epoch 420/500, Training Loss: -0.5598\n",
      "Epoch 421/500, Training Loss: -0.5648\n",
      "Epoch 422/500, Training Loss: -0.5571\n",
      "Epoch 423/500, Training Loss: -0.5544\n",
      "Epoch 424/500, Training Loss: -0.5770\n",
      "Epoch 425/500, Training Loss: -0.5349\n",
      "Epoch 426/500, Training Loss: -0.4972\n",
      "Epoch 427/500, Training Loss: -0.5756\n",
      "Epoch 428/500, Training Loss: -0.5206\n",
      "Epoch 429/500, Training Loss: -0.5495\n",
      "Epoch 430/500, Training Loss: -0.5409\n",
      "Epoch 431/500, Training Loss: -0.5202\n",
      "Epoch 432/500, Training Loss: -0.5471\n",
      "Epoch 433/500, Training Loss: -0.5373\n",
      "Epoch 434/500, Training Loss: -0.5274\n",
      "Epoch 435/500, Training Loss: -0.5740\n",
      "Epoch 436/500, Training Loss: -0.5678\n",
      "Epoch 437/500, Training Loss: -0.5172\n",
      "Epoch 438/500, Training Loss: -0.5506\n",
      "Epoch 439/500, Training Loss: -0.5462\n",
      "Epoch 440/500, Training Loss: -0.5543\n",
      "Epoch 441/500, Training Loss: -0.5588\n",
      "Epoch 442/500, Training Loss: -0.5721\n",
      "Epoch 443/500, Training Loss: -0.5838\n",
      "Epoch 444/500, Training Loss: -0.5539\n",
      "Epoch 445/500, Training Loss: -0.5550\n",
      "Epoch 446/500, Training Loss: -0.5530\n",
      "Epoch 447/500, Training Loss: -0.5570\n",
      "Epoch 448/500, Training Loss: -0.5506\n",
      "Epoch 449/500, Training Loss: -0.5302\n",
      "Epoch 450/500, Training Loss: -0.5821\n",
      "Epoch 451/500, Training Loss: -0.5437\n",
      "Epoch 452/500, Training Loss: -0.5250\n",
      "Epoch 453/500, Training Loss: -0.5694\n",
      "Epoch 454/500, Training Loss: -0.5459\n",
      "Epoch 455/500, Training Loss: -0.5423\n",
      "Epoch 456/500, Training Loss: -0.5258\n",
      "Epoch 457/500, Training Loss: -0.5384\n",
      "Epoch 458/500, Training Loss: -0.5488\n",
      "Epoch 459/500, Training Loss: -0.5481\n",
      "Epoch 460/500, Training Loss: -0.5361\n",
      "Epoch 461/500, Training Loss: -0.5660\n",
      "Epoch 462/500, Training Loss: -0.5505\n",
      "Epoch 463/500, Training Loss: -0.5197\n",
      "Epoch 464/500, Training Loss: -0.5482\n",
      "Epoch 465/500, Training Loss: -0.5394\n",
      "Epoch 466/500, Training Loss: -0.5467\n",
      "Epoch 467/500, Training Loss: -0.5573\n",
      "Epoch 468/500, Training Loss: -0.5438\n",
      "Epoch 469/500, Training Loss: -0.5666\n",
      "Epoch 470/500, Training Loss: -0.5588\n",
      "Epoch 471/500, Training Loss: -0.5559\n",
      "Epoch 472/500, Training Loss: -0.5253\n",
      "Epoch 473/500, Training Loss: -0.5261\n",
      "Epoch 474/500, Training Loss: -0.5348\n",
      "Epoch 475/500, Training Loss: -0.5633\n",
      "Epoch 476/500, Training Loss: -0.5366\n",
      "Epoch 477/500, Training Loss: -0.5241\n",
      "Epoch 478/500, Training Loss: -0.5474\n",
      "Epoch 479/500, Training Loss: -0.5498\n",
      "Epoch 480/500, Training Loss: -0.5118\n",
      "Epoch 481/500, Training Loss: -0.5666\n",
      "Epoch 482/500, Training Loss: -0.5473\n",
      "Epoch 483/500, Training Loss: -0.5538\n",
      "Epoch 484/500, Training Loss: -0.5649\n",
      "Epoch 485/500, Training Loss: -0.5217\n",
      "Epoch 486/500, Training Loss: -0.5606\n",
      "Epoch 487/500, Training Loss: -0.5800\n",
      "Epoch 488/500, Training Loss: -0.5340\n",
      "Epoch 489/500, Training Loss: -0.5086\n",
      "Epoch 490/500, Training Loss: -0.5291\n",
      "Epoch 491/500, Training Loss: -0.5579\n",
      "Epoch 492/500, Training Loss: -0.5469\n",
      "Epoch 493/500, Training Loss: -0.5426\n",
      "Epoch 494/500, Training Loss: -0.5591\n",
      "Epoch 495/500, Training Loss: -0.5539\n",
      "Epoch 496/500, Training Loss: -0.5535\n",
      "Epoch 497/500, Training Loss: -0.5519\n",
      "Epoch 498/500, Training Loss: -0.5423\n",
      "Epoch 499/500, Training Loss: -0.5362\n",
      "Epoch 500/500, Training Loss: -0.5196\n",
      "Validation Loss: -0.5541\n",
      "Model 1, Fold 1: Validation Loss = -0.5541\n",
      "Epoch 1/500, Training Loss: -0.3831\n",
      "Epoch 2/500, Training Loss: -0.4508\n",
      "Epoch 3/500, Training Loss: -0.4756\n",
      "Epoch 4/500, Training Loss: -0.5480\n",
      "Epoch 5/500, Training Loss: -0.5353\n",
      "Epoch 6/500, Training Loss: -0.5418\n",
      "Epoch 7/500, Training Loss: -0.5315\n",
      "Epoch 8/500, Training Loss: -0.5446\n",
      "Epoch 9/500, Training Loss: -0.5578\n",
      "Epoch 10/500, Training Loss: -0.5783\n",
      "Epoch 11/500, Training Loss: -0.5721\n",
      "Epoch 12/500, Training Loss: -0.5404\n",
      "Epoch 13/500, Training Loss: -0.5621\n",
      "Epoch 14/500, Training Loss: -0.5470\n",
      "Epoch 15/500, Training Loss: -0.5980\n",
      "Epoch 16/500, Training Loss: -0.5900\n",
      "Epoch 17/500, Training Loss: -0.5402\n",
      "Epoch 18/500, Training Loss: -0.5742\n",
      "Epoch 19/500, Training Loss: -0.5245\n",
      "Epoch 20/500, Training Loss: -0.5235\n",
      "Epoch 21/500, Training Loss: -0.5512\n",
      "Epoch 22/500, Training Loss: -0.5404\n",
      "Epoch 23/500, Training Loss: -0.6027\n",
      "Epoch 24/500, Training Loss: -0.5993\n",
      "Epoch 25/500, Training Loss: -0.5733\n",
      "Epoch 26/500, Training Loss: -0.5383\n",
      "Epoch 27/500, Training Loss: -0.5550\n",
      "Epoch 28/500, Training Loss: -0.5867\n",
      "Epoch 29/500, Training Loss: -0.5601\n",
      "Epoch 30/500, Training Loss: -0.5793\n",
      "Epoch 31/500, Training Loss: -0.5790\n",
      "Epoch 32/500, Training Loss: -0.5576\n",
      "Epoch 33/500, Training Loss: -0.5711\n",
      "Epoch 34/500, Training Loss: -0.5355\n",
      "Epoch 35/500, Training Loss: -0.5784\n",
      "Epoch 36/500, Training Loss: -0.5862\n",
      "Epoch 37/500, Training Loss: -0.5547\n",
      "Epoch 38/500, Training Loss: -0.6027\n",
      "Epoch 39/500, Training Loss: -0.5624\n",
      "Epoch 40/500, Training Loss: -0.5608\n",
      "Epoch 41/500, Training Loss: -0.5756\n",
      "Epoch 42/500, Training Loss: -0.5708\n",
      "Epoch 43/500, Training Loss: -0.5576\n",
      "Epoch 44/500, Training Loss: -0.5903\n",
      "Epoch 45/500, Training Loss: -0.5435\n",
      "Epoch 46/500, Training Loss: -0.5698\n",
      "Epoch 47/500, Training Loss: -0.5408\n",
      "Epoch 48/500, Training Loss: -0.5796\n",
      "Epoch 49/500, Training Loss: -0.5659\n",
      "Epoch 50/500, Training Loss: -0.5331\n",
      "Epoch 51/500, Training Loss: -0.5680\n",
      "Epoch 52/500, Training Loss: -0.6216\n",
      "Epoch 53/500, Training Loss: -0.5533\n",
      "Epoch 54/500, Training Loss: -0.5479\n",
      "Epoch 55/500, Training Loss: -0.5841\n",
      "Epoch 56/500, Training Loss: -0.5909\n",
      "Epoch 57/500, Training Loss: -0.5869\n",
      "Epoch 58/500, Training Loss: -0.5837\n",
      "Epoch 59/500, Training Loss: -0.5670\n",
      "Epoch 60/500, Training Loss: -0.5272\n",
      "Epoch 61/500, Training Loss: -0.5358\n",
      "Epoch 62/500, Training Loss: -0.5875\n",
      "Epoch 63/500, Training Loss: -0.5567\n",
      "Epoch 64/500, Training Loss: -0.5811\n",
      "Epoch 65/500, Training Loss: -0.5487\n",
      "Epoch 66/500, Training Loss: -0.5476\n",
      "Epoch 67/500, Training Loss: -0.5400\n",
      "Epoch 68/500, Training Loss: -0.5622\n",
      "Epoch 69/500, Training Loss: -0.6043\n",
      "Epoch 70/500, Training Loss: -0.5389\n",
      "Epoch 71/500, Training Loss: -0.5550\n",
      "Epoch 72/500, Training Loss: -0.5862\n",
      "Epoch 73/500, Training Loss: -0.5498\n",
      "Epoch 74/500, Training Loss: -0.5412\n",
      "Epoch 75/500, Training Loss: -0.5650\n",
      "Epoch 76/500, Training Loss: -0.5309\n",
      "Epoch 77/500, Training Loss: -0.5701\n",
      "Epoch 78/500, Training Loss: -0.5541\n",
      "Epoch 79/500, Training Loss: -0.5747\n",
      "Epoch 80/500, Training Loss: -0.5411\n",
      "Epoch 81/500, Training Loss: -0.5639\n",
      "Epoch 82/500, Training Loss: -0.5987\n",
      "Epoch 83/500, Training Loss: -0.6009\n",
      "Epoch 84/500, Training Loss: -0.5514\n",
      "Epoch 85/500, Training Loss: -0.6309\n",
      "Epoch 86/500, Training Loss: -0.5699\n",
      "Epoch 87/500, Training Loss: -0.5810\n",
      "Epoch 88/500, Training Loss: -0.5713\n",
      "Epoch 89/500, Training Loss: -0.5835\n",
      "Epoch 90/500, Training Loss: -0.5708\n",
      "Epoch 91/500, Training Loss: -0.5867\n",
      "Epoch 92/500, Training Loss: -0.5630\n",
      "Epoch 93/500, Training Loss: -0.5559\n",
      "Epoch 94/500, Training Loss: -0.5874\n",
      "Epoch 95/500, Training Loss: -0.5498\n",
      "Epoch 96/500, Training Loss: -0.5264\n",
      "Epoch 97/500, Training Loss: -0.5201\n",
      "Epoch 98/500, Training Loss: -0.5309\n",
      "Epoch 99/500, Training Loss: -0.5643\n",
      "Epoch 100/500, Training Loss: -0.5993\n",
      "Epoch 101/500, Training Loss: -0.5856\n",
      "Epoch 102/500, Training Loss: -0.5462\n",
      "Epoch 103/500, Training Loss: -0.5559\n",
      "Epoch 104/500, Training Loss: -0.5567\n",
      "Epoch 105/500, Training Loss: -0.5510\n",
      "Epoch 106/500, Training Loss: -0.5783\n",
      "Epoch 107/500, Training Loss: -0.6183\n",
      "Epoch 108/500, Training Loss: -0.5390\n",
      "Epoch 109/500, Training Loss: -0.5515\n",
      "Epoch 110/500, Training Loss: -0.5709\n",
      "Epoch 111/500, Training Loss: -0.5152\n",
      "Epoch 112/500, Training Loss: -0.5689\n",
      "Epoch 113/500, Training Loss: -0.5620\n",
      "Epoch 114/500, Training Loss: -0.5726\n",
      "Epoch 115/500, Training Loss: -0.5649\n",
      "Epoch 116/500, Training Loss: -0.5539\n",
      "Epoch 117/500, Training Loss: -0.5598\n",
      "Epoch 118/500, Training Loss: -0.5451\n",
      "Epoch 119/500, Training Loss: -0.5715\n",
      "Epoch 120/500, Training Loss: -0.5777\n",
      "Epoch 121/500, Training Loss: -0.5872\n",
      "Epoch 122/500, Training Loss: -0.5728\n",
      "Epoch 123/500, Training Loss: -0.6220\n",
      "Epoch 124/500, Training Loss: -0.5454\n",
      "Epoch 125/500, Training Loss: -0.5909\n",
      "Epoch 126/500, Training Loss: -0.5931\n",
      "Epoch 127/500, Training Loss: -0.5945\n",
      "Epoch 128/500, Training Loss: -0.5745\n",
      "Epoch 129/500, Training Loss: -0.5721\n",
      "Epoch 130/500, Training Loss: -0.6322\n",
      "Epoch 131/500, Training Loss: -0.5529\n",
      "Epoch 132/500, Training Loss: -0.5749\n",
      "Epoch 133/500, Training Loss: -0.5566\n",
      "Epoch 134/500, Training Loss: -0.5295\n",
      "Epoch 135/500, Training Loss: -0.5594\n",
      "Epoch 136/500, Training Loss: -0.5734\n",
      "Epoch 137/500, Training Loss: -0.5565\n",
      "Epoch 138/500, Training Loss: -0.5818\n",
      "Epoch 139/500, Training Loss: -0.6267\n",
      "Epoch 140/500, Training Loss: -0.5408\n",
      "Epoch 141/500, Training Loss: -0.6095\n",
      "Epoch 142/500, Training Loss: -0.5596\n",
      "Epoch 143/500, Training Loss: -0.5610\n",
      "Epoch 144/500, Training Loss: -0.5875\n",
      "Epoch 145/500, Training Loss: -0.5317\n",
      "Epoch 146/500, Training Loss: -0.5442\n",
      "Epoch 147/500, Training Loss: -0.5653\n",
      "Epoch 148/500, Training Loss: -0.5832\n",
      "Epoch 149/500, Training Loss: -0.5705\n",
      "Epoch 150/500, Training Loss: -0.5759\n",
      "Epoch 151/500, Training Loss: -0.5967\n",
      "Epoch 152/500, Training Loss: -0.5902\n",
      "Epoch 153/500, Training Loss: -0.5382\n",
      "Epoch 154/500, Training Loss: -0.5894\n",
      "Epoch 155/500, Training Loss: -0.5926\n",
      "Epoch 156/500, Training Loss: -0.6400\n",
      "Epoch 157/500, Training Loss: -0.5998\n",
      "Epoch 158/500, Training Loss: -0.5616\n",
      "Epoch 159/500, Training Loss: -0.5669\n",
      "Epoch 160/500, Training Loss: -0.5828\n",
      "Epoch 161/500, Training Loss: -0.5832\n",
      "Epoch 162/500, Training Loss: -0.5785\n",
      "Epoch 163/500, Training Loss: -0.6047\n",
      "Epoch 164/500, Training Loss: -0.5675\n",
      "Epoch 165/500, Training Loss: -0.5617\n",
      "Epoch 166/500, Training Loss: -0.6002\n",
      "Epoch 167/500, Training Loss: -0.5748\n",
      "Epoch 168/500, Training Loss: -0.5835\n",
      "Epoch 169/500, Training Loss: -0.5630\n",
      "Epoch 170/500, Training Loss: -0.5661\n",
      "Epoch 171/500, Training Loss: -0.5305\n",
      "Epoch 172/500, Training Loss: -0.5497\n",
      "Epoch 173/500, Training Loss: -0.6192\n",
      "Epoch 174/500, Training Loss: -0.6117\n",
      "Epoch 175/500, Training Loss: -0.5875\n",
      "Epoch 176/500, Training Loss: -0.6043\n",
      "Epoch 177/500, Training Loss: -0.5788\n",
      "Epoch 178/500, Training Loss: -0.5304\n",
      "Epoch 179/500, Training Loss: -0.5694\n",
      "Epoch 180/500, Training Loss: -0.5870\n",
      "Epoch 181/500, Training Loss: -0.5713\n",
      "Epoch 182/500, Training Loss: -0.5704\n",
      "Epoch 183/500, Training Loss: -0.6132\n",
      "Epoch 184/500, Training Loss: -0.5615\n",
      "Epoch 185/500, Training Loss: -0.5733\n",
      "Epoch 186/500, Training Loss: -0.5498\n",
      "Epoch 187/500, Training Loss: -0.5638\n",
      "Epoch 188/500, Training Loss: -0.5335\n",
      "Epoch 189/500, Training Loss: -0.5703\n",
      "Epoch 190/500, Training Loss: -0.5599\n",
      "Epoch 191/500, Training Loss: -0.5332\n",
      "Epoch 192/500, Training Loss: -0.5656\n",
      "Epoch 193/500, Training Loss: -0.5882\n",
      "Epoch 194/500, Training Loss: -0.5701\n",
      "Epoch 195/500, Training Loss: -0.5544\n",
      "Epoch 196/500, Training Loss: -0.5577\n",
      "Epoch 197/500, Training Loss: -0.5745\n",
      "Epoch 198/500, Training Loss: -0.5649\n",
      "Epoch 199/500, Training Loss: -0.5989\n",
      "Epoch 200/500, Training Loss: -0.5930\n",
      "Epoch 201/500, Training Loss: -0.5636\n",
      "Epoch 202/500, Training Loss: -0.5441\n",
      "Epoch 203/500, Training Loss: -0.5625\n",
      "Epoch 204/500, Training Loss: -0.5654\n",
      "Epoch 205/500, Training Loss: -0.5677\n",
      "Epoch 206/500, Training Loss: -0.5547\n",
      "Epoch 207/500, Training Loss: -0.5847\n",
      "Epoch 208/500, Training Loss: -0.5750\n",
      "Epoch 209/500, Training Loss: -0.5266\n",
      "Epoch 210/500, Training Loss: -0.5677\n",
      "Epoch 211/500, Training Loss: -0.5655\n",
      "Epoch 212/500, Training Loss: -0.5671\n",
      "Epoch 213/500, Training Loss: -0.6201\n",
      "Epoch 214/500, Training Loss: -0.6261\n",
      "Epoch 215/500, Training Loss: -0.5957\n",
      "Epoch 216/500, Training Loss: -0.5527\n",
      "Epoch 217/500, Training Loss: -0.5768\n",
      "Epoch 218/500, Training Loss: -0.5649\n",
      "Epoch 219/500, Training Loss: -0.6083\n",
      "Epoch 220/500, Training Loss: -0.5422\n",
      "Epoch 221/500, Training Loss: -0.5529\n",
      "Epoch 222/500, Training Loss: -0.5679\n",
      "Epoch 223/500, Training Loss: -0.5862\n",
      "Epoch 224/500, Training Loss: -0.5708\n",
      "Epoch 225/500, Training Loss: -0.5752\n",
      "Epoch 226/500, Training Loss: -0.5396\n",
      "Epoch 227/500, Training Loss: -0.5978\n",
      "Epoch 228/500, Training Loss: -0.5607\n",
      "Epoch 229/500, Training Loss: -0.5547\n",
      "Epoch 230/500, Training Loss: -0.5561\n",
      "Epoch 231/500, Training Loss: -0.5316\n",
      "Epoch 232/500, Training Loss: -0.5566\n",
      "Epoch 233/500, Training Loss: -0.5910\n",
      "Epoch 234/500, Training Loss: -0.5911\n",
      "Epoch 235/500, Training Loss: -0.6108\n",
      "Epoch 236/500, Training Loss: -0.5722\n",
      "Epoch 237/500, Training Loss: -0.5592\n",
      "Epoch 238/500, Training Loss: -0.5796\n",
      "Epoch 239/500, Training Loss: -0.5412\n",
      "Epoch 240/500, Training Loss: -0.5550\n",
      "Epoch 241/500, Training Loss: -0.5366\n",
      "Epoch 242/500, Training Loss: -0.5932\n",
      "Epoch 243/500, Training Loss: -0.5885\n",
      "Epoch 244/500, Training Loss: -0.5726\n",
      "Epoch 245/500, Training Loss: -0.5847\n",
      "Epoch 246/500, Training Loss: -0.5695\n",
      "Epoch 247/500, Training Loss: -0.5352\n",
      "Epoch 248/500, Training Loss: -0.5530\n",
      "Epoch 249/500, Training Loss: -0.5787\n",
      "Epoch 250/500, Training Loss: -0.5757\n",
      "Epoch 251/500, Training Loss: -0.5587\n",
      "Epoch 252/500, Training Loss: -0.5697\n",
      "Epoch 253/500, Training Loss: -0.5388\n",
      "Epoch 254/500, Training Loss: -0.5803\n",
      "Epoch 255/500, Training Loss: -0.5744\n",
      "Epoch 256/500, Training Loss: -0.5674\n",
      "Epoch 257/500, Training Loss: -0.5732\n",
      "Epoch 258/500, Training Loss: -0.5846\n",
      "Epoch 259/500, Training Loss: -0.5632\n",
      "Epoch 260/500, Training Loss: -0.5667\n",
      "Epoch 261/500, Training Loss: -0.5580\n",
      "Epoch 262/500, Training Loss: -0.5833\n",
      "Epoch 263/500, Training Loss: -0.5207\n",
      "Epoch 264/500, Training Loss: -0.5617\n",
      "Epoch 265/500, Training Loss: -0.5520\n",
      "Epoch 266/500, Training Loss: -0.5665\n",
      "Epoch 267/500, Training Loss: -0.5622\n",
      "Epoch 268/500, Training Loss: -0.6057\n",
      "Epoch 269/500, Training Loss: -0.5936\n",
      "Epoch 270/500, Training Loss: -0.5809\n",
      "Epoch 271/500, Training Loss: -0.5553\n",
      "Epoch 272/500, Training Loss: -0.5712\n",
      "Epoch 273/500, Training Loss: -0.5849\n",
      "Epoch 274/500, Training Loss: -0.5731\n",
      "Epoch 275/500, Training Loss: -0.5676\n",
      "Epoch 276/500, Training Loss: -0.5288\n",
      "Epoch 277/500, Training Loss: -0.5717\n",
      "Epoch 278/500, Training Loss: -0.5613\n",
      "Epoch 279/500, Training Loss: -0.5573\n",
      "Epoch 280/500, Training Loss: -0.5263\n",
      "Epoch 281/500, Training Loss: -0.5606\n",
      "Epoch 282/500, Training Loss: -0.5744\n",
      "Epoch 283/500, Training Loss: -0.5741\n",
      "Epoch 284/500, Training Loss: -0.5453\n",
      "Epoch 285/500, Training Loss: -0.5606\n",
      "Epoch 286/500, Training Loss: -0.5900\n",
      "Epoch 287/500, Training Loss: -0.5551\n",
      "Epoch 288/500, Training Loss: -0.5949\n",
      "Epoch 289/500, Training Loss: -0.5625\n",
      "Epoch 290/500, Training Loss: -0.5671\n",
      "Epoch 291/500, Training Loss: -0.5457\n",
      "Epoch 292/500, Training Loss: -0.5698\n",
      "Epoch 293/500, Training Loss: -0.5208\n",
      "Epoch 294/500, Training Loss: -0.5669\n",
      "Epoch 295/500, Training Loss: -0.6124\n",
      "Epoch 296/500, Training Loss: -0.5782\n",
      "Epoch 297/500, Training Loss: -0.5500\n",
      "Epoch 298/500, Training Loss: -0.5792\n",
      "Epoch 299/500, Training Loss: -0.5568\n",
      "Epoch 300/500, Training Loss: -0.5700\n",
      "Epoch 301/500, Training Loss: -0.6124\n",
      "Epoch 302/500, Training Loss: -0.6032\n",
      "Epoch 303/500, Training Loss: -0.5982\n",
      "Epoch 304/500, Training Loss: -0.5808\n",
      "Epoch 305/500, Training Loss: -0.5644\n",
      "Epoch 306/500, Training Loss: -0.5290\n",
      "Epoch 307/500, Training Loss: -0.6121\n",
      "Epoch 308/500, Training Loss: -0.5952\n",
      "Epoch 309/500, Training Loss: -0.5620\n",
      "Epoch 310/500, Training Loss: -0.5689\n",
      "Epoch 311/500, Training Loss: -0.5752\n",
      "Epoch 312/500, Training Loss: -0.5610\n",
      "Epoch 313/500, Training Loss: -0.5477\n",
      "Epoch 314/500, Training Loss: -0.5729\n",
      "Epoch 315/500, Training Loss: -0.5737\n",
      "Epoch 316/500, Training Loss: -0.5791\n",
      "Epoch 317/500, Training Loss: -0.5708\n",
      "Epoch 318/500, Training Loss: -0.5028\n",
      "Epoch 319/500, Training Loss: -0.5736\n",
      "Epoch 320/500, Training Loss: -0.5690\n",
      "Epoch 321/500, Training Loss: -0.5635\n",
      "Epoch 322/500, Training Loss: -0.5660\n",
      "Epoch 323/500, Training Loss: -0.5725\n",
      "Epoch 324/500, Training Loss: -0.5338\n",
      "Epoch 325/500, Training Loss: -0.5305\n",
      "Epoch 326/500, Training Loss: -0.5648\n",
      "Epoch 327/500, Training Loss: -0.5957\n",
      "Epoch 328/500, Training Loss: -0.5728\n",
      "Epoch 329/500, Training Loss: -0.6026\n",
      "Epoch 330/500, Training Loss: -0.5759\n",
      "Epoch 331/500, Training Loss: -0.5489\n",
      "Epoch 332/500, Training Loss: -0.6148\n",
      "Epoch 333/500, Training Loss: -0.6020\n",
      "Epoch 334/500, Training Loss: -0.5320\n",
      "Epoch 335/500, Training Loss: -0.5379\n",
      "Epoch 336/500, Training Loss: -0.5760\n",
      "Epoch 337/500, Training Loss: -0.5231\n",
      "Epoch 338/500, Training Loss: -0.5494\n",
      "Epoch 339/500, Training Loss: -0.5431\n",
      "Epoch 340/500, Training Loss: -0.5899\n",
      "Epoch 341/500, Training Loss: -0.5355\n",
      "Epoch 342/500, Training Loss: -0.5534\n",
      "Epoch 343/500, Training Loss: -0.5465\n",
      "Epoch 344/500, Training Loss: -0.5430\n",
      "Epoch 345/500, Training Loss: -0.5641\n",
      "Epoch 346/500, Training Loss: -0.5841\n",
      "Epoch 347/500, Training Loss: -0.5865\n",
      "Epoch 348/500, Training Loss: -0.6100\n",
      "Epoch 349/500, Training Loss: -0.5808\n",
      "Epoch 350/500, Training Loss: -0.5699\n",
      "Epoch 351/500, Training Loss: -0.5638\n",
      "Epoch 352/500, Training Loss: -0.5608\n",
      "Epoch 353/500, Training Loss: -0.5877\n",
      "Epoch 354/500, Training Loss: -0.6056\n",
      "Epoch 355/500, Training Loss: -0.5980\n",
      "Epoch 356/500, Training Loss: -0.5679\n",
      "Epoch 357/500, Training Loss: -0.6056\n",
      "Epoch 358/500, Training Loss: -0.5691\n",
      "Epoch 359/500, Training Loss: -0.5811\n",
      "Epoch 360/500, Training Loss: -0.5883\n",
      "Epoch 361/500, Training Loss: -0.5841\n",
      "Epoch 362/500, Training Loss: -0.5757\n",
      "Epoch 363/500, Training Loss: -0.5608\n",
      "Epoch 364/500, Training Loss: -0.5685\n",
      "Epoch 365/500, Training Loss: -0.5567\n",
      "Epoch 366/500, Training Loss: -0.5811\n",
      "Epoch 367/500, Training Loss: -0.5549\n",
      "Epoch 368/500, Training Loss: -0.5441\n",
      "Epoch 369/500, Training Loss: -0.5458\n",
      "Epoch 370/500, Training Loss: -0.5593\n",
      "Epoch 371/500, Training Loss: -0.5992\n",
      "Epoch 372/500, Training Loss: -0.5620\n",
      "Epoch 373/500, Training Loss: -0.5547\n",
      "Epoch 374/500, Training Loss: -0.5941\n",
      "Epoch 375/500, Training Loss: -0.5675\n",
      "Epoch 376/500, Training Loss: -0.5981\n",
      "Epoch 377/500, Training Loss: -0.5395\n",
      "Epoch 378/500, Training Loss: -0.5474\n",
      "Epoch 379/500, Training Loss: -0.5535\n",
      "Epoch 380/500, Training Loss: -0.5807\n",
      "Epoch 381/500, Training Loss: -0.5764\n",
      "Epoch 382/500, Training Loss: -0.5927\n",
      "Epoch 383/500, Training Loss: -0.5664\n",
      "Epoch 384/500, Training Loss: -0.5806\n",
      "Epoch 385/500, Training Loss: -0.5755\n",
      "Epoch 386/500, Training Loss: -0.6165\n",
      "Epoch 387/500, Training Loss: -0.5618\n",
      "Epoch 388/500, Training Loss: -0.5946\n",
      "Epoch 389/500, Training Loss: -0.5675\n",
      "Epoch 390/500, Training Loss: -0.5399\n",
      "Epoch 391/500, Training Loss: -0.5670\n",
      "Epoch 392/500, Training Loss: -0.5834\n",
      "Epoch 393/500, Training Loss: -0.5799\n",
      "Epoch 394/500, Training Loss: -0.5794\n",
      "Epoch 395/500, Training Loss: -0.5896\n",
      "Epoch 396/500, Training Loss: -0.5883\n",
      "Epoch 397/500, Training Loss: -0.5518\n",
      "Epoch 398/500, Training Loss: -0.5545\n",
      "Epoch 399/500, Training Loss: -0.5770\n",
      "Epoch 400/500, Training Loss: -0.5813\n",
      "Epoch 401/500, Training Loss: -0.6381\n",
      "Epoch 402/500, Training Loss: -0.5857\n",
      "Epoch 403/500, Training Loss: -0.5530\n",
      "Epoch 404/500, Training Loss: -0.5704\n",
      "Epoch 405/500, Training Loss: -0.5861\n",
      "Epoch 406/500, Training Loss: -0.5172\n",
      "Epoch 407/500, Training Loss: -0.5732\n",
      "Epoch 408/500, Training Loss: -0.5816\n",
      "Epoch 409/500, Training Loss: -0.5746\n",
      "Epoch 410/500, Training Loss: -0.5577\n",
      "Epoch 411/500, Training Loss: -0.5793\n",
      "Epoch 412/500, Training Loss: -0.5854\n",
      "Epoch 413/500, Training Loss: -0.5821\n",
      "Epoch 414/500, Training Loss: -0.5733\n",
      "Epoch 415/500, Training Loss: -0.5951\n",
      "Epoch 416/500, Training Loss: -0.5893\n",
      "Epoch 417/500, Training Loss: -0.5679\n",
      "Epoch 418/500, Training Loss: -0.5392\n",
      "Epoch 419/500, Training Loss: -0.5780\n",
      "Epoch 420/500, Training Loss: -0.5756\n",
      "Epoch 421/500, Training Loss: -0.5491\n",
      "Epoch 422/500, Training Loss: -0.5428\n",
      "Epoch 423/500, Training Loss: -0.5417\n",
      "Epoch 424/500, Training Loss: -0.5994\n",
      "Epoch 425/500, Training Loss: -0.6104\n",
      "Epoch 426/500, Training Loss: -0.5585\n",
      "Epoch 427/500, Training Loss: -0.6118\n",
      "Epoch 428/500, Training Loss: -0.5665\n",
      "Epoch 429/500, Training Loss: -0.6032\n",
      "Epoch 430/500, Training Loss: -0.5792\n",
      "Epoch 431/500, Training Loss: -0.5708\n",
      "Epoch 432/500, Training Loss: -0.5736\n",
      "Epoch 433/500, Training Loss: -0.5750\n",
      "Epoch 434/500, Training Loss: -0.6079\n",
      "Epoch 435/500, Training Loss: -0.5971\n",
      "Epoch 436/500, Training Loss: -0.5626\n",
      "Epoch 437/500, Training Loss: -0.5684\n",
      "Epoch 438/500, Training Loss: -0.5898\n",
      "Epoch 439/500, Training Loss: -0.5825\n",
      "Epoch 440/500, Training Loss: -0.5910\n",
      "Epoch 441/500, Training Loss: -0.5376\n",
      "Epoch 442/500, Training Loss: -0.5392\n",
      "Epoch 443/500, Training Loss: -0.5657\n",
      "Epoch 444/500, Training Loss: -0.6047\n",
      "Epoch 445/500, Training Loss: -0.5690\n",
      "Epoch 446/500, Training Loss: -0.5470\n",
      "Epoch 447/500, Training Loss: -0.5992\n",
      "Epoch 448/500, Training Loss: -0.5462\n",
      "Epoch 449/500, Training Loss: -0.5994\n",
      "Epoch 450/500, Training Loss: -0.5910\n",
      "Epoch 451/500, Training Loss: -0.5909\n",
      "Epoch 452/500, Training Loss: -0.5925\n",
      "Epoch 453/500, Training Loss: -0.5442\n",
      "Epoch 454/500, Training Loss: -0.5662\n",
      "Epoch 455/500, Training Loss: -0.5524\n",
      "Epoch 456/500, Training Loss: -0.5663\n",
      "Epoch 457/500, Training Loss: -0.5791\n",
      "Epoch 458/500, Training Loss: -0.5880\n",
      "Epoch 459/500, Training Loss: -0.5622\n",
      "Epoch 460/500, Training Loss: -0.5414\n",
      "Epoch 461/500, Training Loss: -0.5885\n",
      "Epoch 462/500, Training Loss: -0.6070\n",
      "Epoch 463/500, Training Loss: -0.5702\n",
      "Epoch 464/500, Training Loss: -0.5568\n",
      "Epoch 465/500, Training Loss: -0.5136\n",
      "Epoch 466/500, Training Loss: -0.5704\n",
      "Epoch 467/500, Training Loss: -0.5932\n",
      "Epoch 468/500, Training Loss: -0.5796\n",
      "Epoch 469/500, Training Loss: -0.5956\n",
      "Epoch 470/500, Training Loss: -0.6227\n",
      "Epoch 471/500, Training Loss: -0.5632\n",
      "Epoch 472/500, Training Loss: -0.6062\n",
      "Epoch 473/500, Training Loss: -0.5343\n",
      "Epoch 474/500, Training Loss: -0.5885\n",
      "Epoch 475/500, Training Loss: -0.6167\n",
      "Epoch 476/500, Training Loss: -0.5499\n",
      "Epoch 477/500, Training Loss: -0.5745\n",
      "Epoch 478/500, Training Loss: -0.5766\n",
      "Epoch 479/500, Training Loss: -0.5782\n",
      "Epoch 480/500, Training Loss: -0.5830\n",
      "Epoch 481/500, Training Loss: -0.5804\n",
      "Epoch 482/500, Training Loss: -0.5457\n",
      "Epoch 483/500, Training Loss: -0.5703\n",
      "Epoch 484/500, Training Loss: -0.5953\n",
      "Epoch 485/500, Training Loss: -0.6042\n",
      "Epoch 486/500, Training Loss: -0.5694\n",
      "Epoch 487/500, Training Loss: -0.5724\n",
      "Epoch 488/500, Training Loss: -0.5590\n",
      "Epoch 489/500, Training Loss: -0.5646\n",
      "Epoch 490/500, Training Loss: -0.5675\n",
      "Epoch 491/500, Training Loss: -0.6286\n",
      "Epoch 492/500, Training Loss: -0.5915\n",
      "Epoch 493/500, Training Loss: -0.6038\n",
      "Epoch 494/500, Training Loss: -0.5762\n",
      "Epoch 495/500, Training Loss: -0.5824\n",
      "Epoch 496/500, Training Loss: -0.5786\n",
      "Epoch 497/500, Training Loss: -0.5721\n",
      "Epoch 498/500, Training Loss: -0.5747\n",
      "Epoch 499/500, Training Loss: -0.5624\n",
      "Epoch 500/500, Training Loss: -0.5716\n",
      "Validation Loss: -0.5366\n",
      "Model 2, Fold 0: Validation Loss = -0.5366\n",
      "Epoch 1/500, Training Loss: -0.3467\n",
      "Epoch 2/500, Training Loss: -0.5236\n",
      "Epoch 3/500, Training Loss: -0.4985\n",
      "Epoch 4/500, Training Loss: -0.5188\n",
      "Epoch 5/500, Training Loss: -0.5307\n",
      "Epoch 6/500, Training Loss: -0.4882\n",
      "Epoch 7/500, Training Loss: -0.5508\n",
      "Epoch 8/500, Training Loss: -0.5327\n",
      "Epoch 9/500, Training Loss: -0.5369\n",
      "Epoch 10/500, Training Loss: -0.5645\n",
      "Epoch 11/500, Training Loss: -0.5363\n",
      "Epoch 12/500, Training Loss: -0.5374\n",
      "Epoch 13/500, Training Loss: -0.5248\n",
      "Epoch 14/500, Training Loss: -0.5051\n",
      "Epoch 15/500, Training Loss: -0.5537\n",
      "Epoch 16/500, Training Loss: -0.5720\n",
      "Epoch 17/500, Training Loss: -0.5608\n",
      "Epoch 18/500, Training Loss: -0.5479\n",
      "Epoch 19/500, Training Loss: -0.5331\n",
      "Epoch 20/500, Training Loss: -0.5115\n",
      "Epoch 21/500, Training Loss: -0.5496\n",
      "Epoch 22/500, Training Loss: -0.5545\n",
      "Epoch 23/500, Training Loss: -0.5323\n",
      "Epoch 24/500, Training Loss: -0.5534\n",
      "Epoch 25/500, Training Loss: -0.5725\n",
      "Epoch 26/500, Training Loss: -0.5316\n",
      "Epoch 27/500, Training Loss: -0.5640\n",
      "Epoch 28/500, Training Loss: -0.5597\n",
      "Epoch 29/500, Training Loss: -0.5279\n",
      "Epoch 30/500, Training Loss: -0.5874\n",
      "Epoch 31/500, Training Loss: -0.5616\n",
      "Epoch 32/500, Training Loss: -0.5677\n",
      "Epoch 33/500, Training Loss: -0.5421\n",
      "Epoch 34/500, Training Loss: -0.5431\n",
      "Epoch 35/500, Training Loss: -0.5117\n",
      "Epoch 36/500, Training Loss: -0.5461\n",
      "Epoch 37/500, Training Loss: -0.5825\n",
      "Epoch 38/500, Training Loss: -0.5615\n",
      "Epoch 39/500, Training Loss: -0.5314\n",
      "Epoch 40/500, Training Loss: -0.5449\n",
      "Epoch 41/500, Training Loss: -0.5519\n",
      "Epoch 42/500, Training Loss: -0.5981\n",
      "Epoch 43/500, Training Loss: -0.5600\n",
      "Epoch 44/500, Training Loss: -0.5210\n",
      "Epoch 45/500, Training Loss: -0.5384\n",
      "Epoch 46/500, Training Loss: -0.5818\n",
      "Epoch 47/500, Training Loss: -0.5365\n",
      "Epoch 48/500, Training Loss: -0.5740\n",
      "Epoch 49/500, Training Loss: -0.5354\n",
      "Epoch 50/500, Training Loss: -0.5641\n",
      "Epoch 51/500, Training Loss: -0.5437\n",
      "Epoch 52/500, Training Loss: -0.5140\n",
      "Epoch 53/500, Training Loss: -0.5553\n",
      "Epoch 54/500, Training Loss: -0.5682\n",
      "Epoch 55/500, Training Loss: -0.5729\n",
      "Epoch 56/500, Training Loss: -0.5517\n",
      "Epoch 57/500, Training Loss: -0.5425\n",
      "Epoch 58/500, Training Loss: -0.5310\n",
      "Epoch 59/500, Training Loss: -0.5180\n",
      "Epoch 60/500, Training Loss: -0.5655\n",
      "Epoch 61/500, Training Loss: -0.5551\n",
      "Epoch 62/500, Training Loss: -0.4912\n",
      "Epoch 63/500, Training Loss: -0.5459\n",
      "Epoch 64/500, Training Loss: -0.5280\n",
      "Epoch 65/500, Training Loss: -0.5162\n",
      "Epoch 66/500, Training Loss: -0.5449\n",
      "Epoch 67/500, Training Loss: -0.5492\n",
      "Epoch 68/500, Training Loss: -0.5500\n",
      "Epoch 69/500, Training Loss: -0.5462\n",
      "Epoch 70/500, Training Loss: -0.5716\n",
      "Epoch 71/500, Training Loss: -0.5237\n",
      "Epoch 72/500, Training Loss: -0.5766\n",
      "Epoch 73/500, Training Loss: -0.5664\n",
      "Epoch 74/500, Training Loss: -0.5483\n",
      "Epoch 75/500, Training Loss: -0.5644\n",
      "Epoch 76/500, Training Loss: -0.5917\n",
      "Epoch 77/500, Training Loss: -0.5267\n",
      "Epoch 78/500, Training Loss: -0.5341\n",
      "Epoch 79/500, Training Loss: -0.5773\n",
      "Epoch 80/500, Training Loss: -0.5188\n",
      "Epoch 81/500, Training Loss: -0.5640\n",
      "Epoch 82/500, Training Loss: -0.5770\n",
      "Epoch 83/500, Training Loss: -0.5689\n",
      "Epoch 84/500, Training Loss: -0.5436\n",
      "Epoch 85/500, Training Loss: -0.5329\n",
      "Epoch 86/500, Training Loss: -0.5734\n",
      "Epoch 87/500, Training Loss: -0.5471\n",
      "Epoch 88/500, Training Loss: -0.5271\n",
      "Epoch 89/500, Training Loss: -0.5420\n",
      "Epoch 90/500, Training Loss: -0.5466\n",
      "Epoch 91/500, Training Loss: -0.5707\n",
      "Epoch 92/500, Training Loss: -0.5433\n",
      "Epoch 93/500, Training Loss: -0.5517\n",
      "Epoch 94/500, Training Loss: -0.5435\n",
      "Epoch 95/500, Training Loss: -0.5437\n",
      "Epoch 96/500, Training Loss: -0.5601\n",
      "Epoch 97/500, Training Loss: -0.5426\n",
      "Epoch 98/500, Training Loss: -0.5380\n",
      "Epoch 99/500, Training Loss: -0.5522\n",
      "Epoch 100/500, Training Loss: -0.5458\n",
      "Epoch 101/500, Training Loss: -0.5607\n",
      "Epoch 102/500, Training Loss: -0.5874\n",
      "Epoch 103/500, Training Loss: -0.5256\n",
      "Epoch 104/500, Training Loss: -0.5757\n",
      "Epoch 105/500, Training Loss: -0.5938\n",
      "Epoch 106/500, Training Loss: -0.5414\n",
      "Epoch 107/500, Training Loss: -0.5358\n",
      "Epoch 108/500, Training Loss: -0.5418\n",
      "Epoch 109/500, Training Loss: -0.5692\n",
      "Epoch 110/500, Training Loss: -0.5374\n",
      "Epoch 111/500, Training Loss: -0.5625\n",
      "Epoch 112/500, Training Loss: -0.5133\n",
      "Epoch 113/500, Training Loss: -0.5354\n",
      "Epoch 114/500, Training Loss: -0.5243\n",
      "Epoch 115/500, Training Loss: -0.5382\n",
      "Epoch 116/500, Training Loss: -0.5742\n",
      "Epoch 117/500, Training Loss: -0.5752\n",
      "Epoch 118/500, Training Loss: -0.5319\n",
      "Epoch 119/500, Training Loss: -0.5741\n",
      "Epoch 120/500, Training Loss: -0.5489\n",
      "Epoch 121/500, Training Loss: -0.5809\n",
      "Epoch 122/500, Training Loss: -0.5816\n",
      "Epoch 123/500, Training Loss: -0.5344\n",
      "Epoch 124/500, Training Loss: -0.5878\n",
      "Epoch 125/500, Training Loss: -0.5928\n",
      "Epoch 126/500, Training Loss: -0.5335\n",
      "Epoch 127/500, Training Loss: -0.5528\n",
      "Epoch 128/500, Training Loss: -0.5335\n",
      "Epoch 129/500, Training Loss: -0.5313\n",
      "Epoch 130/500, Training Loss: -0.5628\n",
      "Epoch 131/500, Training Loss: -0.5285\n",
      "Epoch 132/500, Training Loss: -0.5509\n",
      "Epoch 133/500, Training Loss: -0.5542\n",
      "Epoch 134/500, Training Loss: -0.5741\n",
      "Epoch 135/500, Training Loss: -0.5520\n",
      "Epoch 136/500, Training Loss: -0.5527\n",
      "Epoch 137/500, Training Loss: -0.5717\n",
      "Epoch 138/500, Training Loss: -0.5500\n",
      "Epoch 139/500, Training Loss: -0.5328\n",
      "Epoch 140/500, Training Loss: -0.5442\n",
      "Epoch 141/500, Training Loss: -0.5473\n",
      "Epoch 142/500, Training Loss: -0.5860\n",
      "Epoch 143/500, Training Loss: -0.5830\n",
      "Epoch 144/500, Training Loss: -0.5560\n",
      "Epoch 145/500, Training Loss: -0.5957\n",
      "Epoch 146/500, Training Loss: -0.5332\n",
      "Epoch 147/500, Training Loss: -0.5157\n",
      "Epoch 148/500, Training Loss: -0.5514\n",
      "Epoch 149/500, Training Loss: -0.5283\n",
      "Epoch 150/500, Training Loss: -0.5388\n",
      "Epoch 151/500, Training Loss: -0.5814\n",
      "Epoch 152/500, Training Loss: -0.5623\n",
      "Epoch 153/500, Training Loss: -0.5172\n",
      "Epoch 154/500, Training Loss: -0.5076\n",
      "Epoch 155/500, Training Loss: -0.5619\n",
      "Epoch 156/500, Training Loss: -0.5804\n",
      "Epoch 157/500, Training Loss: -0.5678\n",
      "Epoch 158/500, Training Loss: -0.5304\n",
      "Epoch 159/500, Training Loss: -0.5524\n",
      "Epoch 160/500, Training Loss: -0.5405\n",
      "Epoch 161/500, Training Loss: -0.5432\n",
      "Epoch 162/500, Training Loss: -0.5660\n",
      "Epoch 163/500, Training Loss: -0.5620\n",
      "Epoch 164/500, Training Loss: -0.5444\n",
      "Epoch 165/500, Training Loss: -0.5541\n",
      "Epoch 166/500, Training Loss: -0.5308\n",
      "Epoch 167/500, Training Loss: -0.5338\n",
      "Epoch 168/500, Training Loss: -0.5549\n",
      "Epoch 169/500, Training Loss: -0.5432\n",
      "Epoch 170/500, Training Loss: -0.5087\n",
      "Epoch 171/500, Training Loss: -0.5329\n",
      "Epoch 172/500, Training Loss: -0.5226\n",
      "Epoch 173/500, Training Loss: -0.5333\n",
      "Epoch 174/500, Training Loss: -0.5258\n",
      "Epoch 175/500, Training Loss: -0.5798\n",
      "Epoch 176/500, Training Loss: -0.5820\n",
      "Epoch 177/500, Training Loss: -0.5440\n",
      "Epoch 178/500, Training Loss: -0.5819\n",
      "Epoch 179/500, Training Loss: -0.5554\n",
      "Epoch 180/500, Training Loss: -0.5653\n",
      "Epoch 181/500, Training Loss: -0.5270\n",
      "Epoch 182/500, Training Loss: -0.5595\n",
      "Epoch 183/500, Training Loss: -0.5203\n",
      "Epoch 184/500, Training Loss: -0.5800\n",
      "Epoch 185/500, Training Loss: -0.5721\n",
      "Epoch 186/500, Training Loss: -0.4977\n",
      "Epoch 187/500, Training Loss: -0.5423\n",
      "Epoch 188/500, Training Loss: -0.5357\n",
      "Epoch 189/500, Training Loss: -0.5457\n",
      "Epoch 190/500, Training Loss: -0.5374\n",
      "Epoch 191/500, Training Loss: -0.5558\n",
      "Epoch 192/500, Training Loss: -0.5616\n",
      "Epoch 193/500, Training Loss: -0.5556\n",
      "Epoch 194/500, Training Loss: -0.5353\n",
      "Epoch 195/500, Training Loss: -0.5340\n",
      "Epoch 196/500, Training Loss: -0.5898\n",
      "Epoch 197/500, Training Loss: -0.5579\n",
      "Epoch 198/500, Training Loss: -0.5406\n",
      "Epoch 199/500, Training Loss: -0.5789\n",
      "Epoch 200/500, Training Loss: -0.5281\n",
      "Epoch 201/500, Training Loss: -0.5574\n",
      "Epoch 202/500, Training Loss: -0.5194\n",
      "Epoch 203/500, Training Loss: -0.5109\n",
      "Epoch 204/500, Training Loss: -0.5463\n",
      "Epoch 205/500, Training Loss: -0.5875\n",
      "Epoch 206/500, Training Loss: -0.5375\n",
      "Epoch 207/500, Training Loss: -0.5619\n",
      "Epoch 208/500, Training Loss: -0.5604\n",
      "Epoch 209/500, Training Loss: -0.5635\n",
      "Epoch 210/500, Training Loss: -0.5404\n",
      "Epoch 211/500, Training Loss: -0.5474\n",
      "Epoch 212/500, Training Loss: -0.5381\n",
      "Epoch 213/500, Training Loss: -0.5563\n",
      "Epoch 214/500, Training Loss: -0.5471\n",
      "Epoch 215/500, Training Loss: -0.5321\n",
      "Epoch 216/500, Training Loss: -0.5794\n",
      "Epoch 217/500, Training Loss: -0.5479\n",
      "Epoch 218/500, Training Loss: -0.5318\n",
      "Epoch 219/500, Training Loss: -0.5402\n",
      "Epoch 220/500, Training Loss: -0.5678\n",
      "Epoch 221/500, Training Loss: -0.5254\n",
      "Epoch 222/500, Training Loss: -0.5460\n",
      "Epoch 223/500, Training Loss: -0.5307\n",
      "Epoch 224/500, Training Loss: -0.5336\n",
      "Epoch 225/500, Training Loss: -0.5559\n",
      "Epoch 226/500, Training Loss: -0.5429\n",
      "Epoch 227/500, Training Loss: -0.5715\n",
      "Epoch 228/500, Training Loss: -0.5624\n",
      "Epoch 229/500, Training Loss: -0.5449\n",
      "Epoch 230/500, Training Loss: -0.5468\n",
      "Epoch 231/500, Training Loss: -0.5593\n",
      "Epoch 232/500, Training Loss: -0.5485\n",
      "Epoch 233/500, Training Loss: -0.5704\n",
      "Epoch 234/500, Training Loss: -0.5597\n",
      "Epoch 235/500, Training Loss: -0.5302\n",
      "Epoch 236/500, Training Loss: -0.6127\n",
      "Epoch 237/500, Training Loss: -0.5450\n",
      "Epoch 238/500, Training Loss: -0.5289\n",
      "Epoch 239/500, Training Loss: -0.5565\n",
      "Epoch 240/500, Training Loss: -0.5215\n",
      "Epoch 241/500, Training Loss: -0.5893\n",
      "Epoch 242/500, Training Loss: -0.5819\n",
      "Epoch 243/500, Training Loss: -0.5450\n",
      "Epoch 244/500, Training Loss: -0.5591\n",
      "Epoch 245/500, Training Loss: -0.5629\n",
      "Epoch 246/500, Training Loss: -0.5403\n",
      "Epoch 247/500, Training Loss: -0.5748\n",
      "Epoch 248/500, Training Loss: -0.5819\n",
      "Epoch 249/500, Training Loss: -0.5577\n",
      "Epoch 250/500, Training Loss: -0.5864\n",
      "Epoch 251/500, Training Loss: -0.5589\n",
      "Epoch 252/500, Training Loss: -0.5531\n",
      "Epoch 253/500, Training Loss: -0.5407\n",
      "Epoch 254/500, Training Loss: -0.5082\n",
      "Epoch 255/500, Training Loss: -0.5656\n",
      "Epoch 256/500, Training Loss: -0.5797\n",
      "Epoch 257/500, Training Loss: -0.5622\n",
      "Epoch 258/500, Training Loss: -0.5431\n",
      "Epoch 259/500, Training Loss: -0.5658\n",
      "Epoch 260/500, Training Loss: -0.5219\n",
      "Epoch 261/500, Training Loss: -0.5404\n",
      "Epoch 262/500, Training Loss: -0.5142\n",
      "Epoch 263/500, Training Loss: -0.5590\n",
      "Epoch 264/500, Training Loss: -0.5500\n",
      "Epoch 265/500, Training Loss: -0.5511\n",
      "Epoch 266/500, Training Loss: -0.5360\n",
      "Epoch 267/500, Training Loss: -0.5653\n",
      "Epoch 268/500, Training Loss: -0.5336\n",
      "Epoch 269/500, Training Loss: -0.5531\n",
      "Epoch 270/500, Training Loss: -0.5431\n",
      "Epoch 271/500, Training Loss: -0.5627\n",
      "Epoch 272/500, Training Loss: -0.5851\n",
      "Epoch 273/500, Training Loss: -0.5395\n",
      "Epoch 274/500, Training Loss: -0.5841\n",
      "Epoch 275/500, Training Loss: -0.5628\n",
      "Epoch 276/500, Training Loss: -0.5131\n",
      "Epoch 277/500, Training Loss: -0.5866\n",
      "Epoch 278/500, Training Loss: -0.5502\n",
      "Epoch 279/500, Training Loss: -0.5369\n",
      "Epoch 280/500, Training Loss: -0.5312\n",
      "Epoch 281/500, Training Loss: -0.5830\n",
      "Epoch 282/500, Training Loss: -0.5389\n",
      "Epoch 283/500, Training Loss: -0.5314\n",
      "Epoch 284/500, Training Loss: -0.5429\n",
      "Epoch 285/500, Training Loss: -0.5449\n",
      "Epoch 286/500, Training Loss: -0.5621\n",
      "Epoch 287/500, Training Loss: -0.5666\n",
      "Epoch 288/500, Training Loss: -0.5200\n",
      "Epoch 289/500, Training Loss: -0.5715\n",
      "Epoch 290/500, Training Loss: -0.5647\n",
      "Epoch 291/500, Training Loss: -0.5713\n",
      "Epoch 292/500, Training Loss: -0.5927\n",
      "Epoch 293/500, Training Loss: -0.5657\n",
      "Epoch 294/500, Training Loss: -0.5806\n",
      "Epoch 295/500, Training Loss: -0.5277\n",
      "Epoch 296/500, Training Loss: -0.5603\n",
      "Epoch 297/500, Training Loss: -0.5210\n",
      "Epoch 298/500, Training Loss: -0.5273\n",
      "Epoch 299/500, Training Loss: -0.5591\n",
      "Epoch 300/500, Training Loss: -0.5895\n",
      "Epoch 301/500, Training Loss: -0.5760\n",
      "Epoch 302/500, Training Loss: -0.5557\n",
      "Epoch 303/500, Training Loss: -0.5496\n",
      "Epoch 304/500, Training Loss: -0.5431\n",
      "Epoch 305/500, Training Loss: -0.5569\n",
      "Epoch 306/500, Training Loss: -0.5460\n",
      "Epoch 307/500, Training Loss: -0.5352\n",
      "Epoch 308/500, Training Loss: -0.5304\n",
      "Epoch 309/500, Training Loss: -0.5442\n",
      "Epoch 310/500, Training Loss: -0.5481\n",
      "Epoch 311/500, Training Loss: -0.5540\n",
      "Epoch 312/500, Training Loss: -0.5589\n",
      "Epoch 313/500, Training Loss: -0.5562\n",
      "Epoch 314/500, Training Loss: -0.5710\n",
      "Epoch 315/500, Training Loss: -0.5515\n",
      "Epoch 316/500, Training Loss: -0.5553\n",
      "Epoch 317/500, Training Loss: -0.5699\n",
      "Epoch 318/500, Training Loss: -0.5709\n",
      "Epoch 319/500, Training Loss: -0.5648\n",
      "Epoch 320/500, Training Loss: -0.5257\n",
      "Epoch 321/500, Training Loss: -0.5460\n",
      "Epoch 322/500, Training Loss: -0.5328\n",
      "Epoch 323/500, Training Loss: -0.5810\n",
      "Epoch 324/500, Training Loss: -0.5314\n",
      "Epoch 325/500, Training Loss: -0.5646\n",
      "Epoch 326/500, Training Loss: -0.5601\n",
      "Epoch 327/500, Training Loss: -0.5483\n",
      "Epoch 328/500, Training Loss: -0.5490\n",
      "Epoch 329/500, Training Loss: -0.5515\n",
      "Epoch 330/500, Training Loss: -0.5184\n",
      "Epoch 331/500, Training Loss: -0.5510\n",
      "Epoch 332/500, Training Loss: -0.5477\n",
      "Epoch 333/500, Training Loss: -0.5214\n",
      "Epoch 334/500, Training Loss: -0.5285\n",
      "Epoch 335/500, Training Loss: -0.5599\n",
      "Epoch 336/500, Training Loss: -0.5527\n",
      "Epoch 337/500, Training Loss: -0.5885\n",
      "Epoch 338/500, Training Loss: -0.5497\n",
      "Epoch 339/500, Training Loss: -0.5612\n",
      "Epoch 340/500, Training Loss: -0.5358\n",
      "Epoch 341/500, Training Loss: -0.6056\n",
      "Epoch 342/500, Training Loss: -0.5267\n",
      "Epoch 343/500, Training Loss: -0.5823\n",
      "Epoch 344/500, Training Loss: -0.5213\n",
      "Epoch 345/500, Training Loss: -0.5402\n",
      "Epoch 346/500, Training Loss: -0.5978\n",
      "Epoch 347/500, Training Loss: -0.5869\n",
      "Epoch 348/500, Training Loss: -0.5927\n",
      "Epoch 349/500, Training Loss: -0.5602\n",
      "Epoch 350/500, Training Loss: -0.5307\n",
      "Epoch 351/500, Training Loss: -0.5362\n",
      "Epoch 352/500, Training Loss: -0.5366\n",
      "Epoch 353/500, Training Loss: -0.5327\n",
      "Epoch 354/500, Training Loss: -0.5862\n",
      "Epoch 355/500, Training Loss: -0.5652\n",
      "Epoch 356/500, Training Loss: -0.5437\n",
      "Epoch 357/500, Training Loss: -0.5344\n",
      "Epoch 358/500, Training Loss: -0.5693\n",
      "Epoch 359/500, Training Loss: -0.5292\n",
      "Epoch 360/500, Training Loss: -0.5556\n",
      "Epoch 361/500, Training Loss: -0.5438\n",
      "Epoch 362/500, Training Loss: -0.5741\n",
      "Epoch 363/500, Training Loss: -0.5387\n",
      "Epoch 364/500, Training Loss: -0.5821\n",
      "Epoch 365/500, Training Loss: -0.5520\n",
      "Epoch 366/500, Training Loss: -0.5312\n",
      "Epoch 367/500, Training Loss: -0.5539\n",
      "Epoch 368/500, Training Loss: -0.5310\n",
      "Epoch 369/500, Training Loss: -0.5264\n",
      "Epoch 370/500, Training Loss: -0.5569\n",
      "Epoch 371/500, Training Loss: -0.5757\n",
      "Epoch 372/500, Training Loss: -0.5647\n",
      "Epoch 373/500, Training Loss: -0.5251\n",
      "Epoch 374/500, Training Loss: -0.5725\n",
      "Epoch 375/500, Training Loss: -0.5121\n",
      "Epoch 376/500, Training Loss: -0.5503\n",
      "Epoch 377/500, Training Loss: -0.5073\n",
      "Epoch 378/500, Training Loss: -0.5569\n",
      "Epoch 379/500, Training Loss: -0.5586\n",
      "Epoch 380/500, Training Loss: -0.5401\n",
      "Epoch 381/500, Training Loss: -0.5606\n",
      "Epoch 382/500, Training Loss: -0.5199\n",
      "Epoch 383/500, Training Loss: -0.5259\n",
      "Epoch 384/500, Training Loss: -0.4925\n",
      "Epoch 385/500, Training Loss: -0.5615\n",
      "Epoch 386/500, Training Loss: -0.5409\n",
      "Epoch 387/500, Training Loss: -0.5731\n",
      "Epoch 388/500, Training Loss: -0.5604\n",
      "Epoch 389/500, Training Loss: -0.5387\n",
      "Epoch 390/500, Training Loss: -0.5900\n",
      "Epoch 391/500, Training Loss: -0.5141\n",
      "Epoch 392/500, Training Loss: -0.5656\n",
      "Epoch 393/500, Training Loss: -0.5784\n",
      "Epoch 394/500, Training Loss: -0.5480\n",
      "Epoch 395/500, Training Loss: -0.5776\n",
      "Epoch 396/500, Training Loss: -0.6008\n",
      "Epoch 397/500, Training Loss: -0.5364\n",
      "Epoch 398/500, Training Loss: -0.5799\n",
      "Epoch 399/500, Training Loss: -0.5467\n",
      "Epoch 400/500, Training Loss: -0.5730\n",
      "Epoch 401/500, Training Loss: -0.5640\n",
      "Epoch 402/500, Training Loss: -0.5337\n",
      "Epoch 403/500, Training Loss: -0.5589\n",
      "Epoch 404/500, Training Loss: -0.5825\n",
      "Epoch 405/500, Training Loss: -0.5534\n",
      "Epoch 406/500, Training Loss: -0.5373\n",
      "Epoch 407/500, Training Loss: -0.5794\n",
      "Epoch 408/500, Training Loss: -0.5335\n",
      "Epoch 409/500, Training Loss: -0.5738\n",
      "Epoch 410/500, Training Loss: -0.5351\n",
      "Epoch 411/500, Training Loss: -0.5531\n",
      "Epoch 412/500, Training Loss: -0.5406\n",
      "Epoch 413/500, Training Loss: -0.5387\n",
      "Epoch 414/500, Training Loss: -0.5779\n",
      "Epoch 415/500, Training Loss: -0.5424\n",
      "Epoch 416/500, Training Loss: -0.5886\n",
      "Epoch 417/500, Training Loss: -0.5393\n",
      "Epoch 418/500, Training Loss: -0.5302\n",
      "Epoch 419/500, Training Loss: -0.5464\n",
      "Epoch 420/500, Training Loss: -0.5753\n",
      "Epoch 421/500, Training Loss: -0.5655\n",
      "Epoch 422/500, Training Loss: -0.5474\n",
      "Epoch 423/500, Training Loss: -0.5600\n",
      "Epoch 424/500, Training Loss: -0.5615\n",
      "Epoch 425/500, Training Loss: -0.5555\n",
      "Epoch 426/500, Training Loss: -0.5643\n",
      "Epoch 427/500, Training Loss: -0.5485\n",
      "Epoch 428/500, Training Loss: -0.5258\n",
      "Epoch 429/500, Training Loss: -0.5409\n",
      "Epoch 430/500, Training Loss: -0.5532\n",
      "Epoch 431/500, Training Loss: -0.5335\n",
      "Epoch 432/500, Training Loss: -0.5603\n",
      "Epoch 433/500, Training Loss: -0.5905\n",
      "Epoch 434/500, Training Loss: -0.5522\n",
      "Epoch 435/500, Training Loss: -0.5578\n",
      "Epoch 436/500, Training Loss: -0.5413\n",
      "Epoch 437/500, Training Loss: -0.5533\n",
      "Epoch 438/500, Training Loss: -0.5634\n",
      "Epoch 439/500, Training Loss: -0.5642\n",
      "Epoch 440/500, Training Loss: -0.5138\n",
      "Epoch 441/500, Training Loss: -0.5466\n",
      "Epoch 442/500, Training Loss: -0.5244\n",
      "Epoch 443/500, Training Loss: -0.6011\n",
      "Epoch 444/500, Training Loss: -0.5279\n",
      "Epoch 445/500, Training Loss: -0.5346\n",
      "Epoch 446/500, Training Loss: -0.5553\n",
      "Epoch 447/500, Training Loss: -0.5795\n",
      "Epoch 448/500, Training Loss: -0.5767\n",
      "Epoch 449/500, Training Loss: -0.5517\n",
      "Epoch 450/500, Training Loss: -0.5512\n",
      "Epoch 451/500, Training Loss: -0.5796\n",
      "Epoch 452/500, Training Loss: -0.5879\n",
      "Epoch 453/500, Training Loss: -0.5528\n",
      "Epoch 454/500, Training Loss: -0.5257\n",
      "Epoch 455/500, Training Loss: -0.5683\n",
      "Epoch 456/500, Training Loss: -0.5512\n",
      "Epoch 457/500, Training Loss: -0.5293\n",
      "Epoch 458/500, Training Loss: -0.5146\n",
      "Epoch 459/500, Training Loss: -0.5390\n",
      "Epoch 460/500, Training Loss: -0.5625\n",
      "Epoch 461/500, Training Loss: -0.5754\n",
      "Epoch 462/500, Training Loss: -0.5306\n",
      "Epoch 463/500, Training Loss: -0.5467\n",
      "Epoch 464/500, Training Loss: -0.5736\n",
      "Epoch 465/500, Training Loss: -0.5498\n",
      "Epoch 466/500, Training Loss: -0.5562\n",
      "Epoch 467/500, Training Loss: -0.5321\n",
      "Epoch 468/500, Training Loss: -0.5781\n",
      "Epoch 469/500, Training Loss: -0.5114\n",
      "Epoch 470/500, Training Loss: -0.5524\n",
      "Epoch 471/500, Training Loss: -0.5558\n",
      "Epoch 472/500, Training Loss: -0.5180\n",
      "Epoch 473/500, Training Loss: -0.5526\n",
      "Epoch 474/500, Training Loss: -0.5590\n",
      "Epoch 475/500, Training Loss: -0.5697\n",
      "Epoch 476/500, Training Loss: -0.5298\n",
      "Epoch 477/500, Training Loss: -0.5572\n",
      "Epoch 478/500, Training Loss: -0.5448\n",
      "Epoch 479/500, Training Loss: -0.5556\n",
      "Epoch 480/500, Training Loss: -0.5423\n",
      "Epoch 481/500, Training Loss: -0.5531\n",
      "Epoch 482/500, Training Loss: -0.5621\n",
      "Epoch 483/500, Training Loss: -0.5599\n",
      "Epoch 484/500, Training Loss: -0.5548\n",
      "Epoch 485/500, Training Loss: -0.5953\n",
      "Epoch 486/500, Training Loss: -0.5860\n",
      "Epoch 487/500, Training Loss: -0.5403\n",
      "Epoch 488/500, Training Loss: -0.5527\n",
      "Epoch 489/500, Training Loss: -0.5724\n",
      "Epoch 490/500, Training Loss: -0.5795\n",
      "Epoch 491/500, Training Loss: -0.5410\n",
      "Epoch 492/500, Training Loss: -0.5495\n",
      "Epoch 493/500, Training Loss: -0.5042\n",
      "Epoch 494/500, Training Loss: -0.5216\n",
      "Epoch 495/500, Training Loss: -0.5416\n",
      "Epoch 496/500, Training Loss: -0.5529\n",
      "Epoch 497/500, Training Loss: -0.5460\n",
      "Epoch 498/500, Training Loss: -0.5758\n",
      "Epoch 499/500, Training Loss: -0.5541\n",
      "Epoch 500/500, Training Loss: -0.5535\n",
      "Validation Loss: -0.5502\n",
      "Model 2, Fold 1: Validation Loss = -0.5502\n",
      "Epoch 1/500, Training Loss: -0.4195\n",
      "Epoch 2/500, Training Loss: -0.5447\n",
      "Epoch 3/500, Training Loss: -0.4990\n",
      "Epoch 4/500, Training Loss: -0.5108\n",
      "Epoch 5/500, Training Loss: -0.5382\n",
      "Epoch 6/500, Training Loss: -0.4841\n",
      "Epoch 7/500, Training Loss: -0.5487\n",
      "Epoch 8/500, Training Loss: -0.5606\n",
      "Epoch 9/500, Training Loss: -0.5590\n",
      "Epoch 10/500, Training Loss: -0.5391\n",
      "Epoch 11/500, Training Loss: -0.5644\n",
      "Epoch 12/500, Training Loss: -0.5396\n",
      "Epoch 13/500, Training Loss: -0.5372\n",
      "Epoch 14/500, Training Loss: -0.4844\n",
      "Epoch 15/500, Training Loss: -0.5643\n",
      "Epoch 16/500, Training Loss: -0.5320\n",
      "Epoch 17/500, Training Loss: -0.5679\n",
      "Epoch 18/500, Training Loss: -0.5141\n",
      "Epoch 19/500, Training Loss: -0.6024\n",
      "Epoch 20/500, Training Loss: -0.5559\n",
      "Epoch 21/500, Training Loss: -0.4845\n",
      "Epoch 22/500, Training Loss: -0.4941\n",
      "Epoch 23/500, Training Loss: -0.5584\n",
      "Epoch 24/500, Training Loss: -0.5579\n",
      "Epoch 25/500, Training Loss: -0.5831\n",
      "Epoch 26/500, Training Loss: -0.5834\n",
      "Epoch 27/500, Training Loss: -0.5596\n",
      "Epoch 28/500, Training Loss: -0.6226\n",
      "Epoch 29/500, Training Loss: -0.5520\n",
      "Epoch 30/500, Training Loss: -0.5633\n",
      "Epoch 31/500, Training Loss: -0.6084\n",
      "Epoch 32/500, Training Loss: -0.5987\n",
      "Epoch 33/500, Training Loss: -0.5003\n",
      "Epoch 34/500, Training Loss: -0.5473\n",
      "Epoch 35/500, Training Loss: -0.5290\n",
      "Epoch 36/500, Training Loss: -0.4835\n",
      "Epoch 37/500, Training Loss: -0.5657\n",
      "Epoch 38/500, Training Loss: -0.5346\n",
      "Epoch 39/500, Training Loss: -0.5333\n",
      "Epoch 40/500, Training Loss: -0.4566\n",
      "Epoch 41/500, Training Loss: -0.5116\n",
      "Epoch 42/500, Training Loss: -0.5376\n",
      "Epoch 43/500, Training Loss: -0.6270\n",
      "Epoch 44/500, Training Loss: -0.5225\n",
      "Epoch 45/500, Training Loss: -0.5607\n",
      "Epoch 46/500, Training Loss: -0.5387\n",
      "Epoch 47/500, Training Loss: -0.5197\n",
      "Epoch 48/500, Training Loss: -0.4752\n",
      "Epoch 49/500, Training Loss: -0.6105\n",
      "Epoch 50/500, Training Loss: -0.4940\n",
      "Epoch 51/500, Training Loss: -0.5486\n",
      "Epoch 52/500, Training Loss: -0.4831\n",
      "Epoch 53/500, Training Loss: -0.5319\n",
      "Epoch 54/500, Training Loss: -0.5270\n",
      "Epoch 55/500, Training Loss: -0.5679\n",
      "Epoch 56/500, Training Loss: -0.5945\n",
      "Epoch 57/500, Training Loss: -0.6416\n",
      "Epoch 58/500, Training Loss: -0.5652\n",
      "Epoch 59/500, Training Loss: -0.5657\n",
      "Epoch 60/500, Training Loss: -0.5772\n",
      "Epoch 61/500, Training Loss: -0.5734\n",
      "Epoch 62/500, Training Loss: -0.5175\n",
      "Epoch 63/500, Training Loss: -0.5534\n",
      "Epoch 64/500, Training Loss: -0.5908\n",
      "Epoch 65/500, Training Loss: -0.5941\n",
      "Epoch 66/500, Training Loss: -0.6314\n",
      "Epoch 67/500, Training Loss: -0.5595\n",
      "Epoch 68/500, Training Loss: -0.5491\n",
      "Epoch 69/500, Training Loss: -0.5511\n",
      "Epoch 70/500, Training Loss: -0.5166\n",
      "Epoch 71/500, Training Loss: -0.5572\n",
      "Epoch 72/500, Training Loss: -0.4970\n",
      "Epoch 73/500, Training Loss: -0.6214\n",
      "Epoch 74/500, Training Loss: -0.5248\n",
      "Epoch 75/500, Training Loss: -0.5783\n",
      "Epoch 76/500, Training Loss: -0.5345\n",
      "Epoch 77/500, Training Loss: -0.6174\n",
      "Epoch 78/500, Training Loss: -0.5378\n",
      "Epoch 79/500, Training Loss: -0.5931\n",
      "Epoch 80/500, Training Loss: -0.5697\n",
      "Epoch 81/500, Training Loss: -0.5680\n",
      "Epoch 82/500, Training Loss: -0.5603\n",
      "Epoch 83/500, Training Loss: -0.5138\n",
      "Epoch 84/500, Training Loss: -0.5519\n",
      "Epoch 85/500, Training Loss: -0.5937\n",
      "Epoch 86/500, Training Loss: -0.6021\n",
      "Epoch 87/500, Training Loss: -0.5449\n",
      "Epoch 88/500, Training Loss: -0.5542\n",
      "Epoch 89/500, Training Loss: -0.4857\n",
      "Epoch 90/500, Training Loss: -0.5278\n",
      "Epoch 91/500, Training Loss: -0.5632\n",
      "Epoch 92/500, Training Loss: -0.6538\n",
      "Epoch 93/500, Training Loss: -0.5797\n",
      "Epoch 94/500, Training Loss: -0.5395\n",
      "Epoch 95/500, Training Loss: -0.5403\n",
      "Epoch 96/500, Training Loss: -0.5211\n",
      "Epoch 97/500, Training Loss: -0.5651\n",
      "Epoch 98/500, Training Loss: -0.5772\n",
      "Epoch 99/500, Training Loss: -0.5503\n",
      "Epoch 100/500, Training Loss: -0.5185\n",
      "Epoch 101/500, Training Loss: -0.5540\n",
      "Epoch 102/500, Training Loss: -0.5648\n",
      "Epoch 103/500, Training Loss: -0.5856\n",
      "Epoch 104/500, Training Loss: -0.4998\n",
      "Epoch 105/500, Training Loss: -0.5843\n",
      "Epoch 106/500, Training Loss: -0.5551\n",
      "Epoch 107/500, Training Loss: -0.5735\n",
      "Epoch 108/500, Training Loss: -0.6258\n",
      "Epoch 109/500, Training Loss: -0.5832\n",
      "Epoch 110/500, Training Loss: -0.5914\n",
      "Epoch 111/500, Training Loss: -0.5611\n",
      "Epoch 112/500, Training Loss: -0.5867\n",
      "Epoch 113/500, Training Loss: -0.5371\n",
      "Epoch 114/500, Training Loss: -0.5946\n",
      "Epoch 115/500, Training Loss: -0.6149\n",
      "Epoch 116/500, Training Loss: -0.5407\n",
      "Epoch 117/500, Training Loss: -0.5883\n",
      "Epoch 118/500, Training Loss: -0.6515\n",
      "Epoch 119/500, Training Loss: -0.6165\n",
      "Epoch 120/500, Training Loss: -0.5400\n",
      "Epoch 121/500, Training Loss: -0.5525\n",
      "Epoch 122/500, Training Loss: -0.5418\n",
      "Epoch 123/500, Training Loss: -0.5275\n",
      "Epoch 124/500, Training Loss: -0.5740\n",
      "Epoch 125/500, Training Loss: -0.5743\n",
      "Epoch 126/500, Training Loss: -0.5757\n",
      "Epoch 127/500, Training Loss: -0.6200\n",
      "Epoch 128/500, Training Loss: -0.5529\n",
      "Epoch 129/500, Training Loss: -0.5034\n",
      "Epoch 130/500, Training Loss: -0.6060\n",
      "Epoch 131/500, Training Loss: -0.6090\n",
      "Epoch 132/500, Training Loss: -0.5878\n",
      "Epoch 133/500, Training Loss: -0.5902\n",
      "Epoch 134/500, Training Loss: -0.5263\n",
      "Epoch 135/500, Training Loss: -0.5366\n",
      "Epoch 136/500, Training Loss: -0.6236\n",
      "Epoch 137/500, Training Loss: -0.5417\n",
      "Epoch 138/500, Training Loss: -0.5232\n",
      "Epoch 139/500, Training Loss: -0.5456\n",
      "Epoch 140/500, Training Loss: -0.5870\n",
      "Epoch 141/500, Training Loss: -0.6086\n",
      "Epoch 142/500, Training Loss: -0.4948\n",
      "Epoch 143/500, Training Loss: -0.6012\n",
      "Epoch 144/500, Training Loss: -0.5170\n",
      "Epoch 145/500, Training Loss: -0.5509\n",
      "Epoch 146/500, Training Loss: -0.5050\n",
      "Epoch 147/500, Training Loss: -0.5806\n",
      "Epoch 148/500, Training Loss: -0.5770\n",
      "Epoch 149/500, Training Loss: -0.5132\n",
      "Epoch 150/500, Training Loss: -0.5536\n",
      "Epoch 151/500, Training Loss: -0.5611\n",
      "Epoch 152/500, Training Loss: -0.4939\n",
      "Epoch 153/500, Training Loss: -0.4822\n",
      "Epoch 154/500, Training Loss: -0.5343\n",
      "Epoch 155/500, Training Loss: -0.6152\n",
      "Epoch 156/500, Training Loss: -0.6253\n",
      "Epoch 157/500, Training Loss: -0.5679\n",
      "Epoch 158/500, Training Loss: -0.5856\n",
      "Epoch 159/500, Training Loss: -0.5895\n",
      "Epoch 160/500, Training Loss: -0.6033\n",
      "Epoch 161/500, Training Loss: -0.6127\n",
      "Epoch 162/500, Training Loss: -0.6008\n",
      "Epoch 163/500, Training Loss: -0.5393\n",
      "Epoch 164/500, Training Loss: -0.6302\n",
      "Epoch 165/500, Training Loss: -0.5365\n",
      "Epoch 166/500, Training Loss: -0.5249\n",
      "Epoch 167/500, Training Loss: -0.6218\n",
      "Epoch 168/500, Training Loss: -0.4897\n",
      "Epoch 169/500, Training Loss: -0.5203\n",
      "Epoch 170/500, Training Loss: -0.5407\n",
      "Epoch 171/500, Training Loss: -0.5892\n",
      "Epoch 172/500, Training Loss: -0.5324\n",
      "Epoch 173/500, Training Loss: -0.5024\n",
      "Epoch 174/500, Training Loss: -0.5738\n",
      "Epoch 175/500, Training Loss: -0.5177\n",
      "Epoch 176/500, Training Loss: -0.5854\n",
      "Epoch 177/500, Training Loss: -0.6355\n",
      "Epoch 178/500, Training Loss: -0.5758\n",
      "Epoch 179/500, Training Loss: -0.5174\n",
      "Epoch 180/500, Training Loss: -0.5937\n",
      "Epoch 181/500, Training Loss: -0.5654\n",
      "Epoch 182/500, Training Loss: -0.5598\n",
      "Epoch 183/500, Training Loss: -0.5808\n",
      "Epoch 184/500, Training Loss: -0.5944\n",
      "Epoch 185/500, Training Loss: -0.5675\n",
      "Epoch 186/500, Training Loss: -0.5767\n",
      "Epoch 187/500, Training Loss: -0.5382\n",
      "Epoch 188/500, Training Loss: -0.5615\n",
      "Epoch 189/500, Training Loss: -0.5706\n",
      "Epoch 190/500, Training Loss: -0.5670\n",
      "Epoch 191/500, Training Loss: -0.5526\n",
      "Epoch 192/500, Training Loss: -0.5383\n",
      "Epoch 193/500, Training Loss: -0.5493\n",
      "Epoch 194/500, Training Loss: -0.5559\n",
      "Epoch 195/500, Training Loss: -0.5819\n",
      "Epoch 196/500, Training Loss: -0.5552\n",
      "Epoch 197/500, Training Loss: -0.5022\n",
      "Epoch 198/500, Training Loss: -0.5533\n",
      "Epoch 199/500, Training Loss: -0.5182\n",
      "Epoch 200/500, Training Loss: -0.6097\n",
      "Epoch 201/500, Training Loss: -0.6434\n",
      "Epoch 202/500, Training Loss: -0.6206\n",
      "Epoch 203/500, Training Loss: -0.6075\n",
      "Epoch 204/500, Training Loss: -0.5132\n",
      "Epoch 205/500, Training Loss: -0.6054\n",
      "Epoch 206/500, Training Loss: -0.5652\n",
      "Epoch 207/500, Training Loss: -0.4832\n",
      "Epoch 208/500, Training Loss: -0.5714\n",
      "Epoch 209/500, Training Loss: -0.6297\n",
      "Epoch 210/500, Training Loss: -0.5754\n",
      "Epoch 211/500, Training Loss: -0.5348\n",
      "Epoch 212/500, Training Loss: -0.5213\n",
      "Epoch 213/500, Training Loss: -0.5158\n",
      "Epoch 214/500, Training Loss: -0.5079\n",
      "Epoch 215/500, Training Loss: -0.5640\n",
      "Epoch 216/500, Training Loss: -0.5691\n",
      "Epoch 217/500, Training Loss: -0.5572\n",
      "Epoch 218/500, Training Loss: -0.5920\n",
      "Epoch 219/500, Training Loss: -0.5605\n",
      "Epoch 220/500, Training Loss: -0.5666\n",
      "Epoch 221/500, Training Loss: -0.5825\n",
      "Epoch 222/500, Training Loss: -0.6086\n",
      "Epoch 223/500, Training Loss: -0.6280\n",
      "Epoch 224/500, Training Loss: -0.6097\n",
      "Epoch 225/500, Training Loss: -0.5782\n",
      "Epoch 226/500, Training Loss: -0.5171\n",
      "Epoch 227/500, Training Loss: -0.5669\n",
      "Epoch 228/500, Training Loss: -0.5172\n",
      "Epoch 229/500, Training Loss: -0.5272\n",
      "Epoch 230/500, Training Loss: -0.4998\n",
      "Epoch 231/500, Training Loss: -0.5395\n",
      "Epoch 232/500, Training Loss: -0.5900\n",
      "Epoch 233/500, Training Loss: -0.6560\n",
      "Epoch 234/500, Training Loss: -0.5889\n",
      "Epoch 235/500, Training Loss: -0.5938\n",
      "Epoch 236/500, Training Loss: -0.5933\n",
      "Epoch 237/500, Training Loss: -0.5816\n",
      "Epoch 238/500, Training Loss: -0.5508\n",
      "Epoch 239/500, Training Loss: -0.5752\n",
      "Epoch 240/500, Training Loss: -0.6167\n",
      "Epoch 241/500, Training Loss: -0.6015\n",
      "Epoch 242/500, Training Loss: -0.5480\n",
      "Epoch 243/500, Training Loss: -0.5721\n",
      "Epoch 244/500, Training Loss: -0.5327\n",
      "Epoch 245/500, Training Loss: -0.5819\n",
      "Epoch 246/500, Training Loss: -0.5703\n",
      "Epoch 247/500, Training Loss: -0.6029\n",
      "Epoch 248/500, Training Loss: -0.6079\n",
      "Epoch 249/500, Training Loss: -0.5638\n",
      "Epoch 250/500, Training Loss: -0.4999\n",
      "Epoch 251/500, Training Loss: -0.6247\n",
      "Epoch 252/500, Training Loss: -0.5844\n",
      "Epoch 253/500, Training Loss: -0.5242\n",
      "Epoch 254/500, Training Loss: -0.5896\n",
      "Epoch 255/500, Training Loss: -0.5909\n",
      "Epoch 256/500, Training Loss: -0.5822\n",
      "Epoch 257/500, Training Loss: -0.5662\n",
      "Epoch 258/500, Training Loss: -0.5647\n",
      "Epoch 259/500, Training Loss: -0.5856\n",
      "Epoch 260/500, Training Loss: -0.5490\n",
      "Epoch 261/500, Training Loss: -0.5686\n",
      "Epoch 262/500, Training Loss: -0.5777\n",
      "Epoch 263/500, Training Loss: -0.5945\n",
      "Epoch 264/500, Training Loss: -0.6051\n",
      "Epoch 265/500, Training Loss: -0.5624\n",
      "Epoch 266/500, Training Loss: -0.5727\n",
      "Epoch 267/500, Training Loss: -0.5994\n",
      "Epoch 268/500, Training Loss: -0.5846\n",
      "Epoch 269/500, Training Loss: -0.5534\n",
      "Epoch 270/500, Training Loss: -0.5544\n",
      "Epoch 271/500, Training Loss: -0.6123\n",
      "Epoch 272/500, Training Loss: -0.5067\n",
      "Epoch 273/500, Training Loss: -0.5532\n",
      "Epoch 274/500, Training Loss: -0.5400\n",
      "Epoch 275/500, Training Loss: -0.5201\n",
      "Epoch 276/500, Training Loss: -0.6187\n",
      "Epoch 277/500, Training Loss: -0.5730\n",
      "Epoch 278/500, Training Loss: -0.6056\n",
      "Epoch 279/500, Training Loss: -0.4976\n",
      "Epoch 280/500, Training Loss: -0.5606\n",
      "Epoch 281/500, Training Loss: -0.5594\n",
      "Epoch 282/500, Training Loss: -0.5768\n",
      "Epoch 283/500, Training Loss: -0.5676\n",
      "Epoch 284/500, Training Loss: -0.5314\n",
      "Epoch 285/500, Training Loss: -0.5388\n",
      "Epoch 286/500, Training Loss: -0.5717\n",
      "Epoch 287/500, Training Loss: -0.6079\n",
      "Epoch 288/500, Training Loss: -0.5461\n",
      "Epoch 289/500, Training Loss: -0.6015\n",
      "Epoch 290/500, Training Loss: -0.5586\n",
      "Epoch 291/500, Training Loss: -0.5645\n",
      "Epoch 292/500, Training Loss: -0.5769\n",
      "Epoch 293/500, Training Loss: -0.5700\n",
      "Epoch 294/500, Training Loss: -0.5320\n",
      "Epoch 295/500, Training Loss: -0.5701\n",
      "Epoch 296/500, Training Loss: -0.5794\n",
      "Epoch 297/500, Training Loss: -0.5379\n",
      "Epoch 298/500, Training Loss: -0.6095\n",
      "Epoch 299/500, Training Loss: -0.6273\n",
      "Epoch 300/500, Training Loss: -0.5600\n",
      "Epoch 301/500, Training Loss: -0.5763\n",
      "Epoch 302/500, Training Loss: -0.5303\n",
      "Epoch 303/500, Training Loss: -0.5504\n",
      "Epoch 304/500, Training Loss: -0.5829\n",
      "Epoch 305/500, Training Loss: -0.5450\n",
      "Epoch 306/500, Training Loss: -0.6092\n",
      "Epoch 307/500, Training Loss: -0.6242\n",
      "Epoch 308/500, Training Loss: -0.5695\n",
      "Epoch 309/500, Training Loss: -0.5674\n",
      "Epoch 310/500, Training Loss: -0.5462\n",
      "Epoch 311/500, Training Loss: -0.5164\n",
      "Epoch 312/500, Training Loss: -0.5496\n",
      "Epoch 313/500, Training Loss: -0.5727\n",
      "Epoch 314/500, Training Loss: -0.5283\n",
      "Epoch 315/500, Training Loss: -0.5510\n",
      "Epoch 316/500, Training Loss: -0.5136\n",
      "Epoch 317/500, Training Loss: -0.5686\n",
      "Epoch 318/500, Training Loss: -0.5652\n",
      "Epoch 319/500, Training Loss: -0.5835\n",
      "Epoch 320/500, Training Loss: -0.5934\n",
      "Epoch 321/500, Training Loss: -0.5000\n",
      "Epoch 322/500, Training Loss: -0.5957\n",
      "Epoch 323/500, Training Loss: -0.5143\n",
      "Epoch 324/500, Training Loss: -0.6397\n",
      "Epoch 325/500, Training Loss: -0.5303\n",
      "Epoch 326/500, Training Loss: -0.6083\n",
      "Epoch 327/500, Training Loss: -0.5741\n",
      "Epoch 328/500, Training Loss: -0.5621\n",
      "Epoch 329/500, Training Loss: -0.6107\n",
      "Epoch 330/500, Training Loss: -0.5962\n",
      "Epoch 331/500, Training Loss: -0.5763\n",
      "Epoch 332/500, Training Loss: -0.5904\n",
      "Epoch 333/500, Training Loss: -0.5926\n",
      "Epoch 334/500, Training Loss: -0.5320\n",
      "Epoch 335/500, Training Loss: -0.5757\n",
      "Epoch 336/500, Training Loss: -0.5922\n",
      "Epoch 337/500, Training Loss: -0.5819\n",
      "Epoch 338/500, Training Loss: -0.5957\n",
      "Epoch 339/500, Training Loss: -0.5249\n",
      "Epoch 340/500, Training Loss: -0.5876\n",
      "Epoch 341/500, Training Loss: -0.5822\n",
      "Epoch 342/500, Training Loss: -0.5408\n",
      "Epoch 343/500, Training Loss: -0.5458\n",
      "Epoch 344/500, Training Loss: -0.5605\n",
      "Epoch 345/500, Training Loss: -0.5188\n",
      "Epoch 346/500, Training Loss: -0.5566\n",
      "Epoch 347/500, Training Loss: -0.5563\n",
      "Epoch 348/500, Training Loss: -0.5501\n",
      "Epoch 349/500, Training Loss: -0.5426\n",
      "Epoch 350/500, Training Loss: -0.6114\n",
      "Epoch 351/500, Training Loss: -0.5183\n",
      "Epoch 352/500, Training Loss: -0.5875\n",
      "Epoch 353/500, Training Loss: -0.5795\n",
      "Epoch 354/500, Training Loss: -0.5814\n",
      "Epoch 355/500, Training Loss: -0.4980\n",
      "Epoch 356/500, Training Loss: -0.5357\n",
      "Epoch 357/500, Training Loss: -0.5694\n",
      "Epoch 358/500, Training Loss: -0.5702\n",
      "Epoch 359/500, Training Loss: -0.5295\n",
      "Epoch 360/500, Training Loss: -0.6741\n",
      "Epoch 361/500, Training Loss: -0.5682\n",
      "Epoch 362/500, Training Loss: -0.6127\n",
      "Epoch 363/500, Training Loss: -0.6434\n",
      "Epoch 364/500, Training Loss: -0.5763\n",
      "Epoch 365/500, Training Loss: -0.6192\n",
      "Epoch 366/500, Training Loss: -0.5510\n",
      "Epoch 367/500, Training Loss: -0.5605\n",
      "Epoch 368/500, Training Loss: -0.5615\n",
      "Epoch 369/500, Training Loss: -0.5494\n",
      "Epoch 370/500, Training Loss: -0.6264\n",
      "Epoch 371/500, Training Loss: -0.5723\n",
      "Epoch 372/500, Training Loss: -0.5448\n",
      "Epoch 373/500, Training Loss: -0.5473\n",
      "Epoch 374/500, Training Loss: -0.6561\n",
      "Epoch 375/500, Training Loss: -0.5768\n",
      "Epoch 376/500, Training Loss: -0.5578\n",
      "Epoch 377/500, Training Loss: -0.5443\n",
      "Epoch 378/500, Training Loss: -0.5927\n",
      "Epoch 379/500, Training Loss: -0.5384\n",
      "Epoch 380/500, Training Loss: -0.5331\n",
      "Epoch 381/500, Training Loss: -0.5269\n",
      "Epoch 382/500, Training Loss: -0.4764\n",
      "Epoch 383/500, Training Loss: -0.5817\n",
      "Epoch 384/500, Training Loss: -0.6351\n",
      "Epoch 385/500, Training Loss: -0.5924\n",
      "Epoch 386/500, Training Loss: -0.5828\n",
      "Epoch 387/500, Training Loss: -0.5261\n",
      "Epoch 388/500, Training Loss: -0.5814\n",
      "Epoch 389/500, Training Loss: -0.5110\n",
      "Epoch 390/500, Training Loss: -0.6117\n",
      "Epoch 391/500, Training Loss: -0.5376\n",
      "Epoch 392/500, Training Loss: -0.5458\n",
      "Epoch 393/500, Training Loss: -0.5510\n",
      "Epoch 394/500, Training Loss: -0.5834\n",
      "Epoch 395/500, Training Loss: -0.5319\n",
      "Epoch 396/500, Training Loss: -0.5137\n",
      "Epoch 397/500, Training Loss: -0.5727\n",
      "Epoch 398/500, Training Loss: -0.5542\n",
      "Epoch 399/500, Training Loss: -0.5964\n",
      "Epoch 400/500, Training Loss: -0.6134\n",
      "Epoch 401/500, Training Loss: -0.5654\n",
      "Epoch 402/500, Training Loss: -0.5165\n",
      "Epoch 403/500, Training Loss: -0.5450\n",
      "Epoch 404/500, Training Loss: -0.5771\n",
      "Epoch 405/500, Training Loss: -0.6192\n",
      "Epoch 406/500, Training Loss: -0.5473\n",
      "Epoch 407/500, Training Loss: -0.5377\n",
      "Epoch 408/500, Training Loss: -0.6343\n",
      "Epoch 409/500, Training Loss: -0.5679\n",
      "Epoch 410/500, Training Loss: -0.5740\n",
      "Epoch 411/500, Training Loss: -0.6266\n",
      "Epoch 412/500, Training Loss: -0.6067\n",
      "Epoch 413/500, Training Loss: -0.5041\n",
      "Epoch 414/500, Training Loss: -0.5839\n",
      "Epoch 415/500, Training Loss: -0.5173\n",
      "Epoch 416/500, Training Loss: -0.5608\n",
      "Epoch 417/500, Training Loss: -0.5150\n",
      "Epoch 418/500, Training Loss: -0.5614\n",
      "Epoch 419/500, Training Loss: -0.5510\n",
      "Epoch 420/500, Training Loss: -0.4983\n",
      "Epoch 421/500, Training Loss: -0.5911\n",
      "Epoch 422/500, Training Loss: -0.5573\n",
      "Epoch 423/500, Training Loss: -0.5071\n",
      "Epoch 424/500, Training Loss: -0.5426\n",
      "Epoch 425/500, Training Loss: -0.5591\n",
      "Epoch 426/500, Training Loss: -0.5014\n",
      "Epoch 427/500, Training Loss: -0.5461\n",
      "Epoch 428/500, Training Loss: -0.5997\n",
      "Epoch 429/500, Training Loss: -0.5865\n",
      "Epoch 430/500, Training Loss: -0.5530\n",
      "Epoch 431/500, Training Loss: -0.6044\n",
      "Epoch 432/500, Training Loss: -0.5794\n",
      "Epoch 433/500, Training Loss: -0.6092\n",
      "Epoch 434/500, Training Loss: -0.5368\n",
      "Epoch 435/500, Training Loss: -0.6137\n",
      "Epoch 436/500, Training Loss: -0.5917\n",
      "Epoch 437/500, Training Loss: -0.6082\n",
      "Epoch 438/500, Training Loss: -0.5448\n",
      "Epoch 439/500, Training Loss: -0.5743\n",
      "Epoch 440/500, Training Loss: -0.5260\n",
      "Epoch 441/500, Training Loss: -0.5315\n",
      "Epoch 442/500, Training Loss: -0.5983\n",
      "Epoch 443/500, Training Loss: -0.5085\n",
      "Epoch 444/500, Training Loss: -0.6063\n",
      "Epoch 445/500, Training Loss: -0.5496\n",
      "Epoch 446/500, Training Loss: -0.6055\n",
      "Epoch 447/500, Training Loss: -0.5868\n",
      "Epoch 448/500, Training Loss: -0.6009\n",
      "Epoch 449/500, Training Loss: -0.5646\n",
      "Epoch 450/500, Training Loss: -0.5867\n",
      "Epoch 451/500, Training Loss: -0.5980\n",
      "Epoch 452/500, Training Loss: -0.5727\n",
      "Epoch 453/500, Training Loss: -0.5743\n",
      "Epoch 454/500, Training Loss: -0.6032\n",
      "Epoch 455/500, Training Loss: -0.5403\n",
      "Epoch 456/500, Training Loss: -0.5135\n",
      "Epoch 457/500, Training Loss: -0.4693\n",
      "Epoch 458/500, Training Loss: -0.5646\n",
      "Epoch 459/500, Training Loss: -0.5773\n",
      "Epoch 460/500, Training Loss: -0.5713\n",
      "Epoch 461/500, Training Loss: -0.6254\n",
      "Epoch 462/500, Training Loss: -0.5677\n",
      "Epoch 463/500, Training Loss: -0.5335\n",
      "Epoch 464/500, Training Loss: -0.6064\n",
      "Epoch 465/500, Training Loss: -0.5063\n",
      "Epoch 466/500, Training Loss: -0.5612\n",
      "Epoch 467/500, Training Loss: -0.5926\n",
      "Epoch 468/500, Training Loss: -0.5275\n",
      "Epoch 469/500, Training Loss: -0.6207\n",
      "Epoch 470/500, Training Loss: -0.5326\n",
      "Epoch 471/500, Training Loss: -0.5191\n",
      "Epoch 472/500, Training Loss: -0.5686\n",
      "Epoch 473/500, Training Loss: -0.5548\n",
      "Epoch 474/500, Training Loss: -0.5304\n",
      "Epoch 475/500, Training Loss: -0.5819\n",
      "Epoch 476/500, Training Loss: -0.5615\n",
      "Epoch 477/500, Training Loss: -0.5441\n",
      "Epoch 478/500, Training Loss: -0.5998\n",
      "Epoch 479/500, Training Loss: -0.5662\n",
      "Epoch 480/500, Training Loss: -0.5865\n",
      "Epoch 481/500, Training Loss: -0.5179\n",
      "Epoch 482/500, Training Loss: -0.5578\n",
      "Epoch 483/500, Training Loss: -0.5792\n",
      "Epoch 484/500, Training Loss: -0.5812\n",
      "Epoch 485/500, Training Loss: -0.5632\n",
      "Epoch 486/500, Training Loss: -0.5090\n",
      "Epoch 487/500, Training Loss: -0.5371\n",
      "Epoch 488/500, Training Loss: -0.5587\n",
      "Epoch 489/500, Training Loss: -0.6188\n",
      "Epoch 490/500, Training Loss: -0.6177\n",
      "Epoch 491/500, Training Loss: -0.5759\n",
      "Epoch 492/500, Training Loss: -0.5461\n",
      "Epoch 493/500, Training Loss: -0.5077\n",
      "Epoch 494/500, Training Loss: -0.5869\n",
      "Epoch 495/500, Training Loss: -0.5580\n",
      "Epoch 496/500, Training Loss: -0.5642\n",
      "Epoch 497/500, Training Loss: -0.5547\n",
      "Epoch 498/500, Training Loss: -0.5734\n",
      "Epoch 499/500, Training Loss: -0.5277\n",
      "Epoch 500/500, Training Loss: -0.5572\n",
      "Validation Loss: -0.5378\n",
      "Model 3, Fold 0: Validation Loss = -0.5378\n",
      "Epoch 1/500, Training Loss: -0.3650\n",
      "Epoch 2/500, Training Loss: -0.4668\n",
      "Epoch 3/500, Training Loss: -0.5083\n",
      "Epoch 4/500, Training Loss: -0.5028\n",
      "Epoch 5/500, Training Loss: -0.4477\n",
      "Epoch 6/500, Training Loss: -0.4937\n",
      "Epoch 7/500, Training Loss: -0.5133\n",
      "Epoch 8/500, Training Loss: -0.4308\n",
      "Epoch 9/500, Training Loss: -0.5116\n",
      "Epoch 10/500, Training Loss: -0.5325\n",
      "Epoch 11/500, Training Loss: -0.5179\n",
      "Epoch 12/500, Training Loss: -0.5111\n",
      "Epoch 13/500, Training Loss: -0.5738\n",
      "Epoch 14/500, Training Loss: -0.4694\n",
      "Epoch 15/500, Training Loss: -0.5698\n",
      "Epoch 16/500, Training Loss: -0.5124\n",
      "Epoch 17/500, Training Loss: -0.4883\n",
      "Epoch 18/500, Training Loss: -0.5185\n",
      "Epoch 19/500, Training Loss: -0.5394\n",
      "Epoch 20/500, Training Loss: -0.5455\n",
      "Epoch 21/500, Training Loss: -0.5668\n",
      "Epoch 22/500, Training Loss: -0.6052\n",
      "Epoch 23/500, Training Loss: -0.5459\n",
      "Epoch 24/500, Training Loss: -0.5024\n",
      "Epoch 25/500, Training Loss: -0.4564\n",
      "Epoch 26/500, Training Loss: -0.5254\n",
      "Epoch 27/500, Training Loss: -0.5171\n",
      "Epoch 28/500, Training Loss: -0.5150\n",
      "Epoch 29/500, Training Loss: -0.4991\n",
      "Epoch 30/500, Training Loss: -0.5494\n",
      "Epoch 31/500, Training Loss: -0.5611\n",
      "Epoch 32/500, Training Loss: -0.5526\n",
      "Epoch 33/500, Training Loss: -0.5773\n",
      "Epoch 34/500, Training Loss: -0.5373\n",
      "Epoch 35/500, Training Loss: -0.5593\n",
      "Epoch 36/500, Training Loss: -0.6072\n",
      "Epoch 37/500, Training Loss: -0.5447\n",
      "Epoch 38/500, Training Loss: -0.5514\n",
      "Epoch 39/500, Training Loss: -0.5832\n",
      "Epoch 40/500, Training Loss: -0.5457\n",
      "Epoch 41/500, Training Loss: -0.5218\n",
      "Epoch 42/500, Training Loss: -0.5779\n",
      "Epoch 43/500, Training Loss: -0.4919\n",
      "Epoch 44/500, Training Loss: -0.5327\n",
      "Epoch 45/500, Training Loss: -0.5375\n",
      "Epoch 46/500, Training Loss: -0.5581\n",
      "Epoch 47/500, Training Loss: -0.5868\n",
      "Epoch 48/500, Training Loss: -0.5193\n",
      "Epoch 49/500, Training Loss: -0.5799\n",
      "Epoch 50/500, Training Loss: -0.5643\n",
      "Epoch 51/500, Training Loss: -0.5638\n",
      "Epoch 52/500, Training Loss: -0.5717\n",
      "Epoch 53/500, Training Loss: -0.5478\n",
      "Epoch 54/500, Training Loss: -0.6200\n",
      "Epoch 55/500, Training Loss: -0.5837\n",
      "Epoch 56/500, Training Loss: -0.5924\n",
      "Epoch 57/500, Training Loss: -0.5202\n",
      "Epoch 58/500, Training Loss: -0.5844\n",
      "Epoch 59/500, Training Loss: -0.5010\n",
      "Epoch 60/500, Training Loss: -0.5556\n",
      "Epoch 61/500, Training Loss: -0.5788\n",
      "Epoch 62/500, Training Loss: -0.5494\n",
      "Epoch 63/500, Training Loss: -0.5568\n",
      "Epoch 64/500, Training Loss: -0.5665\n",
      "Epoch 65/500, Training Loss: -0.5652\n",
      "Epoch 66/500, Training Loss: -0.6029\n",
      "Epoch 67/500, Training Loss: -0.5532\n",
      "Epoch 68/500, Training Loss: -0.5831\n",
      "Epoch 69/500, Training Loss: -0.5245\n",
      "Epoch 70/500, Training Loss: -0.5470\n",
      "Epoch 71/500, Training Loss: -0.4843\n",
      "Epoch 72/500, Training Loss: -0.5704\n",
      "Epoch 73/500, Training Loss: -0.5821\n",
      "Epoch 74/500, Training Loss: -0.4826\n",
      "Epoch 75/500, Training Loss: -0.5391\n",
      "Epoch 76/500, Training Loss: -0.5068\n",
      "Epoch 77/500, Training Loss: -0.5784\n",
      "Epoch 78/500, Training Loss: -0.5581\n",
      "Epoch 79/500, Training Loss: -0.5495\n",
      "Epoch 80/500, Training Loss: -0.5861\n",
      "Epoch 81/500, Training Loss: -0.5898\n",
      "Epoch 82/500, Training Loss: -0.5603\n",
      "Epoch 83/500, Training Loss: -0.5732\n",
      "Epoch 84/500, Training Loss: -0.5795\n",
      "Epoch 85/500, Training Loss: -0.5104\n",
      "Epoch 86/500, Training Loss: -0.6026\n",
      "Epoch 87/500, Training Loss: -0.5869\n",
      "Epoch 88/500, Training Loss: -0.5015\n",
      "Epoch 89/500, Training Loss: -0.5002\n",
      "Epoch 90/500, Training Loss: -0.6135\n",
      "Epoch 91/500, Training Loss: -0.4876\n",
      "Epoch 92/500, Training Loss: -0.5351\n",
      "Epoch 93/500, Training Loss: -0.5305\n",
      "Epoch 94/500, Training Loss: -0.5400\n",
      "Epoch 95/500, Training Loss: -0.5211\n",
      "Epoch 96/500, Training Loss: -0.5144\n",
      "Epoch 97/500, Training Loss: -0.5347\n",
      "Epoch 98/500, Training Loss: -0.5736\n",
      "Epoch 99/500, Training Loss: -0.5575\n",
      "Epoch 100/500, Training Loss: -0.5454\n",
      "Epoch 101/500, Training Loss: -0.5471\n",
      "Epoch 102/500, Training Loss: -0.4988\n",
      "Epoch 103/500, Training Loss: -0.5470\n",
      "Epoch 104/500, Training Loss: -0.4910\n",
      "Epoch 105/500, Training Loss: -0.5248\n",
      "Epoch 106/500, Training Loss: -0.5324\n",
      "Epoch 107/500, Training Loss: -0.6009\n",
      "Epoch 108/500, Training Loss: -0.5586\n",
      "Epoch 109/500, Training Loss: -0.5072\n",
      "Epoch 110/500, Training Loss: -0.5207\n",
      "Epoch 111/500, Training Loss: -0.5145\n",
      "Epoch 112/500, Training Loss: -0.5324\n",
      "Epoch 113/500, Training Loss: -0.5328\n",
      "Epoch 114/500, Training Loss: -0.5443\n",
      "Epoch 115/500, Training Loss: -0.4759\n",
      "Epoch 116/500, Training Loss: -0.5883\n",
      "Epoch 117/500, Training Loss: -0.4782\n",
      "Epoch 118/500, Training Loss: -0.5766\n",
      "Epoch 119/500, Training Loss: -0.5208\n",
      "Epoch 120/500, Training Loss: -0.5644\n",
      "Epoch 121/500, Training Loss: -0.5811\n",
      "Epoch 122/500, Training Loss: -0.4896\n",
      "Epoch 123/500, Training Loss: -0.5667\n",
      "Epoch 124/500, Training Loss: -0.5354\n",
      "Epoch 125/500, Training Loss: -0.6228\n",
      "Epoch 126/500, Training Loss: -0.5171\n",
      "Epoch 127/500, Training Loss: -0.5116\n",
      "Epoch 128/500, Training Loss: -0.4700\n",
      "Epoch 129/500, Training Loss: -0.5672\n",
      "Epoch 130/500, Training Loss: -0.5315\n",
      "Epoch 131/500, Training Loss: -0.5705\n",
      "Epoch 132/500, Training Loss: -0.5191\n",
      "Epoch 133/500, Training Loss: -0.6213\n",
      "Epoch 134/500, Training Loss: -0.5437\n",
      "Epoch 135/500, Training Loss: -0.5783\n",
      "Epoch 136/500, Training Loss: -0.5609\n",
      "Epoch 137/500, Training Loss: -0.5635\n",
      "Epoch 138/500, Training Loss: -0.4707\n",
      "Epoch 139/500, Training Loss: -0.5508\n",
      "Epoch 140/500, Training Loss: -0.5034\n",
      "Epoch 141/500, Training Loss: -0.5448\n",
      "Epoch 142/500, Training Loss: -0.5785\n",
      "Epoch 143/500, Training Loss: -0.5523\n",
      "Epoch 144/500, Training Loss: -0.5353\n",
      "Epoch 145/500, Training Loss: -0.5622\n",
      "Epoch 146/500, Training Loss: -0.5131\n",
      "Epoch 147/500, Training Loss: -0.5610\n",
      "Epoch 148/500, Training Loss: -0.5193\n",
      "Epoch 149/500, Training Loss: -0.5128\n",
      "Epoch 150/500, Training Loss: -0.5334\n",
      "Epoch 151/500, Training Loss: -0.4779\n",
      "Epoch 152/500, Training Loss: -0.5719\n",
      "Epoch 153/500, Training Loss: -0.5696\n",
      "Epoch 154/500, Training Loss: -0.5827\n",
      "Epoch 155/500, Training Loss: -0.5258\n",
      "Epoch 156/500, Training Loss: -0.5340\n",
      "Epoch 157/500, Training Loss: -0.5561\n",
      "Epoch 158/500, Training Loss: -0.5232\n",
      "Epoch 159/500, Training Loss: -0.5707\n",
      "Epoch 160/500, Training Loss: -0.4780\n",
      "Epoch 161/500, Training Loss: -0.6427\n",
      "Epoch 162/500, Training Loss: -0.5269\n",
      "Epoch 163/500, Training Loss: -0.5548\n",
      "Epoch 164/500, Training Loss: -0.5074\n",
      "Epoch 165/500, Training Loss: -0.5133\n",
      "Epoch 166/500, Training Loss: -0.4550\n",
      "Epoch 167/500, Training Loss: -0.5755\n",
      "Epoch 168/500, Training Loss: -0.5639\n",
      "Epoch 169/500, Training Loss: -0.4958\n",
      "Epoch 170/500, Training Loss: -0.6053\n",
      "Epoch 171/500, Training Loss: -0.5402\n",
      "Epoch 172/500, Training Loss: -0.5795\n",
      "Epoch 173/500, Training Loss: -0.5046\n",
      "Epoch 174/500, Training Loss: -0.5074\n",
      "Epoch 175/500, Training Loss: -0.5767\n",
      "Epoch 176/500, Training Loss: -0.5313\n",
      "Epoch 177/500, Training Loss: -0.5227\n",
      "Epoch 178/500, Training Loss: -0.5356\n",
      "Epoch 179/500, Training Loss: -0.5601\n",
      "Epoch 180/500, Training Loss: -0.5869\n",
      "Epoch 181/500, Training Loss: -0.5107\n",
      "Epoch 182/500, Training Loss: -0.5565\n",
      "Epoch 183/500, Training Loss: -0.5853\n",
      "Epoch 184/500, Training Loss: -0.5844\n",
      "Epoch 185/500, Training Loss: -0.5047\n",
      "Epoch 186/500, Training Loss: -0.5543\n",
      "Epoch 187/500, Training Loss: -0.5285\n",
      "Epoch 188/500, Training Loss: -0.5578\n",
      "Epoch 189/500, Training Loss: -0.5584\n",
      "Epoch 190/500, Training Loss: -0.5505\n",
      "Epoch 191/500, Training Loss: -0.6011\n",
      "Epoch 192/500, Training Loss: -0.5552\n",
      "Epoch 193/500, Training Loss: -0.5978\n",
      "Epoch 194/500, Training Loss: -0.5540\n",
      "Epoch 195/500, Training Loss: -0.5353\n",
      "Epoch 196/500, Training Loss: -0.5493\n",
      "Epoch 197/500, Training Loss: -0.5202\n",
      "Epoch 198/500, Training Loss: -0.5445\n",
      "Epoch 199/500, Training Loss: -0.5742\n",
      "Epoch 200/500, Training Loss: -0.5691\n",
      "Epoch 201/500, Training Loss: -0.5775\n",
      "Epoch 202/500, Training Loss: -0.5041\n",
      "Epoch 203/500, Training Loss: -0.6419\n",
      "Epoch 204/500, Training Loss: -0.5341\n",
      "Epoch 205/500, Training Loss: -0.5944\n",
      "Epoch 206/500, Training Loss: -0.5181\n",
      "Epoch 207/500, Training Loss: -0.5024\n",
      "Epoch 208/500, Training Loss: -0.5441\n",
      "Epoch 209/500, Training Loss: -0.5149\n",
      "Epoch 210/500, Training Loss: -0.4713\n",
      "Epoch 211/500, Training Loss: -0.5698\n",
      "Epoch 212/500, Training Loss: -0.5808\n",
      "Epoch 213/500, Training Loss: -0.5881\n",
      "Epoch 214/500, Training Loss: -0.4907\n",
      "Epoch 215/500, Training Loss: -0.4685\n",
      "Epoch 216/500, Training Loss: -0.5530\n",
      "Epoch 217/500, Training Loss: -0.5332\n",
      "Epoch 218/500, Training Loss: -0.5516\n",
      "Epoch 219/500, Training Loss: -0.5313\n",
      "Epoch 220/500, Training Loss: -0.5061\n",
      "Epoch 221/500, Training Loss: -0.5664\n",
      "Epoch 222/500, Training Loss: -0.5770\n",
      "Epoch 223/500, Training Loss: -0.5361\n",
      "Epoch 224/500, Training Loss: -0.4839\n",
      "Epoch 225/500, Training Loss: -0.5402\n",
      "Epoch 226/500, Training Loss: -0.5923\n",
      "Epoch 227/500, Training Loss: -0.6030\n",
      "Epoch 228/500, Training Loss: -0.6311\n",
      "Epoch 229/500, Training Loss: -0.5342\n",
      "Epoch 230/500, Training Loss: -0.5683\n",
      "Epoch 231/500, Training Loss: -0.5778\n",
      "Epoch 232/500, Training Loss: -0.5582\n",
      "Epoch 233/500, Training Loss: -0.5278\n",
      "Epoch 234/500, Training Loss: -0.5564\n",
      "Epoch 235/500, Training Loss: -0.5505\n",
      "Epoch 236/500, Training Loss: -0.4827\n",
      "Epoch 237/500, Training Loss: -0.5594\n",
      "Epoch 238/500, Training Loss: -0.5085\n",
      "Epoch 239/500, Training Loss: -0.5227\n",
      "Epoch 240/500, Training Loss: -0.5456\n",
      "Epoch 241/500, Training Loss: -0.5233\n",
      "Epoch 242/500, Training Loss: -0.5420\n",
      "Epoch 243/500, Training Loss: -0.5482\n",
      "Epoch 244/500, Training Loss: -0.5862\n",
      "Epoch 245/500, Training Loss: -0.5366\n",
      "Epoch 246/500, Training Loss: -0.6071\n",
      "Epoch 247/500, Training Loss: -0.5383\n",
      "Epoch 248/500, Training Loss: -0.5204\n",
      "Epoch 249/500, Training Loss: -0.5859\n",
      "Epoch 250/500, Training Loss: -0.5655\n",
      "Epoch 251/500, Training Loss: -0.5446\n",
      "Epoch 252/500, Training Loss: -0.6285\n",
      "Epoch 253/500, Training Loss: -0.5745\n",
      "Epoch 254/500, Training Loss: -0.6125\n",
      "Epoch 255/500, Training Loss: -0.5636\n",
      "Epoch 256/500, Training Loss: -0.5041\n",
      "Epoch 257/500, Training Loss: -0.4389\n",
      "Epoch 258/500, Training Loss: -0.5227\n",
      "Epoch 259/500, Training Loss: -0.5918\n",
      "Epoch 260/500, Training Loss: -0.5577\n",
      "Epoch 261/500, Training Loss: -0.4916\n",
      "Epoch 262/500, Training Loss: -0.5298\n",
      "Epoch 263/500, Training Loss: -0.5553\n",
      "Epoch 264/500, Training Loss: -0.5775\n",
      "Epoch 265/500, Training Loss: -0.6004\n",
      "Epoch 266/500, Training Loss: -0.6557\n",
      "Epoch 267/500, Training Loss: -0.5179\n",
      "Epoch 268/500, Training Loss: -0.5048\n",
      "Epoch 269/500, Training Loss: -0.4879\n",
      "Epoch 270/500, Training Loss: -0.5701\n",
      "Epoch 271/500, Training Loss: -0.6439\n",
      "Epoch 272/500, Training Loss: -0.5238\n",
      "Epoch 273/500, Training Loss: -0.5320\n",
      "Epoch 274/500, Training Loss: -0.4909\n",
      "Epoch 275/500, Training Loss: -0.5356\n",
      "Epoch 276/500, Training Loss: -0.5951\n",
      "Epoch 277/500, Training Loss: -0.5302\n",
      "Epoch 278/500, Training Loss: -0.5463\n",
      "Epoch 279/500, Training Loss: -0.5381\n",
      "Epoch 280/500, Training Loss: -0.5030\n",
      "Epoch 281/500, Training Loss: -0.5770\n",
      "Epoch 282/500, Training Loss: -0.5591\n",
      "Epoch 283/500, Training Loss: -0.5708\n",
      "Epoch 284/500, Training Loss: -0.5121\n",
      "Epoch 285/500, Training Loss: -0.5826\n",
      "Epoch 286/500, Training Loss: -0.5832\n",
      "Epoch 287/500, Training Loss: -0.5014\n",
      "Epoch 288/500, Training Loss: -0.5622\n",
      "Epoch 289/500, Training Loss: -0.6125\n",
      "Epoch 290/500, Training Loss: -0.5176\n",
      "Epoch 291/500, Training Loss: -0.5786\n",
      "Epoch 292/500, Training Loss: -0.5684\n",
      "Epoch 293/500, Training Loss: -0.5368\n",
      "Epoch 294/500, Training Loss: -0.5405\n",
      "Epoch 295/500, Training Loss: -0.5071\n",
      "Epoch 296/500, Training Loss: -0.5271\n",
      "Epoch 297/500, Training Loss: -0.5419\n",
      "Epoch 298/500, Training Loss: -0.5383\n",
      "Epoch 299/500, Training Loss: -0.5635\n",
      "Epoch 300/500, Training Loss: -0.5027\n",
      "Epoch 301/500, Training Loss: -0.5970\n",
      "Epoch 302/500, Training Loss: -0.5158\n",
      "Epoch 303/500, Training Loss: -0.5465\n",
      "Epoch 304/500, Training Loss: -0.5659\n",
      "Epoch 305/500, Training Loss: -0.5806\n",
      "Epoch 306/500, Training Loss: -0.6137\n",
      "Epoch 307/500, Training Loss: -0.5602\n",
      "Epoch 308/500, Training Loss: -0.4781\n",
      "Epoch 309/500, Training Loss: -0.5620\n",
      "Epoch 310/500, Training Loss: -0.5357\n",
      "Epoch 311/500, Training Loss: -0.5586\n",
      "Epoch 312/500, Training Loss: -0.5599\n",
      "Epoch 313/500, Training Loss: -0.4768\n",
      "Epoch 314/500, Training Loss: -0.5447\n",
      "Epoch 315/500, Training Loss: -0.5930\n",
      "Epoch 316/500, Training Loss: -0.5495\n",
      "Epoch 317/500, Training Loss: -0.5457\n",
      "Epoch 318/500, Training Loss: -0.5626\n",
      "Epoch 319/500, Training Loss: -0.6258\n",
      "Epoch 320/500, Training Loss: -0.5247\n",
      "Epoch 321/500, Training Loss: -0.5594\n",
      "Epoch 322/500, Training Loss: -0.5222\n",
      "Epoch 323/500, Training Loss: -0.5684\n",
      "Epoch 324/500, Training Loss: -0.5408\n",
      "Epoch 325/500, Training Loss: -0.5908\n",
      "Epoch 326/500, Training Loss: -0.5456\n",
      "Epoch 327/500, Training Loss: -0.5135\n",
      "Epoch 328/500, Training Loss: -0.5647\n",
      "Epoch 329/500, Training Loss: -0.5471\n",
      "Epoch 330/500, Training Loss: -0.5701\n",
      "Epoch 331/500, Training Loss: -0.5216\n",
      "Epoch 332/500, Training Loss: -0.5449\n",
      "Epoch 333/500, Training Loss: -0.5839\n",
      "Epoch 334/500, Training Loss: -0.5172\n",
      "Epoch 335/500, Training Loss: -0.5301\n",
      "Epoch 336/500, Training Loss: -0.5615\n",
      "Epoch 337/500, Training Loss: -0.4944\n",
      "Epoch 338/500, Training Loss: -0.6265\n",
      "Epoch 339/500, Training Loss: -0.5943\n",
      "Epoch 340/500, Training Loss: -0.5796\n",
      "Epoch 341/500, Training Loss: -0.6207\n",
      "Epoch 342/500, Training Loss: -0.5848\n",
      "Epoch 343/500, Training Loss: -0.5619\n",
      "Epoch 344/500, Training Loss: -0.5339\n",
      "Epoch 345/500, Training Loss: -0.5370\n",
      "Epoch 346/500, Training Loss: -0.5002\n",
      "Epoch 347/500, Training Loss: -0.5247\n",
      "Epoch 348/500, Training Loss: -0.5804\n",
      "Epoch 349/500, Training Loss: -0.6031\n",
      "Epoch 350/500, Training Loss: -0.5890\n",
      "Epoch 351/500, Training Loss: -0.5557\n",
      "Epoch 352/500, Training Loss: -0.5284\n",
      "Epoch 353/500, Training Loss: -0.5800\n",
      "Epoch 354/500, Training Loss: -0.5541\n",
      "Epoch 355/500, Training Loss: -0.5557\n",
      "Epoch 356/500, Training Loss: -0.5720\n",
      "Epoch 357/500, Training Loss: -0.5594\n",
      "Epoch 358/500, Training Loss: -0.5544\n",
      "Epoch 359/500, Training Loss: -0.5829\n",
      "Epoch 360/500, Training Loss: -0.5356\n",
      "Epoch 361/500, Training Loss: -0.5448\n",
      "Epoch 362/500, Training Loss: -0.5512\n",
      "Epoch 363/500, Training Loss: -0.5510\n",
      "Epoch 364/500, Training Loss: -0.5538\n",
      "Epoch 365/500, Training Loss: -0.6207\n",
      "Epoch 366/500, Training Loss: -0.5513\n",
      "Epoch 367/500, Training Loss: -0.5252\n",
      "Epoch 368/500, Training Loss: -0.5652\n",
      "Epoch 369/500, Training Loss: -0.4842\n",
      "Epoch 370/500, Training Loss: -0.5541\n",
      "Epoch 371/500, Training Loss: -0.5259\n",
      "Epoch 372/500, Training Loss: -0.5164\n",
      "Epoch 373/500, Training Loss: -0.5148\n",
      "Epoch 374/500, Training Loss: -0.5531\n",
      "Epoch 375/500, Training Loss: -0.5955\n",
      "Epoch 376/500, Training Loss: -0.5648\n",
      "Epoch 377/500, Training Loss: -0.6017\n",
      "Epoch 378/500, Training Loss: -0.5364\n",
      "Epoch 379/500, Training Loss: -0.5509\n",
      "Epoch 380/500, Training Loss: -0.5558\n",
      "Epoch 381/500, Training Loss: -0.4985\n",
      "Epoch 382/500, Training Loss: -0.5922\n",
      "Epoch 383/500, Training Loss: -0.5106\n",
      "Epoch 384/500, Training Loss: -0.5048\n",
      "Epoch 385/500, Training Loss: -0.6014\n",
      "Epoch 386/500, Training Loss: -0.5474\n",
      "Epoch 387/500, Training Loss: -0.5332\n",
      "Epoch 388/500, Training Loss: -0.5181\n",
      "Epoch 389/500, Training Loss: -0.5607\n",
      "Epoch 390/500, Training Loss: -0.5882\n",
      "Epoch 391/500, Training Loss: -0.4542\n",
      "Epoch 392/500, Training Loss: -0.5383\n",
      "Epoch 393/500, Training Loss: -0.5035\n",
      "Epoch 394/500, Training Loss: -0.5289\n",
      "Epoch 395/500, Training Loss: -0.5282\n",
      "Epoch 396/500, Training Loss: -0.5481\n",
      "Epoch 397/500, Training Loss: -0.5574\n",
      "Epoch 398/500, Training Loss: -0.5855\n",
      "Epoch 399/500, Training Loss: -0.5473\n",
      "Epoch 400/500, Training Loss: -0.5020\n",
      "Epoch 401/500, Training Loss: -0.5170\n",
      "Epoch 402/500, Training Loss: -0.5327\n",
      "Epoch 403/500, Training Loss: -0.5721\n",
      "Epoch 404/500, Training Loss: -0.5592\n",
      "Epoch 405/500, Training Loss: -0.5712\n",
      "Epoch 406/500, Training Loss: -0.5231\n",
      "Epoch 407/500, Training Loss: -0.5756\n",
      "Epoch 408/500, Training Loss: -0.5406\n",
      "Epoch 409/500, Training Loss: -0.5036\n",
      "Epoch 410/500, Training Loss: -0.5081\n",
      "Epoch 411/500, Training Loss: -0.5574\n",
      "Epoch 412/500, Training Loss: -0.5649\n",
      "Epoch 413/500, Training Loss: -0.5490\n",
      "Epoch 414/500, Training Loss: -0.5896\n",
      "Epoch 415/500, Training Loss: -0.5284\n",
      "Epoch 416/500, Training Loss: -0.6215\n",
      "Epoch 417/500, Training Loss: -0.5928\n",
      "Epoch 418/500, Training Loss: -0.5731\n",
      "Epoch 419/500, Training Loss: -0.5378\n",
      "Epoch 420/500, Training Loss: -0.5668\n",
      "Epoch 421/500, Training Loss: -0.5905\n",
      "Epoch 422/500, Training Loss: -0.5278\n",
      "Epoch 423/500, Training Loss: -0.5199\n",
      "Epoch 424/500, Training Loss: -0.5642\n",
      "Epoch 425/500, Training Loss: -0.5285\n",
      "Epoch 426/500, Training Loss: -0.4683\n",
      "Epoch 427/500, Training Loss: -0.5373\n",
      "Epoch 428/500, Training Loss: -0.5263\n",
      "Epoch 429/500, Training Loss: -0.4897\n",
      "Epoch 430/500, Training Loss: -0.5487\n",
      "Epoch 431/500, Training Loss: -0.4993\n",
      "Epoch 432/500, Training Loss: -0.6026\n",
      "Epoch 433/500, Training Loss: -0.5648\n",
      "Epoch 434/500, Training Loss: -0.5526\n",
      "Epoch 435/500, Training Loss: -0.5366\n",
      "Epoch 436/500, Training Loss: -0.5063\n",
      "Epoch 437/500, Training Loss: -0.5681\n",
      "Epoch 438/500, Training Loss: -0.5685\n",
      "Epoch 439/500, Training Loss: -0.5310\n",
      "Epoch 440/500, Training Loss: -0.5695\n",
      "Epoch 441/500, Training Loss: -0.5501\n",
      "Epoch 442/500, Training Loss: -0.5770\n",
      "Epoch 443/500, Training Loss: -0.5508\n",
      "Epoch 444/500, Training Loss: -0.5675\n",
      "Epoch 445/500, Training Loss: -0.5498\n",
      "Epoch 446/500, Training Loss: -0.5022\n",
      "Epoch 447/500, Training Loss: -0.5285\n",
      "Epoch 448/500, Training Loss: -0.5477\n",
      "Epoch 449/500, Training Loss: -0.5414\n",
      "Epoch 450/500, Training Loss: -0.5693\n",
      "Epoch 451/500, Training Loss: -0.5518\n",
      "Epoch 452/500, Training Loss: -0.5748\n",
      "Epoch 453/500, Training Loss: -0.5356\n",
      "Epoch 454/500, Training Loss: -0.5876\n",
      "Epoch 455/500, Training Loss: -0.5716\n",
      "Epoch 456/500, Training Loss: -0.5627\n",
      "Epoch 457/500, Training Loss: -0.5120\n",
      "Epoch 458/500, Training Loss: -0.5308\n",
      "Epoch 459/500, Training Loss: -0.5301\n",
      "Epoch 460/500, Training Loss: -0.5394\n",
      "Epoch 461/500, Training Loss: -0.5704\n",
      "Epoch 462/500, Training Loss: -0.4845\n",
      "Epoch 463/500, Training Loss: -0.5399\n",
      "Epoch 464/500, Training Loss: -0.4981\n",
      "Epoch 465/500, Training Loss: -0.4906\n",
      "Epoch 466/500, Training Loss: -0.5588\n",
      "Epoch 467/500, Training Loss: -0.4983\n",
      "Epoch 468/500, Training Loss: -0.5480\n",
      "Epoch 469/500, Training Loss: -0.5022\n",
      "Epoch 470/500, Training Loss: -0.6052\n",
      "Epoch 471/500, Training Loss: -0.5845\n",
      "Epoch 472/500, Training Loss: -0.5639\n",
      "Epoch 473/500, Training Loss: -0.5375\n",
      "Epoch 474/500, Training Loss: -0.5386\n",
      "Epoch 475/500, Training Loss: -0.5290\n",
      "Epoch 476/500, Training Loss: -0.5638\n",
      "Epoch 477/500, Training Loss: -0.5184\n",
      "Epoch 478/500, Training Loss: -0.5478\n",
      "Epoch 479/500, Training Loss: -0.5671\n",
      "Epoch 480/500, Training Loss: -0.5046\n",
      "Epoch 481/500, Training Loss: -0.5947\n",
      "Epoch 482/500, Training Loss: -0.5683\n",
      "Epoch 483/500, Training Loss: -0.5395\n",
      "Epoch 484/500, Training Loss: -0.5430\n",
      "Epoch 485/500, Training Loss: -0.5832\n",
      "Epoch 486/500, Training Loss: -0.5331\n",
      "Epoch 487/500, Training Loss: -0.5176\n",
      "Epoch 488/500, Training Loss: -0.5137\n",
      "Epoch 489/500, Training Loss: -0.5603\n",
      "Epoch 490/500, Training Loss: -0.6055\n",
      "Epoch 491/500, Training Loss: -0.5224\n",
      "Epoch 492/500, Training Loss: -0.5140\n",
      "Epoch 493/500, Training Loss: -0.5322\n",
      "Epoch 494/500, Training Loss: -0.4499\n",
      "Epoch 495/500, Training Loss: -0.5862\n",
      "Epoch 496/500, Training Loss: -0.5798\n",
      "Epoch 497/500, Training Loss: -0.6270\n",
      "Epoch 498/500, Training Loss: -0.5947\n",
      "Epoch 499/500, Training Loss: -0.6096\n",
      "Epoch 500/500, Training Loss: -0.5966\n",
      "Validation Loss: -0.5444\n",
      "Model 3, Fold 1: Validation Loss = -0.5444\n",
      "Best overall model index: 0 with average validation loss -0.5538\n",
      "Retraining best model on full dataset...\n",
      "Epoch 1/500, Training Loss: 0.0520\n",
      "Epoch 2/500, Training Loss: 0.0608\n",
      "Epoch 3/500, Training Loss: -0.0052\n",
      "Epoch 4/500, Training Loss: -0.0895\n",
      "Epoch 5/500, Training Loss: -0.0701\n",
      "Epoch 6/500, Training Loss: -0.1397\n",
      "Epoch 7/500, Training Loss: -0.1752\n",
      "Epoch 8/500, Training Loss: -0.2082\n",
      "Epoch 9/500, Training Loss: -0.2924\n",
      "Epoch 10/500, Training Loss: -0.3150\n",
      "Epoch 11/500, Training Loss: -0.3463\n",
      "Epoch 12/500, Training Loss: -0.4464\n",
      "Epoch 13/500, Training Loss: -0.4593\n",
      "Epoch 14/500, Training Loss: -0.4897\n",
      "Epoch 15/500, Training Loss: -0.5225\n",
      "Epoch 16/500, Training Loss: -0.4958\n",
      "Epoch 17/500, Training Loss: -0.5316\n",
      "Epoch 18/500, Training Loss: -0.5322\n",
      "Epoch 19/500, Training Loss: -0.5436\n",
      "Epoch 20/500, Training Loss: -0.5496\n",
      "Epoch 21/500, Training Loss: -0.5410\n",
      "Epoch 22/500, Training Loss: -0.5640\n",
      "Epoch 23/500, Training Loss: -0.5489\n",
      "Epoch 24/500, Training Loss: -0.5494\n",
      "Epoch 25/500, Training Loss: -0.5511\n",
      "Epoch 26/500, Training Loss: -0.5268\n",
      "Epoch 27/500, Training Loss: -0.5524\n",
      "Epoch 28/500, Training Loss: -0.5634\n",
      "Epoch 29/500, Training Loss: -0.5416\n",
      "Epoch 30/500, Training Loss: -0.5190\n",
      "Epoch 31/500, Training Loss: -0.5554\n",
      "Epoch 32/500, Training Loss: -0.5625\n",
      "Epoch 33/500, Training Loss: -0.5688\n",
      "Epoch 34/500, Training Loss: -0.5576\n",
      "Epoch 35/500, Training Loss: -0.5468\n",
      "Epoch 36/500, Training Loss: -0.5610\n",
      "Epoch 37/500, Training Loss: -0.5557\n",
      "Epoch 38/500, Training Loss: -0.5537\n",
      "Epoch 39/500, Training Loss: -0.5446\n",
      "Epoch 40/500, Training Loss: -0.5321\n",
      "Epoch 41/500, Training Loss: -0.5546\n",
      "Epoch 42/500, Training Loss: -0.5373\n",
      "Epoch 43/500, Training Loss: -0.5535\n",
      "Epoch 44/500, Training Loss: -0.5529\n",
      "Epoch 45/500, Training Loss: -0.5545\n",
      "Epoch 46/500, Training Loss: -0.5571\n",
      "Epoch 47/500, Training Loss: -0.5591\n",
      "Epoch 48/500, Training Loss: -0.5904\n",
      "Epoch 49/500, Training Loss: -0.5254\n",
      "Epoch 50/500, Training Loss: -0.5396\n",
      "Epoch 51/500, Training Loss: -0.5498\n",
      "Epoch 52/500, Training Loss: -0.5800\n",
      "Epoch 53/500, Training Loss: -0.5504\n",
      "Epoch 54/500, Training Loss: -0.5605\n",
      "Epoch 55/500, Training Loss: -0.5520\n",
      "Epoch 56/500, Training Loss: -0.5563\n",
      "Epoch 57/500, Training Loss: -0.5543\n",
      "Epoch 58/500, Training Loss: -0.5821\n",
      "Epoch 59/500, Training Loss: -0.5428\n",
      "Epoch 60/500, Training Loss: -0.5400\n",
      "Epoch 61/500, Training Loss: -0.5620\n",
      "Epoch 62/500, Training Loss: -0.5560\n",
      "Epoch 63/500, Training Loss: -0.5478\n",
      "Epoch 64/500, Training Loss: -0.5348\n",
      "Epoch 65/500, Training Loss: -0.5590\n",
      "Epoch 66/500, Training Loss: -0.5415\n",
      "Epoch 67/500, Training Loss: -0.5544\n",
      "Epoch 68/500, Training Loss: -0.5513\n",
      "Epoch 69/500, Training Loss: -0.5545\n",
      "Epoch 70/500, Training Loss: -0.5772\n",
      "Epoch 71/500, Training Loss: -0.5729\n",
      "Epoch 72/500, Training Loss: -0.5937\n",
      "Epoch 73/500, Training Loss: -0.5504\n",
      "Epoch 74/500, Training Loss: -0.5401\n",
      "Epoch 75/500, Training Loss: -0.5692\n",
      "Epoch 76/500, Training Loss: -0.5669\n",
      "Epoch 77/500, Training Loss: -0.5467\n",
      "Epoch 78/500, Training Loss: -0.5366\n",
      "Epoch 79/500, Training Loss: -0.5779\n",
      "Epoch 80/500, Training Loss: -0.5554\n",
      "Epoch 81/500, Training Loss: -0.5329\n",
      "Epoch 82/500, Training Loss: -0.5570\n",
      "Epoch 83/500, Training Loss: -0.5366\n",
      "Epoch 84/500, Training Loss: -0.5571\n",
      "Epoch 85/500, Training Loss: -0.5961\n",
      "Epoch 86/500, Training Loss: -0.5357\n",
      "Epoch 87/500, Training Loss: -0.5883\n",
      "Epoch 88/500, Training Loss: -0.5516\n",
      "Epoch 89/500, Training Loss: -0.5629\n",
      "Epoch 90/500, Training Loss: -0.5814\n",
      "Epoch 91/500, Training Loss: -0.5374\n",
      "Epoch 92/500, Training Loss: -0.5487\n",
      "Epoch 93/500, Training Loss: -0.5710\n",
      "Epoch 94/500, Training Loss: -0.5534\n",
      "Epoch 95/500, Training Loss: -0.5555\n",
      "Epoch 96/500, Training Loss: -0.5476\n",
      "Epoch 97/500, Training Loss: -0.5635\n",
      "Epoch 98/500, Training Loss: -0.5820\n",
      "Epoch 99/500, Training Loss: -0.5436\n",
      "Epoch 100/500, Training Loss: -0.5501\n",
      "Epoch 101/500, Training Loss: -0.5465\n",
      "Epoch 102/500, Training Loss: -0.5380\n",
      "Epoch 103/500, Training Loss: -0.5789\n",
      "Epoch 104/500, Training Loss: -0.5460\n",
      "Epoch 105/500, Training Loss: -0.5639\n",
      "Epoch 106/500, Training Loss: -0.5672\n",
      "Epoch 107/500, Training Loss: -0.5719\n",
      "Epoch 108/500, Training Loss: -0.5736\n",
      "Epoch 109/500, Training Loss: -0.5640\n",
      "Epoch 110/500, Training Loss: -0.5582\n",
      "Epoch 111/500, Training Loss: -0.5698\n",
      "Epoch 112/500, Training Loss: -0.5411\n",
      "Epoch 113/500, Training Loss: -0.5469\n",
      "Epoch 114/500, Training Loss: -0.5411\n",
      "Epoch 115/500, Training Loss: -0.5508\n",
      "Epoch 116/500, Training Loss: -0.5492\n",
      "Epoch 117/500, Training Loss: -0.5552\n",
      "Epoch 118/500, Training Loss: -0.5618\n",
      "Epoch 119/500, Training Loss: -0.5750\n",
      "Epoch 120/500, Training Loss: -0.5223\n",
      "Epoch 121/500, Training Loss: -0.5653\n",
      "Epoch 122/500, Training Loss: -0.5417\n",
      "Epoch 123/500, Training Loss: -0.5760\n",
      "Epoch 124/500, Training Loss: -0.5292\n",
      "Epoch 125/500, Training Loss: -0.5544\n",
      "Epoch 126/500, Training Loss: -0.5631\n",
      "Epoch 127/500, Training Loss: -0.5478\n",
      "Epoch 128/500, Training Loss: -0.5579\n",
      "Epoch 129/500, Training Loss: -0.5561\n",
      "Epoch 130/500, Training Loss: -0.5739\n",
      "Epoch 131/500, Training Loss: -0.5567\n",
      "Epoch 132/500, Training Loss: -0.5848\n",
      "Epoch 133/500, Training Loss: -0.5671\n",
      "Epoch 134/500, Training Loss: -0.5481\n",
      "Epoch 135/500, Training Loss: -0.5714\n",
      "Epoch 136/500, Training Loss: -0.5565\n",
      "Epoch 137/500, Training Loss: -0.5494\n",
      "Epoch 138/500, Training Loss: -0.5919\n",
      "Epoch 139/500, Training Loss: -0.5451\n",
      "Epoch 140/500, Training Loss: -0.5552\n",
      "Epoch 141/500, Training Loss: -0.5594\n",
      "Epoch 142/500, Training Loss: -0.5515\n",
      "Epoch 143/500, Training Loss: -0.5280\n",
      "Epoch 144/500, Training Loss: -0.5425\n",
      "Epoch 145/500, Training Loss: -0.5656\n",
      "Epoch 146/500, Training Loss: -0.5542\n",
      "Epoch 147/500, Training Loss: -0.5758\n",
      "Epoch 148/500, Training Loss: -0.5575\n",
      "Epoch 149/500, Training Loss: -0.5471\n",
      "Epoch 150/500, Training Loss: -0.5742\n",
      "Epoch 151/500, Training Loss: -0.5496\n",
      "Epoch 152/500, Training Loss: -0.5381\n",
      "Epoch 153/500, Training Loss: -0.5585\n",
      "Epoch 154/500, Training Loss: -0.5288\n",
      "Epoch 155/500, Training Loss: -0.5768\n",
      "Epoch 156/500, Training Loss: -0.5532\n",
      "Epoch 157/500, Training Loss: -0.5570\n",
      "Epoch 158/500, Training Loss: -0.5692\n",
      "Epoch 159/500, Training Loss: -0.5444\n",
      "Epoch 160/500, Training Loss: -0.5605\n",
      "Epoch 161/500, Training Loss: -0.5686\n",
      "Epoch 162/500, Training Loss: -0.5375\n",
      "Epoch 163/500, Training Loss: -0.5591\n",
      "Epoch 164/500, Training Loss: -0.5626\n",
      "Epoch 165/500, Training Loss: -0.5467\n",
      "Epoch 166/500, Training Loss: -0.5636\n",
      "Epoch 167/500, Training Loss: -0.5699\n",
      "Epoch 168/500, Training Loss: -0.5478\n",
      "Epoch 169/500, Training Loss: -0.5765\n",
      "Epoch 170/500, Training Loss: -0.5638\n",
      "Epoch 171/500, Training Loss: -0.5582\n",
      "Epoch 172/500, Training Loss: -0.5510\n",
      "Epoch 173/500, Training Loss: -0.5382\n",
      "Epoch 174/500, Training Loss: -0.5258\n",
      "Epoch 175/500, Training Loss: -0.5465\n",
      "Epoch 176/500, Training Loss: -0.5186\n",
      "Epoch 177/500, Training Loss: -0.5717\n",
      "Epoch 178/500, Training Loss: -0.5392\n",
      "Epoch 179/500, Training Loss: -0.5631\n",
      "Epoch 180/500, Training Loss: -0.5716\n",
      "Epoch 181/500, Training Loss: -0.5355\n",
      "Epoch 182/500, Training Loss: -0.5552\n",
      "Epoch 183/500, Training Loss: -0.5235\n",
      "Epoch 184/500, Training Loss: -0.5613\n",
      "Epoch 185/500, Training Loss: -0.5801\n",
      "Epoch 186/500, Training Loss: -0.5802\n",
      "Epoch 187/500, Training Loss: -0.5466\n",
      "Epoch 188/500, Training Loss: -0.5495\n",
      "Epoch 189/500, Training Loss: -0.5361\n",
      "Epoch 190/500, Training Loss: -0.5715\n",
      "Epoch 191/500, Training Loss: -0.5532\n",
      "Epoch 192/500, Training Loss: -0.5636\n",
      "Epoch 193/500, Training Loss: -0.5622\n",
      "Epoch 194/500, Training Loss: -0.5321\n",
      "Epoch 195/500, Training Loss: -0.5754\n",
      "Epoch 196/500, Training Loss: -0.5617\n",
      "Epoch 197/500, Training Loss: -0.5582\n",
      "Epoch 198/500, Training Loss: -0.5705\n",
      "Epoch 199/500, Training Loss: -0.5502\n",
      "Epoch 200/500, Training Loss: -0.5518\n",
      "Epoch 201/500, Training Loss: -0.5397\n",
      "Epoch 202/500, Training Loss: -0.5478\n",
      "Epoch 203/500, Training Loss: -0.5487\n",
      "Epoch 204/500, Training Loss: -0.5588\n",
      "Epoch 205/500, Training Loss: -0.5522\n",
      "Epoch 206/500, Training Loss: -0.5900\n",
      "Epoch 207/500, Training Loss: -0.5566\n",
      "Epoch 208/500, Training Loss: -0.5521\n",
      "Epoch 209/500, Training Loss: -0.5618\n",
      "Epoch 210/500, Training Loss: -0.5592\n",
      "Epoch 211/500, Training Loss: -0.5674\n",
      "Epoch 212/500, Training Loss: -0.5398\n",
      "Epoch 213/500, Training Loss: -0.5514\n",
      "Epoch 214/500, Training Loss: -0.5649\n",
      "Epoch 215/500, Training Loss: -0.5618\n",
      "Epoch 216/500, Training Loss: -0.5605\n",
      "Epoch 217/500, Training Loss: -0.5544\n",
      "Epoch 218/500, Training Loss: -0.5446\n",
      "Epoch 219/500, Training Loss: -0.5293\n",
      "Epoch 220/500, Training Loss: -0.5836\n",
      "Epoch 221/500, Training Loss: -0.5349\n",
      "Epoch 222/500, Training Loss: -0.5461\n",
      "Epoch 223/500, Training Loss: -0.5592\n",
      "Epoch 224/500, Training Loss: -0.5858\n",
      "Epoch 225/500, Training Loss: -0.5439\n",
      "Epoch 226/500, Training Loss: -0.5497\n",
      "Epoch 227/500, Training Loss: -0.5691\n",
      "Epoch 228/500, Training Loss: -0.5636\n",
      "Epoch 229/500, Training Loss: -0.5231\n",
      "Epoch 230/500, Training Loss: -0.5675\n",
      "Epoch 231/500, Training Loss: -0.5401\n",
      "Epoch 232/500, Training Loss: -0.5665\n",
      "Epoch 233/500, Training Loss: -0.5302\n",
      "Epoch 234/500, Training Loss: -0.5807\n",
      "Epoch 235/500, Training Loss: -0.5723\n",
      "Epoch 236/500, Training Loss: -0.5633\n",
      "Epoch 237/500, Training Loss: -0.5654\n",
      "Epoch 238/500, Training Loss: -0.5718\n",
      "Epoch 239/500, Training Loss: -0.5530\n",
      "Epoch 240/500, Training Loss: -0.5582\n",
      "Epoch 241/500, Training Loss: -0.5647\n",
      "Epoch 242/500, Training Loss: -0.5738\n",
      "Epoch 243/500, Training Loss: -0.5739\n",
      "Epoch 244/500, Training Loss: -0.5307\n",
      "Epoch 245/500, Training Loss: -0.5626\n",
      "Epoch 246/500, Training Loss: -0.5591\n",
      "Epoch 247/500, Training Loss: -0.5716\n",
      "Epoch 248/500, Training Loss: -0.5341\n",
      "Epoch 249/500, Training Loss: -0.5468\n",
      "Epoch 250/500, Training Loss: -0.5478\n",
      "Epoch 251/500, Training Loss: -0.5404\n",
      "Epoch 252/500, Training Loss: -0.5413\n",
      "Epoch 253/500, Training Loss: -0.5624\n",
      "Epoch 254/500, Training Loss: -0.5659\n",
      "Epoch 255/500, Training Loss: -0.5629\n",
      "Epoch 256/500, Training Loss: -0.5575\n",
      "Epoch 257/500, Training Loss: -0.5504\n",
      "Epoch 258/500, Training Loss: -0.5469\n",
      "Epoch 259/500, Training Loss: -0.5688\n",
      "Epoch 260/500, Training Loss: -0.5579\n",
      "Epoch 261/500, Training Loss: -0.5706\n",
      "Epoch 262/500, Training Loss: -0.5456\n",
      "Epoch 263/500, Training Loss: -0.5940\n",
      "Epoch 264/500, Training Loss: -0.5207\n",
      "Epoch 265/500, Training Loss: -0.5577\n",
      "Epoch 266/500, Training Loss: -0.5591\n",
      "Epoch 267/500, Training Loss: -0.5679\n",
      "Epoch 268/500, Training Loss: -0.5548\n",
      "Epoch 269/500, Training Loss: -0.5783\n",
      "Epoch 270/500, Training Loss: -0.5710\n",
      "Epoch 271/500, Training Loss: -0.5408\n",
      "Epoch 272/500, Training Loss: -0.5602\n",
      "Epoch 273/500, Training Loss: -0.5939\n",
      "Epoch 274/500, Training Loss: -0.5510\n",
      "Epoch 275/500, Training Loss: -0.5308\n",
      "Epoch 276/500, Training Loss: -0.5662\n",
      "Epoch 277/500, Training Loss: -0.5516\n",
      "Epoch 278/500, Training Loss: -0.5545\n",
      "Epoch 279/500, Training Loss: -0.5830\n",
      "Epoch 280/500, Training Loss: -0.5657\n",
      "Epoch 281/500, Training Loss: -0.5734\n",
      "Epoch 282/500, Training Loss: -0.5616\n",
      "Epoch 283/500, Training Loss: -0.5483\n",
      "Epoch 284/500, Training Loss: -0.5692\n",
      "Epoch 285/500, Training Loss: -0.5603\n",
      "Epoch 286/500, Training Loss: -0.5545\n",
      "Epoch 287/500, Training Loss: -0.5628\n",
      "Epoch 288/500, Training Loss: -0.5159\n",
      "Epoch 289/500, Training Loss: -0.5656\n",
      "Epoch 290/500, Training Loss: -0.5647\n",
      "Epoch 291/500, Training Loss: -0.5901\n",
      "Epoch 292/500, Training Loss: -0.5635\n",
      "Epoch 293/500, Training Loss: -0.5572\n",
      "Epoch 294/500, Training Loss: -0.5615\n",
      "Epoch 295/500, Training Loss: -0.5596\n",
      "Epoch 296/500, Training Loss: -0.5757\n",
      "Epoch 297/500, Training Loss: -0.5294\n",
      "Epoch 298/500, Training Loss: -0.5763\n",
      "Epoch 299/500, Training Loss: -0.5388\n",
      "Epoch 300/500, Training Loss: -0.5595\n",
      "Epoch 301/500, Training Loss: -0.5707\n",
      "Epoch 302/500, Training Loss: -0.5548\n",
      "Epoch 303/500, Training Loss: -0.5510\n",
      "Epoch 304/500, Training Loss: -0.5524\n",
      "Epoch 305/500, Training Loss: -0.5540\n",
      "Epoch 306/500, Training Loss: -0.5440\n",
      "Epoch 307/500, Training Loss: -0.5502\n",
      "Epoch 308/500, Training Loss: -0.5736\n",
      "Epoch 309/500, Training Loss: -0.5566\n",
      "Epoch 310/500, Training Loss: -0.5485\n",
      "Epoch 311/500, Training Loss: -0.5395\n",
      "Epoch 312/500, Training Loss: -0.5779\n",
      "Epoch 313/500, Training Loss: -0.5745\n",
      "Epoch 314/500, Training Loss: -0.5782\n",
      "Epoch 315/500, Training Loss: -0.5485\n",
      "Epoch 316/500, Training Loss: -0.5572\n",
      "Epoch 317/500, Training Loss: -0.5454\n",
      "Epoch 318/500, Training Loss: -0.5800\n",
      "Epoch 319/500, Training Loss: -0.5693\n",
      "Epoch 320/500, Training Loss: -0.5751\n",
      "Epoch 321/500, Training Loss: -0.5668\n",
      "Epoch 322/500, Training Loss: -0.5461\n",
      "Epoch 323/500, Training Loss: -0.5548\n",
      "Epoch 324/500, Training Loss: -0.5638\n",
      "Epoch 325/500, Training Loss: -0.5432\n",
      "Epoch 326/500, Training Loss: -0.5685\n",
      "Epoch 327/500, Training Loss: -0.5481\n",
      "Epoch 328/500, Training Loss: -0.5760\n",
      "Epoch 329/500, Training Loss: -0.5498\n",
      "Epoch 330/500, Training Loss: -0.5427\n",
      "Epoch 331/500, Training Loss: -0.5757\n",
      "Epoch 332/500, Training Loss: -0.5476\n",
      "Epoch 333/500, Training Loss: -0.5619\n",
      "Epoch 334/500, Training Loss: -0.5713\n",
      "Epoch 335/500, Training Loss: -0.5430\n",
      "Epoch 336/500, Training Loss: -0.5558\n",
      "Epoch 337/500, Training Loss: -0.5386\n",
      "Epoch 338/500, Training Loss: -0.5591\n",
      "Epoch 339/500, Training Loss: -0.5494\n",
      "Epoch 340/500, Training Loss: -0.5599\n",
      "Epoch 341/500, Training Loss: -0.5407\n",
      "Epoch 342/500, Training Loss: -0.5148\n",
      "Epoch 343/500, Training Loss: -0.5488\n",
      "Epoch 344/500, Training Loss: -0.5490\n",
      "Epoch 345/500, Training Loss: -0.5645\n",
      "Epoch 346/500, Training Loss: -0.5739\n",
      "Epoch 347/500, Training Loss: -0.5431\n",
      "Epoch 348/500, Training Loss: -0.5653\n",
      "Epoch 349/500, Training Loss: -0.5298\n",
      "Epoch 350/500, Training Loss: -0.5509\n",
      "Epoch 351/500, Training Loss: -0.5804\n",
      "Epoch 352/500, Training Loss: -0.5388\n",
      "Epoch 353/500, Training Loss: -0.5183\n",
      "Epoch 354/500, Training Loss: -0.5387\n",
      "Epoch 355/500, Training Loss: -0.5450\n",
      "Epoch 356/500, Training Loss: -0.5795\n",
      "Epoch 357/500, Training Loss: -0.5644\n",
      "Epoch 358/500, Training Loss: -0.5437\n",
      "Epoch 359/500, Training Loss: -0.5440\n",
      "Epoch 360/500, Training Loss: -0.5501\n",
      "Epoch 361/500, Training Loss: -0.5747\n",
      "Epoch 362/500, Training Loss: -0.5661\n",
      "Epoch 363/500, Training Loss: -0.5474\n",
      "Epoch 364/500, Training Loss: -0.5665\n",
      "Epoch 365/500, Training Loss: -0.5525\n",
      "Epoch 366/500, Training Loss: -0.5644\n",
      "Epoch 367/500, Training Loss: -0.5855\n",
      "Epoch 368/500, Training Loss: -0.5773\n",
      "Epoch 369/500, Training Loss: -0.5645\n",
      "Epoch 370/500, Training Loss: -0.5510\n",
      "Epoch 371/500, Training Loss: -0.5713\n",
      "Epoch 372/500, Training Loss: -0.5380\n",
      "Epoch 373/500, Training Loss: -0.5659\n",
      "Epoch 374/500, Training Loss: -0.5535\n",
      "Epoch 375/500, Training Loss: -0.5683\n",
      "Epoch 376/500, Training Loss: -0.5627\n",
      "Epoch 377/500, Training Loss: -0.5722\n",
      "Epoch 378/500, Training Loss: -0.5531\n",
      "Epoch 379/500, Training Loss: -0.5766\n",
      "Epoch 380/500, Training Loss: -0.5708\n",
      "Epoch 381/500, Training Loss: -0.5598\n",
      "Epoch 382/500, Training Loss: -0.5687\n",
      "Epoch 383/500, Training Loss: -0.5365\n",
      "Epoch 384/500, Training Loss: -0.5731\n",
      "Epoch 385/500, Training Loss: -0.5359\n",
      "Epoch 386/500, Training Loss: -0.5554\n",
      "Epoch 387/500, Training Loss: -0.5457\n",
      "Epoch 388/500, Training Loss: -0.5467\n",
      "Epoch 389/500, Training Loss: -0.5686\n",
      "Epoch 390/500, Training Loss: -0.5727\n",
      "Epoch 391/500, Training Loss: -0.5534\n",
      "Epoch 392/500, Training Loss: -0.5708\n",
      "Epoch 393/500, Training Loss: -0.5342\n",
      "Epoch 394/500, Training Loss: -0.5593\n",
      "Epoch 395/500, Training Loss: -0.5624\n",
      "Epoch 396/500, Training Loss: -0.5336\n",
      "Epoch 397/500, Training Loss: -0.5713\n",
      "Epoch 398/500, Training Loss: -0.5622\n",
      "Epoch 399/500, Training Loss: -0.5607\n",
      "Epoch 400/500, Training Loss: -0.5670\n",
      "Epoch 401/500, Training Loss: -0.5861\n",
      "Epoch 402/500, Training Loss: -0.5639\n",
      "Epoch 403/500, Training Loss: -0.5625\n",
      "Epoch 404/500, Training Loss: -0.5363\n",
      "Epoch 405/500, Training Loss: -0.5651\n",
      "Epoch 406/500, Training Loss: -0.5774\n",
      "Epoch 407/500, Training Loss: -0.5773\n",
      "Epoch 408/500, Training Loss: -0.5585\n",
      "Epoch 409/500, Training Loss: -0.5571\n",
      "Epoch 410/500, Training Loss: -0.5460\n",
      "Epoch 411/500, Training Loss: -0.5560\n",
      "Epoch 412/500, Training Loss: -0.5672\n",
      "Epoch 413/500, Training Loss: -0.5524\n",
      "Epoch 414/500, Training Loss: -0.5590\n",
      "Epoch 415/500, Training Loss: -0.5538\n",
      "Epoch 416/500, Training Loss: -0.5731\n",
      "Epoch 417/500, Training Loss: -0.5529\n",
      "Epoch 418/500, Training Loss: -0.5633\n",
      "Epoch 419/500, Training Loss: -0.5382\n",
      "Epoch 420/500, Training Loss: -0.5382\n",
      "Epoch 421/500, Training Loss: -0.5471\n",
      "Epoch 422/500, Training Loss: -0.5381\n",
      "Epoch 423/500, Training Loss: -0.5916\n",
      "Epoch 424/500, Training Loss: -0.5636\n",
      "Epoch 425/500, Training Loss: -0.5749\n",
      "Epoch 426/500, Training Loss: -0.5436\n",
      "Epoch 427/500, Training Loss: -0.5454\n",
      "Epoch 428/500, Training Loss: -0.5526\n",
      "Epoch 429/500, Training Loss: -0.5772\n",
      "Epoch 430/500, Training Loss: -0.5649\n",
      "Epoch 431/500, Training Loss: -0.5557\n",
      "Epoch 432/500, Training Loss: -0.5511\n",
      "Epoch 433/500, Training Loss: -0.5646\n",
      "Epoch 434/500, Training Loss: -0.5431\n",
      "Epoch 435/500, Training Loss: -0.5705\n",
      "Epoch 436/500, Training Loss: -0.5384\n",
      "Epoch 437/500, Training Loss: -0.5660\n",
      "Epoch 438/500, Training Loss: -0.5552\n",
      "Epoch 439/500, Training Loss: -0.5595\n",
      "Epoch 440/500, Training Loss: -0.5593\n",
      "Epoch 441/500, Training Loss: -0.5492\n",
      "Epoch 442/500, Training Loss: -0.5606\n",
      "Epoch 443/500, Training Loss: -0.5666\n",
      "Epoch 444/500, Training Loss: -0.5398\n",
      "Epoch 445/500, Training Loss: -0.5601\n",
      "Epoch 446/500, Training Loss: -0.5504\n",
      "Epoch 447/500, Training Loss: -0.5525\n",
      "Epoch 448/500, Training Loss: -0.5405\n",
      "Epoch 449/500, Training Loss: -0.5340\n",
      "Epoch 450/500, Training Loss: -0.5619\n",
      "Epoch 451/500, Training Loss: -0.5695\n",
      "Epoch 452/500, Training Loss: -0.5477\n",
      "Epoch 453/500, Training Loss: -0.5590\n",
      "Epoch 454/500, Training Loss: -0.5572\n",
      "Epoch 455/500, Training Loss: -0.5478\n",
      "Epoch 456/500, Training Loss: -0.5582\n",
      "Epoch 457/500, Training Loss: -0.5549\n",
      "Epoch 458/500, Training Loss: -0.5474\n",
      "Epoch 459/500, Training Loss: -0.5487\n",
      "Epoch 460/500, Training Loss: -0.5255\n",
      "Epoch 461/500, Training Loss: -0.5483\n",
      "Epoch 462/500, Training Loss: -0.5966\n",
      "Epoch 463/500, Training Loss: -0.5226\n",
      "Epoch 464/500, Training Loss: -0.5444\n",
      "Epoch 465/500, Training Loss: -0.5706\n",
      "Epoch 466/500, Training Loss: -0.5360\n",
      "Epoch 467/500, Training Loss: -0.5600\n",
      "Epoch 468/500, Training Loss: -0.5473\n",
      "Epoch 469/500, Training Loss: -0.5470\n",
      "Epoch 470/500, Training Loss: -0.5812\n",
      "Epoch 471/500, Training Loss: -0.5673\n",
      "Epoch 472/500, Training Loss: -0.5793\n",
      "Epoch 473/500, Training Loss: -0.5530\n",
      "Epoch 474/500, Training Loss: -0.5511\n",
      "Epoch 475/500, Training Loss: -0.5421\n",
      "Epoch 476/500, Training Loss: -0.5687\n",
      "Epoch 477/500, Training Loss: -0.5589\n",
      "Epoch 478/500, Training Loss: -0.5311\n",
      "Epoch 479/500, Training Loss: -0.5382\n",
      "Epoch 480/500, Training Loss: -0.5763\n",
      "Epoch 481/500, Training Loss: -0.5934\n",
      "Epoch 482/500, Training Loss: -0.5403\n",
      "Epoch 483/500, Training Loss: -0.5502\n",
      "Epoch 484/500, Training Loss: -0.5608\n",
      "Epoch 485/500, Training Loss: -0.5522\n",
      "Epoch 486/500, Training Loss: -0.5375\n",
      "Epoch 487/500, Training Loss: -0.5583\n",
      "Epoch 488/500, Training Loss: -0.5691\n",
      "Epoch 489/500, Training Loss: -0.5802\n",
      "Epoch 490/500, Training Loss: -0.5530\n",
      "Epoch 491/500, Training Loss: -0.5598\n",
      "Epoch 492/500, Training Loss: -0.5327\n",
      "Epoch 493/500, Training Loss: -0.5439\n",
      "Epoch 494/500, Training Loss: -0.5661\n",
      "Epoch 495/500, Training Loss: -0.5478\n",
      "Epoch 496/500, Training Loss: -0.5522\n",
      "Epoch 497/500, Training Loss: -0.5477\n",
      "Epoch 498/500, Training Loss: -0.5850\n",
      "Epoch 499/500, Training Loss: -0.5530\n",
      "Epoch 500/500, Training Loss: -0.5664\n",
      "Best overall model index: 0 with average validation loss -0.5538\n"
     ]
    }
   ],
   "source": [
    "# Training with urr\n",
    "kernel = [gaussian_kernel()] * 2\n",
    "loss_factory = CocycleLossFactory(kernel)\n",
    "loss= loss_factory.build_loss(\"URR\", X, Y, subsamples=10**4)\n",
    "loss_val= loss_factory.build_loss(\"URR_N\", X, Y, subsamples=10**4)\n",
    "final_model_urr, (best_index_urr, val_loss_urr) = validate(\n",
    "        models_urr_gauss,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss_val,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs=hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_urr} with average validation loss {val_loss_urr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c375a88-7755-47ec-9fd8-47ddae5eb72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Training Loss: -0.1647\n",
      "Epoch 2/500, Training Loss: -0.2069\n",
      "Epoch 3/500, Training Loss: -0.2138\n",
      "Epoch 4/500, Training Loss: -0.3055\n",
      "Epoch 5/500, Training Loss: -0.2902\n",
      "Epoch 6/500, Training Loss: -0.3436\n",
      "Epoch 7/500, Training Loss: -0.2951\n",
      "Epoch 8/500, Training Loss: -0.3222\n",
      "Epoch 9/500, Training Loss: -0.3448\n",
      "Epoch 10/500, Training Loss: -0.3523\n",
      "Epoch 11/500, Training Loss: -0.3962\n",
      "Epoch 12/500, Training Loss: -0.4142\n",
      "Epoch 13/500, Training Loss: -0.4054\n",
      "Epoch 14/500, Training Loss: -0.4282\n",
      "Epoch 15/500, Training Loss: -0.4452\n",
      "Epoch 16/500, Training Loss: -0.4808\n",
      "Epoch 17/500, Training Loss: -0.4172\n",
      "Epoch 18/500, Training Loss: -0.5139\n",
      "Epoch 19/500, Training Loss: -0.4656\n",
      "Epoch 20/500, Training Loss: -0.4897\n",
      "Epoch 21/500, Training Loss: -0.5277\n",
      "Epoch 22/500, Training Loss: -0.5335\n",
      "Epoch 23/500, Training Loss: -0.5784\n",
      "Epoch 24/500, Training Loss: -0.5389\n",
      "Epoch 25/500, Training Loss: -0.4829\n",
      "Epoch 26/500, Training Loss: -0.5273\n",
      "Epoch 27/500, Training Loss: -0.5680\n",
      "Epoch 28/500, Training Loss: -0.5353\n",
      "Epoch 29/500, Training Loss: -0.5423\n",
      "Epoch 30/500, Training Loss: -0.5704\n",
      "Epoch 31/500, Training Loss: -0.5799\n",
      "Epoch 32/500, Training Loss: -0.5290\n",
      "Epoch 33/500, Training Loss: -0.5390\n",
      "Epoch 34/500, Training Loss: -0.5747\n",
      "Epoch 35/500, Training Loss: -0.5512\n",
      "Epoch 36/500, Training Loss: -0.5548\n",
      "Epoch 37/500, Training Loss: -0.5168\n",
      "Epoch 38/500, Training Loss: -0.5891\n",
      "Epoch 39/500, Training Loss: -0.5398\n",
      "Epoch 40/500, Training Loss: -0.5751\n",
      "Epoch 41/500, Training Loss: -0.5830\n",
      "Epoch 42/500, Training Loss: -0.5465\n",
      "Epoch 43/500, Training Loss: -0.5583\n",
      "Epoch 44/500, Training Loss: -0.5120\n",
      "Epoch 45/500, Training Loss: -0.5400\n",
      "Epoch 46/500, Training Loss: -0.5795\n",
      "Epoch 47/500, Training Loss: -0.5493\n",
      "Epoch 48/500, Training Loss: -0.5391\n",
      "Epoch 49/500, Training Loss: -0.5691\n",
      "Epoch 50/500, Training Loss: -0.5507\n",
      "Epoch 51/500, Training Loss: -0.5379\n",
      "Epoch 52/500, Training Loss: -0.5231\n",
      "Epoch 53/500, Training Loss: -0.5298\n",
      "Epoch 54/500, Training Loss: -0.5550\n",
      "Epoch 55/500, Training Loss: -0.5466\n",
      "Epoch 56/500, Training Loss: -0.6063\n",
      "Epoch 57/500, Training Loss: -0.5271\n",
      "Epoch 58/500, Training Loss: -0.5838\n",
      "Epoch 59/500, Training Loss: -0.5391\n",
      "Epoch 60/500, Training Loss: -0.5590\n",
      "Epoch 61/500, Training Loss: -0.5715\n",
      "Epoch 62/500, Training Loss: -0.5403\n",
      "Epoch 63/500, Training Loss: -0.5861\n",
      "Epoch 64/500, Training Loss: -0.5550\n",
      "Epoch 65/500, Training Loss: -0.5756\n",
      "Epoch 66/500, Training Loss: -0.5409\n",
      "Epoch 67/500, Training Loss: -0.5664\n",
      "Epoch 68/500, Training Loss: -0.5895\n",
      "Epoch 69/500, Training Loss: -0.5311\n",
      "Epoch 70/500, Training Loss: -0.5460\n",
      "Epoch 71/500, Training Loss: -0.5535\n",
      "Epoch 72/500, Training Loss: -0.5551\n",
      "Epoch 73/500, Training Loss: -0.5389\n",
      "Epoch 74/500, Training Loss: -0.5685\n",
      "Epoch 75/500, Training Loss: -0.6173\n",
      "Epoch 76/500, Training Loss: -0.5534\n",
      "Epoch 77/500, Training Loss: -0.5620\n",
      "Epoch 78/500, Training Loss: -0.5937\n",
      "Epoch 79/500, Training Loss: -0.5536\n",
      "Epoch 80/500, Training Loss: -0.5522\n",
      "Epoch 81/500, Training Loss: -0.5912\n",
      "Epoch 82/500, Training Loss: -0.5505\n",
      "Epoch 83/500, Training Loss: -0.5397\n",
      "Epoch 84/500, Training Loss: -0.5629\n",
      "Epoch 85/500, Training Loss: -0.5660\n",
      "Epoch 86/500, Training Loss: -0.5421\n",
      "Epoch 87/500, Training Loss: -0.5666\n",
      "Epoch 88/500, Training Loss: -0.5644\n",
      "Epoch 89/500, Training Loss: -0.5422\n",
      "Epoch 90/500, Training Loss: -0.5619\n",
      "Epoch 91/500, Training Loss: -0.5610\n",
      "Epoch 92/500, Training Loss: -0.5575\n",
      "Epoch 93/500, Training Loss: -0.5606\n",
      "Epoch 94/500, Training Loss: -0.5468\n",
      "Epoch 95/500, Training Loss: -0.5547\n",
      "Epoch 96/500, Training Loss: -0.5185\n",
      "Epoch 97/500, Training Loss: -0.5315\n",
      "Epoch 98/500, Training Loss: -0.5883\n",
      "Epoch 99/500, Training Loss: -0.5498\n",
      "Epoch 100/500, Training Loss: -0.5750\n",
      "Epoch 101/500, Training Loss: -0.5396\n",
      "Epoch 102/500, Training Loss: -0.6011\n",
      "Epoch 103/500, Training Loss: -0.5149\n",
      "Epoch 104/500, Training Loss: -0.5283\n",
      "Epoch 105/500, Training Loss: -0.5568\n",
      "Epoch 106/500, Training Loss: -0.5357\n",
      "Epoch 107/500, Training Loss: -0.5861\n",
      "Epoch 108/500, Training Loss: -0.5901\n",
      "Epoch 109/500, Training Loss: -0.5114\n",
      "Epoch 110/500, Training Loss: -0.5428\n",
      "Epoch 111/500, Training Loss: -0.5607\n",
      "Epoch 112/500, Training Loss: -0.5536\n",
      "Epoch 113/500, Training Loss: -0.5081\n",
      "Epoch 114/500, Training Loss: -0.5658\n",
      "Epoch 115/500, Training Loss: -0.5616\n",
      "Epoch 116/500, Training Loss: -0.5658\n",
      "Epoch 117/500, Training Loss: -0.5936\n",
      "Epoch 118/500, Training Loss: -0.5479\n",
      "Epoch 119/500, Training Loss: -0.5250\n",
      "Epoch 120/500, Training Loss: -0.5281\n",
      "Epoch 121/500, Training Loss: -0.5623\n",
      "Epoch 122/500, Training Loss: -0.5552\n",
      "Epoch 123/500, Training Loss: -0.5631\n",
      "Epoch 124/500, Training Loss: -0.5249\n",
      "Epoch 125/500, Training Loss: -0.5318\n",
      "Epoch 126/500, Training Loss: -0.5398\n",
      "Epoch 127/500, Training Loss: -0.5734\n",
      "Epoch 128/500, Training Loss: -0.5706\n",
      "Epoch 129/500, Training Loss: -0.5847\n",
      "Epoch 130/500, Training Loss: -0.5444\n",
      "Epoch 131/500, Training Loss: -0.5491\n",
      "Epoch 132/500, Training Loss: -0.5836\n",
      "Epoch 133/500, Training Loss: -0.5503\n",
      "Epoch 134/500, Training Loss: -0.5831\n",
      "Epoch 135/500, Training Loss: -0.5257\n",
      "Epoch 136/500, Training Loss: -0.5549\n",
      "Epoch 137/500, Training Loss: -0.5469\n",
      "Epoch 138/500, Training Loss: -0.5457\n",
      "Epoch 139/500, Training Loss: -0.5359\n",
      "Epoch 140/500, Training Loss: -0.5939\n",
      "Epoch 141/500, Training Loss: -0.5509\n",
      "Epoch 142/500, Training Loss: -0.5384\n",
      "Epoch 143/500, Training Loss: -0.5980\n",
      "Epoch 144/500, Training Loss: -0.5512\n",
      "Epoch 145/500, Training Loss: -0.5521\n",
      "Epoch 146/500, Training Loss: -0.5773\n",
      "Epoch 147/500, Training Loss: -0.5234\n",
      "Epoch 148/500, Training Loss: -0.5648\n",
      "Epoch 149/500, Training Loss: -0.5457\n",
      "Epoch 150/500, Training Loss: -0.5457\n",
      "Epoch 151/500, Training Loss: -0.5777\n",
      "Epoch 152/500, Training Loss: -0.5436\n",
      "Epoch 153/500, Training Loss: -0.5573\n",
      "Epoch 154/500, Training Loss: -0.5689\n",
      "Epoch 155/500, Training Loss: -0.5309\n",
      "Epoch 156/500, Training Loss: -0.5809\n",
      "Epoch 157/500, Training Loss: -0.4889\n",
      "Epoch 158/500, Training Loss: -0.5534\n",
      "Epoch 159/500, Training Loss: -0.5458\n",
      "Epoch 160/500, Training Loss: -0.5600\n",
      "Epoch 161/500, Training Loss: -0.5584\n",
      "Epoch 162/500, Training Loss: -0.6055\n",
      "Epoch 163/500, Training Loss: -0.5596\n",
      "Epoch 164/500, Training Loss: -0.5463\n",
      "Epoch 165/500, Training Loss: -0.5971\n",
      "Epoch 166/500, Training Loss: -0.5665\n",
      "Epoch 167/500, Training Loss: -0.5633\n",
      "Epoch 168/500, Training Loss: -0.5496\n",
      "Epoch 169/500, Training Loss: -0.5394\n",
      "Epoch 170/500, Training Loss: -0.5505\n",
      "Epoch 171/500, Training Loss: -0.5335\n",
      "Epoch 172/500, Training Loss: -0.5357\n",
      "Epoch 173/500, Training Loss: -0.5879\n",
      "Epoch 174/500, Training Loss: -0.5572\n",
      "Epoch 175/500, Training Loss: -0.5518\n",
      "Epoch 176/500, Training Loss: -0.5453\n",
      "Epoch 177/500, Training Loss: -0.5783\n",
      "Epoch 178/500, Training Loss: -0.5715\n",
      "Epoch 179/500, Training Loss: -0.5373\n",
      "Epoch 180/500, Training Loss: -0.5731\n",
      "Epoch 181/500, Training Loss: -0.5422\n",
      "Epoch 182/500, Training Loss: -0.5386\n",
      "Epoch 183/500, Training Loss: -0.5844\n",
      "Epoch 184/500, Training Loss: -0.5713\n",
      "Epoch 185/500, Training Loss: -0.6039\n",
      "Epoch 186/500, Training Loss: -0.5634\n",
      "Epoch 187/500, Training Loss: -0.5850\n",
      "Epoch 188/500, Training Loss: -0.5732\n",
      "Epoch 189/500, Training Loss: -0.5233\n",
      "Epoch 190/500, Training Loss: -0.5421\n",
      "Epoch 191/500, Training Loss: -0.5527\n",
      "Epoch 192/500, Training Loss: -0.5925\n",
      "Epoch 193/500, Training Loss: -0.5597\n",
      "Epoch 194/500, Training Loss: -0.5118\n",
      "Epoch 195/500, Training Loss: -0.5871\n",
      "Epoch 196/500, Training Loss: -0.5672\n",
      "Epoch 197/500, Training Loss: -0.5356\n",
      "Epoch 198/500, Training Loss: -0.5609\n",
      "Epoch 199/500, Training Loss: -0.6045\n",
      "Epoch 200/500, Training Loss: -0.5676\n",
      "Epoch 201/500, Training Loss: -0.5408\n",
      "Epoch 202/500, Training Loss: -0.5658\n",
      "Epoch 203/500, Training Loss: -0.5667\n",
      "Epoch 204/500, Training Loss: -0.5102\n",
      "Epoch 205/500, Training Loss: -0.5648\n",
      "Epoch 206/500, Training Loss: -0.5565\n",
      "Epoch 207/500, Training Loss: -0.5556\n",
      "Epoch 208/500, Training Loss: -0.5442\n",
      "Epoch 209/500, Training Loss: -0.4615\n",
      "Epoch 210/500, Training Loss: -0.5616\n",
      "Epoch 211/500, Training Loss: -0.5443\n",
      "Epoch 212/500, Training Loss: -0.5621\n",
      "Epoch 213/500, Training Loss: -0.5853\n",
      "Epoch 214/500, Training Loss: -0.5128\n",
      "Epoch 215/500, Training Loss: -0.5547\n",
      "Epoch 216/500, Training Loss: -0.5758\n",
      "Epoch 217/500, Training Loss: -0.5752\n",
      "Epoch 218/500, Training Loss: -0.5830\n",
      "Epoch 219/500, Training Loss: -0.5339\n",
      "Epoch 220/500, Training Loss: -0.5562\n",
      "Epoch 221/500, Training Loss: -0.5646\n",
      "Epoch 222/500, Training Loss: -0.5416\n",
      "Epoch 223/500, Training Loss: -0.5713\n",
      "Epoch 224/500, Training Loss: -0.5297\n",
      "Epoch 225/500, Training Loss: -0.5507\n",
      "Epoch 226/500, Training Loss: -0.5592\n",
      "Epoch 227/500, Training Loss: -0.5235\n",
      "Epoch 228/500, Training Loss: -0.5291\n",
      "Epoch 229/500, Training Loss: -0.5287\n",
      "Epoch 230/500, Training Loss: -0.5657\n",
      "Epoch 231/500, Training Loss: -0.5644\n",
      "Epoch 232/500, Training Loss: -0.5481\n",
      "Epoch 233/500, Training Loss: -0.5486\n",
      "Epoch 234/500, Training Loss: -0.5336\n",
      "Epoch 235/500, Training Loss: -0.5828\n",
      "Epoch 236/500, Training Loss: -0.5581\n",
      "Epoch 237/500, Training Loss: -0.5415\n",
      "Epoch 238/500, Training Loss: -0.5597\n",
      "Epoch 239/500, Training Loss: -0.5311\n",
      "Epoch 240/500, Training Loss: -0.5606\n",
      "Epoch 241/500, Training Loss: -0.5805\n",
      "Epoch 242/500, Training Loss: -0.5685\n",
      "Epoch 243/500, Training Loss: -0.5266\n",
      "Epoch 244/500, Training Loss: -0.5207\n",
      "Epoch 245/500, Training Loss: -0.5722\n",
      "Epoch 246/500, Training Loss: -0.5566\n",
      "Epoch 247/500, Training Loss: -0.5443\n",
      "Epoch 248/500, Training Loss: -0.5256\n",
      "Epoch 249/500, Training Loss: -0.5326\n",
      "Epoch 250/500, Training Loss: -0.5530\n",
      "Epoch 251/500, Training Loss: -0.5546\n",
      "Epoch 252/500, Training Loss: -0.6113\n",
      "Epoch 253/500, Training Loss: -0.5570\n",
      "Epoch 254/500, Training Loss: -0.5585\n",
      "Epoch 255/500, Training Loss: -0.5291\n",
      "Epoch 256/500, Training Loss: -0.6076\n",
      "Epoch 257/500, Training Loss: -0.5294\n",
      "Epoch 258/500, Training Loss: -0.5736\n",
      "Epoch 259/500, Training Loss: -0.5847\n",
      "Epoch 260/500, Training Loss: -0.5573\n",
      "Epoch 261/500, Training Loss: -0.5442\n",
      "Epoch 262/500, Training Loss: -0.5572\n",
      "Epoch 263/500, Training Loss: -0.5552\n",
      "Epoch 264/500, Training Loss: -0.5694\n",
      "Epoch 265/500, Training Loss: -0.5539\n",
      "Epoch 266/500, Training Loss: -0.5305\n",
      "Epoch 267/500, Training Loss: -0.5411\n",
      "Epoch 268/500, Training Loss: -0.5590\n",
      "Epoch 269/500, Training Loss: -0.5323\n",
      "Epoch 270/500, Training Loss: -0.5397\n",
      "Epoch 271/500, Training Loss: -0.5694\n",
      "Epoch 272/500, Training Loss: -0.5433\n",
      "Epoch 273/500, Training Loss: -0.5609\n",
      "Epoch 274/500, Training Loss: -0.5728\n",
      "Epoch 275/500, Training Loss: -0.5350\n",
      "Epoch 276/500, Training Loss: -0.5174\n",
      "Epoch 277/500, Training Loss: -0.5962\n",
      "Epoch 278/500, Training Loss: -0.5084\n",
      "Epoch 279/500, Training Loss: -0.5847\n",
      "Epoch 280/500, Training Loss: -0.5883\n",
      "Epoch 281/500, Training Loss: -0.5541\n",
      "Epoch 282/500, Training Loss: -0.5320\n",
      "Epoch 283/500, Training Loss: -0.5855\n",
      "Epoch 284/500, Training Loss: -0.5867\n",
      "Epoch 285/500, Training Loss: -0.5644\n",
      "Epoch 286/500, Training Loss: -0.5784\n",
      "Epoch 287/500, Training Loss: -0.5653\n",
      "Epoch 288/500, Training Loss: -0.5419\n",
      "Epoch 289/500, Training Loss: -0.5316\n",
      "Epoch 290/500, Training Loss: -0.5642\n",
      "Epoch 291/500, Training Loss: -0.5808\n",
      "Epoch 292/500, Training Loss: -0.5844\n",
      "Epoch 293/500, Training Loss: -0.5470\n",
      "Epoch 294/500, Training Loss: -0.5810\n",
      "Epoch 295/500, Training Loss: -0.5691\n",
      "Epoch 296/500, Training Loss: -0.5602\n",
      "Epoch 297/500, Training Loss: -0.5793\n",
      "Epoch 298/500, Training Loss: -0.5617\n",
      "Epoch 299/500, Training Loss: -0.5832\n",
      "Epoch 300/500, Training Loss: -0.5592\n",
      "Epoch 301/500, Training Loss: -0.5978\n",
      "Epoch 302/500, Training Loss: -0.5734\n",
      "Epoch 303/500, Training Loss: -0.5485\n",
      "Epoch 304/500, Training Loss: -0.6033\n",
      "Epoch 305/500, Training Loss: -0.5054\n",
      "Epoch 306/500, Training Loss: -0.5495\n",
      "Epoch 307/500, Training Loss: -0.5490\n",
      "Epoch 308/500, Training Loss: -0.6203\n",
      "Epoch 309/500, Training Loss: -0.5737\n",
      "Epoch 310/500, Training Loss: -0.5282\n",
      "Epoch 311/500, Training Loss: -0.5671\n",
      "Epoch 312/500, Training Loss: -0.5245\n",
      "Epoch 313/500, Training Loss: -0.5297\n",
      "Epoch 314/500, Training Loss: -0.5544\n",
      "Epoch 315/500, Training Loss: -0.5680\n",
      "Epoch 316/500, Training Loss: -0.5476\n",
      "Epoch 317/500, Training Loss: -0.5494\n",
      "Epoch 318/500, Training Loss: -0.6117\n",
      "Epoch 319/500, Training Loss: -0.5675\n",
      "Epoch 320/500, Training Loss: -0.5296\n",
      "Epoch 321/500, Training Loss: -0.5605\n",
      "Epoch 322/500, Training Loss: -0.5813\n",
      "Epoch 323/500, Training Loss: -0.5244\n",
      "Epoch 324/500, Training Loss: -0.5960\n",
      "Epoch 325/500, Training Loss: -0.5767\n",
      "Epoch 326/500, Training Loss: -0.5503\n",
      "Epoch 327/500, Training Loss: -0.5811\n",
      "Epoch 328/500, Training Loss: -0.5329\n",
      "Epoch 329/500, Training Loss: -0.5594\n",
      "Epoch 330/500, Training Loss: -0.5647\n",
      "Epoch 331/500, Training Loss: -0.5534\n",
      "Epoch 332/500, Training Loss: -0.5174\n",
      "Epoch 333/500, Training Loss: -0.5787\n",
      "Epoch 334/500, Training Loss: -0.5596\n",
      "Epoch 335/500, Training Loss: -0.5999\n",
      "Epoch 336/500, Training Loss: -0.5213\n",
      "Epoch 337/500, Training Loss: -0.5934\n",
      "Epoch 338/500, Training Loss: -0.5265\n",
      "Epoch 339/500, Training Loss: -0.5714\n",
      "Epoch 340/500, Training Loss: -0.5488\n",
      "Epoch 341/500, Training Loss: -0.5489\n",
      "Epoch 342/500, Training Loss: -0.5286\n",
      "Epoch 343/500, Training Loss: -0.5277\n",
      "Epoch 344/500, Training Loss: -0.5478\n",
      "Epoch 345/500, Training Loss: -0.5508\n",
      "Epoch 346/500, Training Loss: -0.5572\n",
      "Epoch 347/500, Training Loss: -0.5115\n",
      "Epoch 348/500, Training Loss: -0.5788\n",
      "Epoch 349/500, Training Loss: -0.5721\n",
      "Epoch 350/500, Training Loss: -0.5239\n",
      "Epoch 351/500, Training Loss: -0.5896\n",
      "Epoch 352/500, Training Loss: -0.5396\n",
      "Epoch 353/500, Training Loss: -0.5916\n",
      "Epoch 354/500, Training Loss: -0.5726\n",
      "Epoch 355/500, Training Loss: -0.5817\n",
      "Epoch 356/500, Training Loss: -0.6100\n",
      "Epoch 357/500, Training Loss: -0.5658\n",
      "Epoch 358/500, Training Loss: -0.5640\n",
      "Epoch 359/500, Training Loss: -0.5639\n",
      "Epoch 360/500, Training Loss: -0.5611\n",
      "Epoch 361/500, Training Loss: -0.5726\n",
      "Epoch 362/500, Training Loss: -0.5482\n",
      "Epoch 363/500, Training Loss: -0.5942\n",
      "Epoch 364/500, Training Loss: -0.5773\n",
      "Epoch 365/500, Training Loss: -0.5631\n",
      "Epoch 366/500, Training Loss: -0.5233\n",
      "Epoch 367/500, Training Loss: -0.5805\n",
      "Epoch 368/500, Training Loss: -0.5735\n",
      "Epoch 369/500, Training Loss: -0.5845\n",
      "Epoch 370/500, Training Loss: -0.5336\n",
      "Epoch 371/500, Training Loss: -0.5631\n",
      "Epoch 372/500, Training Loss: -0.5728\n",
      "Epoch 373/500, Training Loss: -0.5813\n",
      "Epoch 374/500, Training Loss: -0.5544\n",
      "Epoch 375/500, Training Loss: -0.5688\n",
      "Epoch 376/500, Training Loss: -0.5561\n",
      "Epoch 377/500, Training Loss: -0.5518\n",
      "Epoch 378/500, Training Loss: -0.5016\n",
      "Epoch 379/500, Training Loss: -0.5685\n",
      "Epoch 380/500, Training Loss: -0.5583\n",
      "Epoch 381/500, Training Loss: -0.5593\n",
      "Epoch 382/500, Training Loss: -0.5242\n",
      "Epoch 383/500, Training Loss: -0.5816\n",
      "Epoch 384/500, Training Loss: -0.5375\n",
      "Epoch 385/500, Training Loss: -0.5577\n",
      "Epoch 386/500, Training Loss: -0.5829\n",
      "Epoch 387/500, Training Loss: -0.5579\n",
      "Epoch 388/500, Training Loss: -0.5506\n",
      "Epoch 389/500, Training Loss: -0.5801\n",
      "Epoch 390/500, Training Loss: -0.5786\n",
      "Epoch 391/500, Training Loss: -0.5490\n",
      "Epoch 392/500, Training Loss: -0.5276\n",
      "Epoch 393/500, Training Loss: -0.5534\n",
      "Epoch 394/500, Training Loss: -0.5620\n",
      "Epoch 395/500, Training Loss: -0.5533\n",
      "Epoch 396/500, Training Loss: -0.5604\n",
      "Epoch 397/500, Training Loss: -0.5498\n",
      "Epoch 398/500, Training Loss: -0.5196\n",
      "Epoch 399/500, Training Loss: -0.5465\n",
      "Epoch 400/500, Training Loss: -0.5714\n",
      "Epoch 401/500, Training Loss: -0.5590\n",
      "Epoch 402/500, Training Loss: -0.5521\n",
      "Epoch 403/500, Training Loss: -0.5839\n",
      "Epoch 404/500, Training Loss: -0.5479\n",
      "Epoch 405/500, Training Loss: -0.5633\n",
      "Epoch 406/500, Training Loss: -0.5121\n",
      "Epoch 407/500, Training Loss: -0.5387\n",
      "Epoch 408/500, Training Loss: -0.5394\n",
      "Epoch 409/500, Training Loss: -0.5551\n",
      "Epoch 410/500, Training Loss: -0.5310\n",
      "Epoch 411/500, Training Loss: -0.5697\n",
      "Epoch 412/500, Training Loss: -0.5370\n",
      "Epoch 413/500, Training Loss: -0.5595\n",
      "Epoch 414/500, Training Loss: -0.5645\n",
      "Epoch 415/500, Training Loss: -0.5958\n",
      "Epoch 416/500, Training Loss: -0.5574\n",
      "Epoch 417/500, Training Loss: -0.5653\n",
      "Epoch 418/500, Training Loss: -0.5590\n",
      "Epoch 419/500, Training Loss: -0.5714\n",
      "Epoch 420/500, Training Loss: -0.5901\n",
      "Epoch 421/500, Training Loss: -0.4886\n",
      "Epoch 422/500, Training Loss: -0.5465\n",
      "Epoch 423/500, Training Loss: -0.5602\n",
      "Epoch 424/500, Training Loss: -0.5663\n",
      "Epoch 425/500, Training Loss: -0.5560\n",
      "Epoch 426/500, Training Loss: -0.5304\n",
      "Epoch 427/500, Training Loss: -0.5548\n",
      "Epoch 428/500, Training Loss: -0.5664\n",
      "Epoch 429/500, Training Loss: -0.5873\n",
      "Epoch 430/500, Training Loss: -0.5778\n",
      "Epoch 431/500, Training Loss: -0.5373\n",
      "Epoch 432/500, Training Loss: -0.5477\n",
      "Epoch 433/500, Training Loss: -0.5167\n",
      "Epoch 434/500, Training Loss: -0.5587\n",
      "Epoch 435/500, Training Loss: -0.5303\n",
      "Epoch 436/500, Training Loss: -0.5323\n",
      "Epoch 437/500, Training Loss: -0.5248\n",
      "Epoch 438/500, Training Loss: -0.5470\n",
      "Epoch 439/500, Training Loss: -0.5908\n",
      "Epoch 440/500, Training Loss: -0.5178\n",
      "Epoch 441/500, Training Loss: -0.5754\n",
      "Epoch 442/500, Training Loss: -0.5623\n",
      "Epoch 443/500, Training Loss: -0.5679\n",
      "Epoch 444/500, Training Loss: -0.5554\n",
      "Epoch 445/500, Training Loss: -0.5918\n",
      "Epoch 446/500, Training Loss: -0.5574\n",
      "Epoch 447/500, Training Loss: -0.5843\n",
      "Epoch 448/500, Training Loss: -0.5878\n",
      "Epoch 449/500, Training Loss: -0.5725\n",
      "Epoch 450/500, Training Loss: -0.5564\n",
      "Epoch 451/500, Training Loss: -0.5209\n",
      "Epoch 452/500, Training Loss: -0.5314\n",
      "Epoch 453/500, Training Loss: -0.5587\n",
      "Epoch 454/500, Training Loss: -0.5690\n",
      "Epoch 455/500, Training Loss: -0.5703\n",
      "Epoch 456/500, Training Loss: -0.5282\n",
      "Epoch 457/500, Training Loss: -0.5509\n",
      "Epoch 458/500, Training Loss: -0.5771\n",
      "Epoch 459/500, Training Loss: -0.5715\n",
      "Epoch 460/500, Training Loss: -0.5959\n",
      "Epoch 461/500, Training Loss: -0.5250\n",
      "Epoch 462/500, Training Loss: -0.5717\n",
      "Epoch 463/500, Training Loss: -0.5485\n",
      "Epoch 464/500, Training Loss: -0.5608\n",
      "Epoch 465/500, Training Loss: -0.5709\n",
      "Epoch 466/500, Training Loss: -0.5862\n",
      "Epoch 467/500, Training Loss: -0.5917\n",
      "Epoch 468/500, Training Loss: -0.5429\n",
      "Epoch 469/500, Training Loss: -0.5787\n",
      "Epoch 470/500, Training Loss: -0.5375\n",
      "Epoch 471/500, Training Loss: -0.5579\n",
      "Epoch 472/500, Training Loss: -0.5592\n",
      "Epoch 473/500, Training Loss: -0.5607\n",
      "Epoch 474/500, Training Loss: -0.5653\n",
      "Epoch 475/500, Training Loss: -0.5270\n",
      "Epoch 476/500, Training Loss: -0.5833\n",
      "Epoch 477/500, Training Loss: -0.5617\n",
      "Epoch 478/500, Training Loss: -0.5552\n",
      "Epoch 479/500, Training Loss: -0.5431\n",
      "Epoch 480/500, Training Loss: -0.5823\n",
      "Epoch 481/500, Training Loss: -0.5474\n",
      "Epoch 482/500, Training Loss: -0.5795\n",
      "Epoch 483/500, Training Loss: -0.5799\n",
      "Epoch 484/500, Training Loss: -0.5805\n",
      "Epoch 485/500, Training Loss: -0.5479\n",
      "Epoch 486/500, Training Loss: -0.5475\n",
      "Epoch 487/500, Training Loss: -0.5364\n",
      "Epoch 488/500, Training Loss: -0.5524\n",
      "Epoch 489/500, Training Loss: -0.5416\n",
      "Epoch 490/500, Training Loss: -0.5576\n",
      "Epoch 491/500, Training Loss: -0.5306\n",
      "Epoch 492/500, Training Loss: -0.5541\n",
      "Epoch 493/500, Training Loss: -0.5782\n",
      "Epoch 494/500, Training Loss: -0.5621\n",
      "Epoch 495/500, Training Loss: -0.5602\n",
      "Epoch 496/500, Training Loss: -0.5898\n",
      "Epoch 497/500, Training Loss: -0.5775\n",
      "Epoch 498/500, Training Loss: -0.5456\n",
      "Epoch 499/500, Training Loss: -0.5960\n",
      "Epoch 500/500, Training Loss: -0.5394\n",
      "Validation Loss: -0.5613\n",
      "Model 0, Fold 0: Validation Loss = -0.5613\n",
      "Epoch 1/500, Training Loss: -0.1914\n",
      "Epoch 2/500, Training Loss: -0.2162\n",
      "Epoch 3/500, Training Loss: -0.2308\n",
      "Epoch 4/500, Training Loss: -0.2538\n",
      "Epoch 5/500, Training Loss: -0.2811\n",
      "Epoch 6/500, Training Loss: -0.3060\n",
      "Epoch 7/500, Training Loss: -0.3030\n",
      "Epoch 8/500, Training Loss: -0.2761\n",
      "Epoch 9/500, Training Loss: -0.2990\n",
      "Epoch 10/500, Training Loss: -0.3278\n",
      "Epoch 11/500, Training Loss: -0.3894\n",
      "Epoch 12/500, Training Loss: -0.3136\n",
      "Epoch 13/500, Training Loss: -0.4260\n",
      "Epoch 14/500, Training Loss: -0.4074\n",
      "Epoch 15/500, Training Loss: -0.4603\n",
      "Epoch 16/500, Training Loss: -0.3996\n",
      "Epoch 17/500, Training Loss: -0.5056\n",
      "Epoch 18/500, Training Loss: -0.5046\n",
      "Epoch 19/500, Training Loss: -0.4655\n",
      "Epoch 20/500, Training Loss: -0.5383\n",
      "Epoch 21/500, Training Loss: -0.4726\n",
      "Epoch 22/500, Training Loss: -0.5539\n",
      "Epoch 23/500, Training Loss: -0.4993\n",
      "Epoch 24/500, Training Loss: -0.5118\n",
      "Epoch 25/500, Training Loss: -0.5179\n",
      "Epoch 26/500, Training Loss: -0.5243\n",
      "Epoch 27/500, Training Loss: -0.5466\n",
      "Epoch 28/500, Training Loss: -0.5330\n",
      "Epoch 29/500, Training Loss: -0.5137\n",
      "Epoch 30/500, Training Loss: -0.5536\n",
      "Epoch 31/500, Training Loss: -0.5511\n",
      "Epoch 32/500, Training Loss: -0.5690\n",
      "Epoch 33/500, Training Loss: -0.5669\n",
      "Epoch 34/500, Training Loss: -0.5268\n",
      "Epoch 35/500, Training Loss: -0.5567\n",
      "Epoch 36/500, Training Loss: -0.5277\n",
      "Epoch 37/500, Training Loss: -0.5544\n",
      "Epoch 38/500, Training Loss: -0.5516\n",
      "Epoch 39/500, Training Loss: -0.5710\n",
      "Epoch 40/500, Training Loss: -0.5742\n",
      "Epoch 41/500, Training Loss: -0.5075\n",
      "Epoch 42/500, Training Loss: -0.5716\n",
      "Epoch 43/500, Training Loss: -0.5441\n",
      "Epoch 44/500, Training Loss: -0.5350\n",
      "Epoch 45/500, Training Loss: -0.5283\n",
      "Epoch 46/500, Training Loss: -0.5082\n",
      "Epoch 47/500, Training Loss: -0.4888\n",
      "Epoch 48/500, Training Loss: -0.4950\n",
      "Epoch 49/500, Training Loss: -0.5278\n",
      "Epoch 50/500, Training Loss: -0.5609\n",
      "Epoch 51/500, Training Loss: -0.5397\n",
      "Epoch 52/500, Training Loss: -0.5455\n",
      "Epoch 53/500, Training Loss: -0.5364\n",
      "Epoch 54/500, Training Loss: -0.5400\n",
      "Epoch 55/500, Training Loss: -0.5125\n",
      "Epoch 56/500, Training Loss: -0.5498\n",
      "Epoch 57/500, Training Loss: -0.5547\n",
      "Epoch 58/500, Training Loss: -0.5683\n",
      "Epoch 59/500, Training Loss: -0.5536\n",
      "Epoch 60/500, Training Loss: -0.5297\n",
      "Epoch 61/500, Training Loss: -0.5351\n",
      "Epoch 62/500, Training Loss: -0.5361\n",
      "Epoch 63/500, Training Loss: -0.5611\n",
      "Epoch 64/500, Training Loss: -0.5115\n",
      "Epoch 65/500, Training Loss: -0.5669\n",
      "Epoch 66/500, Training Loss: -0.5436\n",
      "Epoch 67/500, Training Loss: -0.5305\n",
      "Epoch 68/500, Training Loss: -0.5568\n",
      "Epoch 69/500, Training Loss: -0.5098\n",
      "Epoch 70/500, Training Loss: -0.5203\n",
      "Epoch 71/500, Training Loss: -0.5348\n",
      "Epoch 72/500, Training Loss: -0.5302\n",
      "Epoch 73/500, Training Loss: -0.5435\n",
      "Epoch 74/500, Training Loss: -0.5704\n",
      "Epoch 75/500, Training Loss: -0.5084\n",
      "Epoch 76/500, Training Loss: -0.5843\n",
      "Epoch 77/500, Training Loss: -0.5392\n",
      "Epoch 78/500, Training Loss: -0.5422\n",
      "Epoch 79/500, Training Loss: -0.5349\n",
      "Epoch 80/500, Training Loss: -0.5370\n",
      "Epoch 81/500, Training Loss: -0.5244\n",
      "Epoch 82/500, Training Loss: -0.5604\n",
      "Epoch 83/500, Training Loss: -0.5161\n",
      "Epoch 84/500, Training Loss: -0.5646\n",
      "Epoch 85/500, Training Loss: -0.5356\n",
      "Epoch 86/500, Training Loss: -0.5325\n",
      "Epoch 87/500, Training Loss: -0.5513\n",
      "Epoch 88/500, Training Loss: -0.5612\n",
      "Epoch 89/500, Training Loss: -0.5170\n",
      "Epoch 90/500, Training Loss: -0.5065\n",
      "Epoch 91/500, Training Loss: -0.5587\n",
      "Epoch 92/500, Training Loss: -0.4951\n",
      "Epoch 93/500, Training Loss: -0.5553\n",
      "Epoch 94/500, Training Loss: -0.5477\n",
      "Epoch 95/500, Training Loss: -0.5860\n",
      "Epoch 96/500, Training Loss: -0.5392\n",
      "Epoch 97/500, Training Loss: -0.5485\n",
      "Epoch 98/500, Training Loss: -0.5266\n",
      "Epoch 99/500, Training Loss: -0.5607\n",
      "Epoch 100/500, Training Loss: -0.5585\n",
      "Epoch 101/500, Training Loss: -0.5210\n",
      "Epoch 102/500, Training Loss: -0.5329\n",
      "Epoch 103/500, Training Loss: -0.5348\n",
      "Epoch 104/500, Training Loss: -0.5114\n",
      "Epoch 105/500, Training Loss: -0.5526\n",
      "Epoch 106/500, Training Loss: -0.5447\n",
      "Epoch 107/500, Training Loss: -0.5529\n",
      "Epoch 108/500, Training Loss: -0.5441\n",
      "Epoch 109/500, Training Loss: -0.5134\n",
      "Epoch 110/500, Training Loss: -0.5245\n",
      "Epoch 111/500, Training Loss: -0.5220\n",
      "Epoch 112/500, Training Loss: -0.5301\n",
      "Epoch 113/500, Training Loss: -0.5224\n",
      "Epoch 114/500, Training Loss: -0.5043\n",
      "Epoch 115/500, Training Loss: -0.5128\n",
      "Epoch 116/500, Training Loss: -0.5566\n",
      "Epoch 117/500, Training Loss: -0.5126\n",
      "Epoch 118/500, Training Loss: -0.5473\n",
      "Epoch 119/500, Training Loss: -0.5790\n",
      "Epoch 120/500, Training Loss: -0.5655\n",
      "Epoch 121/500, Training Loss: -0.6027\n",
      "Epoch 122/500, Training Loss: -0.5299\n",
      "Epoch 123/500, Training Loss: -0.5674\n",
      "Epoch 124/500, Training Loss: -0.5626\n",
      "Epoch 125/500, Training Loss: -0.5690\n",
      "Epoch 126/500, Training Loss: -0.5351\n",
      "Epoch 127/500, Training Loss: -0.5246\n",
      "Epoch 128/500, Training Loss: -0.5491\n",
      "Epoch 129/500, Training Loss: -0.5190\n",
      "Epoch 130/500, Training Loss: -0.5689\n",
      "Epoch 131/500, Training Loss: -0.5453\n",
      "Epoch 132/500, Training Loss: -0.5230\n",
      "Epoch 133/500, Training Loss: -0.5324\n",
      "Epoch 134/500, Training Loss: -0.5649\n",
      "Epoch 135/500, Training Loss: -0.5582\n",
      "Epoch 136/500, Training Loss: -0.5548\n",
      "Epoch 137/500, Training Loss: -0.5576\n",
      "Epoch 138/500, Training Loss: -0.5597\n",
      "Epoch 139/500, Training Loss: -0.5902\n",
      "Epoch 140/500, Training Loss: -0.5454\n",
      "Epoch 141/500, Training Loss: -0.5500\n",
      "Epoch 142/500, Training Loss: -0.5090\n",
      "Epoch 143/500, Training Loss: -0.5166\n",
      "Epoch 144/500, Training Loss: -0.5417\n",
      "Epoch 145/500, Training Loss: -0.5529\n",
      "Epoch 146/500, Training Loss: -0.5759\n",
      "Epoch 147/500, Training Loss: -0.5297\n",
      "Epoch 148/500, Training Loss: -0.5657\n",
      "Epoch 149/500, Training Loss: -0.4929\n",
      "Epoch 150/500, Training Loss: -0.5111\n",
      "Epoch 151/500, Training Loss: -0.4899\n",
      "Epoch 152/500, Training Loss: -0.5362\n",
      "Epoch 153/500, Training Loss: -0.5457\n",
      "Epoch 154/500, Training Loss: -0.5338\n",
      "Epoch 155/500, Training Loss: -0.5579\n",
      "Epoch 156/500, Training Loss: -0.5747\n",
      "Epoch 157/500, Training Loss: -0.5529\n",
      "Epoch 158/500, Training Loss: -0.5379\n",
      "Epoch 159/500, Training Loss: -0.5392\n",
      "Epoch 160/500, Training Loss: -0.5248\n",
      "Epoch 161/500, Training Loss: -0.5842\n",
      "Epoch 162/500, Training Loss: -0.5300\n",
      "Epoch 163/500, Training Loss: -0.5626\n",
      "Epoch 164/500, Training Loss: -0.5310\n",
      "Epoch 165/500, Training Loss: -0.5262\n",
      "Epoch 166/500, Training Loss: -0.4815\n",
      "Epoch 167/500, Training Loss: -0.5852\n",
      "Epoch 168/500, Training Loss: -0.5781\n",
      "Epoch 169/500, Training Loss: -0.5427\n",
      "Epoch 170/500, Training Loss: -0.5352\n",
      "Epoch 171/500, Training Loss: -0.5494\n",
      "Epoch 172/500, Training Loss: -0.5164\n",
      "Epoch 173/500, Training Loss: -0.5361\n",
      "Epoch 174/500, Training Loss: -0.5165\n",
      "Epoch 175/500, Training Loss: -0.5512\n",
      "Epoch 176/500, Training Loss: -0.5661\n",
      "Epoch 177/500, Training Loss: -0.5747\n",
      "Epoch 178/500, Training Loss: -0.5129\n",
      "Epoch 179/500, Training Loss: -0.5388\n",
      "Epoch 180/500, Training Loss: -0.5662\n",
      "Epoch 181/500, Training Loss: -0.4999\n",
      "Epoch 182/500, Training Loss: -0.5445\n",
      "Epoch 183/500, Training Loss: -0.5426\n",
      "Epoch 184/500, Training Loss: -0.5647\n",
      "Epoch 185/500, Training Loss: -0.5549\n",
      "Epoch 186/500, Training Loss: -0.5601\n",
      "Epoch 187/500, Training Loss: -0.5186\n",
      "Epoch 188/500, Training Loss: -0.5369\n",
      "Epoch 189/500, Training Loss: -0.5354\n",
      "Epoch 190/500, Training Loss: -0.5445\n",
      "Epoch 191/500, Training Loss: -0.5455\n",
      "Epoch 192/500, Training Loss: -0.5526\n",
      "Epoch 193/500, Training Loss: -0.5570\n",
      "Epoch 194/500, Training Loss: -0.5202\n",
      "Epoch 195/500, Training Loss: -0.5447\n",
      "Epoch 196/500, Training Loss: -0.5855\n",
      "Epoch 197/500, Training Loss: -0.5093\n",
      "Epoch 198/500, Training Loss: -0.5166\n",
      "Epoch 199/500, Training Loss: -0.5360\n",
      "Epoch 200/500, Training Loss: -0.5418\n",
      "Epoch 201/500, Training Loss: -0.5287\n",
      "Epoch 202/500, Training Loss: -0.5510\n",
      "Epoch 203/500, Training Loss: -0.5198\n",
      "Epoch 204/500, Training Loss: -0.5356\n",
      "Epoch 205/500, Training Loss: -0.5625\n",
      "Epoch 206/500, Training Loss: -0.5363\n",
      "Epoch 207/500, Training Loss: -0.5371\n",
      "Epoch 208/500, Training Loss: -0.5813\n",
      "Epoch 209/500, Training Loss: -0.5099\n",
      "Epoch 210/500, Training Loss: -0.5250\n",
      "Epoch 211/500, Training Loss: -0.5568\n",
      "Epoch 212/500, Training Loss: -0.5860\n",
      "Epoch 213/500, Training Loss: -0.5221\n",
      "Epoch 214/500, Training Loss: -0.5272\n",
      "Epoch 215/500, Training Loss: -0.5458\n",
      "Epoch 216/500, Training Loss: -0.5244\n",
      "Epoch 217/500, Training Loss: -0.5469\n",
      "Epoch 218/500, Training Loss: -0.5522\n",
      "Epoch 219/500, Training Loss: -0.5456\n",
      "Epoch 220/500, Training Loss: -0.5919\n",
      "Epoch 221/500, Training Loss: -0.5256\n",
      "Epoch 222/500, Training Loss: -0.5589\n",
      "Epoch 223/500, Training Loss: -0.5765\n",
      "Epoch 224/500, Training Loss: -0.5423\n",
      "Epoch 225/500, Training Loss: -0.5324\n",
      "Epoch 226/500, Training Loss: -0.5646\n",
      "Epoch 227/500, Training Loss: -0.5334\n",
      "Epoch 228/500, Training Loss: -0.5626\n",
      "Epoch 229/500, Training Loss: -0.5648\n",
      "Epoch 230/500, Training Loss: -0.5184\n",
      "Epoch 231/500, Training Loss: -0.5999\n",
      "Epoch 232/500, Training Loss: -0.5529\n",
      "Epoch 233/500, Training Loss: -0.5739\n",
      "Epoch 234/500, Training Loss: -0.5165\n",
      "Epoch 235/500, Training Loss: -0.5433\n",
      "Epoch 236/500, Training Loss: -0.5694\n",
      "Epoch 237/500, Training Loss: -0.5404\n",
      "Epoch 238/500, Training Loss: -0.5473\n",
      "Epoch 239/500, Training Loss: -0.5480\n",
      "Epoch 240/500, Training Loss: -0.5541\n",
      "Epoch 241/500, Training Loss: -0.5257\n",
      "Epoch 242/500, Training Loss: -0.5526\n",
      "Epoch 243/500, Training Loss: -0.5494\n",
      "Epoch 244/500, Training Loss: -0.5428\n",
      "Epoch 245/500, Training Loss: -0.5275\n",
      "Epoch 246/500, Training Loss: -0.5064\n",
      "Epoch 247/500, Training Loss: -0.5277\n",
      "Epoch 248/500, Training Loss: -0.5288\n",
      "Epoch 249/500, Training Loss: -0.5451\n",
      "Epoch 250/500, Training Loss: -0.5515\n",
      "Epoch 251/500, Training Loss: -0.5275\n",
      "Epoch 252/500, Training Loss: -0.5411\n",
      "Epoch 253/500, Training Loss: -0.5147\n",
      "Epoch 254/500, Training Loss: -0.5505\n",
      "Epoch 255/500, Training Loss: -0.5392\n",
      "Epoch 256/500, Training Loss: -0.5377\n",
      "Epoch 257/500, Training Loss: -0.5487\n",
      "Epoch 258/500, Training Loss: -0.5225\n",
      "Epoch 259/500, Training Loss: -0.5738\n",
      "Epoch 260/500, Training Loss: -0.5428\n",
      "Epoch 261/500, Training Loss: -0.5375\n",
      "Epoch 262/500, Training Loss: -0.5454\n",
      "Epoch 263/500, Training Loss: -0.5679\n",
      "Epoch 264/500, Training Loss: -0.5261\n",
      "Epoch 265/500, Training Loss: -0.5403\n",
      "Epoch 266/500, Training Loss: -0.5621\n",
      "Epoch 267/500, Training Loss: -0.5231\n",
      "Epoch 268/500, Training Loss: -0.5625\n",
      "Epoch 269/500, Training Loss: -0.5548\n",
      "Epoch 270/500, Training Loss: -0.5568\n",
      "Epoch 271/500, Training Loss: -0.5367\n",
      "Epoch 272/500, Training Loss: -0.5559\n",
      "Epoch 273/500, Training Loss: -0.5154\n",
      "Epoch 274/500, Training Loss: -0.5320\n",
      "Epoch 275/500, Training Loss: -0.5552\n",
      "Epoch 276/500, Training Loss: -0.5484\n",
      "Epoch 277/500, Training Loss: -0.5474\n",
      "Epoch 278/500, Training Loss: -0.5323\n",
      "Epoch 279/500, Training Loss: -0.5630\n",
      "Epoch 280/500, Training Loss: -0.5276\n",
      "Epoch 281/500, Training Loss: -0.5537\n",
      "Epoch 282/500, Training Loss: -0.5410\n",
      "Epoch 283/500, Training Loss: -0.5555\n",
      "Epoch 284/500, Training Loss: -0.5425\n",
      "Epoch 285/500, Training Loss: -0.5763\n",
      "Epoch 286/500, Training Loss: -0.5685\n",
      "Epoch 287/500, Training Loss: -0.5248\n",
      "Epoch 288/500, Training Loss: -0.5412\n",
      "Epoch 289/500, Training Loss: -0.5773\n",
      "Epoch 290/500, Training Loss: -0.5193\n",
      "Epoch 291/500, Training Loss: -0.5759\n",
      "Epoch 292/500, Training Loss: -0.5112\n",
      "Epoch 293/500, Training Loss: -0.5389\n",
      "Epoch 294/500, Training Loss: -0.5420\n",
      "Epoch 295/500, Training Loss: -0.5104\n",
      "Epoch 296/500, Training Loss: -0.5511\n",
      "Epoch 297/500, Training Loss: -0.5618\n",
      "Epoch 298/500, Training Loss: -0.5227\n",
      "Epoch 299/500, Training Loss: -0.5486\n",
      "Epoch 300/500, Training Loss: -0.5165\n",
      "Epoch 301/500, Training Loss: -0.5334\n",
      "Epoch 302/500, Training Loss: -0.5116\n",
      "Epoch 303/500, Training Loss: -0.5711\n",
      "Epoch 304/500, Training Loss: -0.5656\n",
      "Epoch 305/500, Training Loss: -0.5794\n",
      "Epoch 306/500, Training Loss: -0.5495\n",
      "Epoch 307/500, Training Loss: -0.5370\n",
      "Epoch 308/500, Training Loss: -0.5301\n",
      "Epoch 309/500, Training Loss: -0.5378\n",
      "Epoch 310/500, Training Loss: -0.5504\n",
      "Epoch 311/500, Training Loss: -0.5400\n",
      "Epoch 312/500, Training Loss: -0.5450\n",
      "Epoch 313/500, Training Loss: -0.5541\n",
      "Epoch 314/500, Training Loss: -0.5194\n",
      "Epoch 315/500, Training Loss: -0.5487\n",
      "Epoch 316/500, Training Loss: -0.5304\n",
      "Epoch 317/500, Training Loss: -0.5015\n",
      "Epoch 318/500, Training Loss: -0.5272\n",
      "Epoch 319/500, Training Loss: -0.5534\n",
      "Epoch 320/500, Training Loss: -0.5242\n",
      "Epoch 321/500, Training Loss: -0.5191\n",
      "Epoch 322/500, Training Loss: -0.5752\n",
      "Epoch 323/500, Training Loss: -0.5560\n",
      "Epoch 324/500, Training Loss: -0.6227\n",
      "Epoch 325/500, Training Loss: -0.5485\n",
      "Epoch 326/500, Training Loss: -0.5578\n",
      "Epoch 327/500, Training Loss: -0.5096\n",
      "Epoch 328/500, Training Loss: -0.5529\n",
      "Epoch 329/500, Training Loss: -0.5418\n",
      "Epoch 330/500, Training Loss: -0.5493\n",
      "Epoch 331/500, Training Loss: -0.5068\n",
      "Epoch 332/500, Training Loss: -0.5628\n",
      "Epoch 333/500, Training Loss: -0.5386\n",
      "Epoch 334/500, Training Loss: -0.5635\n",
      "Epoch 335/500, Training Loss: -0.5536\n",
      "Epoch 336/500, Training Loss: -0.5223\n",
      "Epoch 337/500, Training Loss: -0.5513\n",
      "Epoch 338/500, Training Loss: -0.5499\n",
      "Epoch 339/500, Training Loss: -0.5632\n",
      "Epoch 340/500, Training Loss: -0.5839\n",
      "Epoch 341/500, Training Loss: -0.5443\n",
      "Epoch 342/500, Training Loss: -0.5513\n",
      "Epoch 343/500, Training Loss: -0.5849\n",
      "Epoch 344/500, Training Loss: -0.5008\n",
      "Epoch 345/500, Training Loss: -0.5699\n",
      "Epoch 346/500, Training Loss: -0.5524\n",
      "Epoch 347/500, Training Loss: -0.5352\n",
      "Epoch 348/500, Training Loss: -0.5403\n",
      "Epoch 349/500, Training Loss: -0.5890\n",
      "Epoch 350/500, Training Loss: -0.5749\n",
      "Epoch 351/500, Training Loss: -0.5477\n",
      "Epoch 352/500, Training Loss: -0.5544\n",
      "Epoch 353/500, Training Loss: -0.5248\n",
      "Epoch 354/500, Training Loss: -0.5128\n",
      "Epoch 355/500, Training Loss: -0.5730\n",
      "Epoch 356/500, Training Loss: -0.5995\n",
      "Epoch 357/500, Training Loss: -0.5814\n",
      "Epoch 358/500, Training Loss: -0.5321\n",
      "Epoch 359/500, Training Loss: -0.5838\n",
      "Epoch 360/500, Training Loss: -0.5833\n",
      "Epoch 361/500, Training Loss: -0.5485\n",
      "Epoch 362/500, Training Loss: -0.5412\n",
      "Epoch 363/500, Training Loss: -0.5481\n",
      "Epoch 364/500, Training Loss: -0.5500\n",
      "Epoch 365/500, Training Loss: -0.5776\n",
      "Epoch 366/500, Training Loss: -0.5345\n",
      "Epoch 367/500, Training Loss: -0.5155\n",
      "Epoch 368/500, Training Loss: -0.5547\n",
      "Epoch 369/500, Training Loss: -0.5303\n",
      "Epoch 370/500, Training Loss: -0.5156\n",
      "Epoch 371/500, Training Loss: -0.5722\n",
      "Epoch 372/500, Training Loss: -0.5127\n",
      "Epoch 373/500, Training Loss: -0.5370\n",
      "Epoch 374/500, Training Loss: -0.5708\n",
      "Epoch 375/500, Training Loss: -0.5461\n",
      "Epoch 376/500, Training Loss: -0.5740\n",
      "Epoch 377/500, Training Loss: -0.5680\n",
      "Epoch 378/500, Training Loss: -0.5275\n",
      "Epoch 379/500, Training Loss: -0.5572\n",
      "Epoch 380/500, Training Loss: -0.5373\n",
      "Epoch 381/500, Training Loss: -0.5736\n",
      "Epoch 382/500, Training Loss: -0.5071\n",
      "Epoch 383/500, Training Loss: -0.5534\n",
      "Epoch 384/500, Training Loss: -0.5029\n",
      "Epoch 385/500, Training Loss: -0.5723\n",
      "Epoch 386/500, Training Loss: -0.5284\n",
      "Epoch 387/500, Training Loss: -0.5483\n",
      "Epoch 388/500, Training Loss: -0.5379\n",
      "Epoch 389/500, Training Loss: -0.5262\n",
      "Epoch 390/500, Training Loss: -0.5239\n",
      "Epoch 391/500, Training Loss: -0.5367\n",
      "Epoch 392/500, Training Loss: -0.5721\n",
      "Epoch 393/500, Training Loss: -0.5668\n",
      "Epoch 394/500, Training Loss: -0.5274\n",
      "Epoch 395/500, Training Loss: -0.5336\n",
      "Epoch 396/500, Training Loss: -0.5866\n",
      "Epoch 397/500, Training Loss: -0.5303\n",
      "Epoch 398/500, Training Loss: -0.5343\n",
      "Epoch 399/500, Training Loss: -0.5584\n",
      "Epoch 400/500, Training Loss: -0.5482\n",
      "Epoch 401/500, Training Loss: -0.5681\n",
      "Epoch 402/500, Training Loss: -0.5116\n",
      "Epoch 403/500, Training Loss: -0.5436\n",
      "Epoch 404/500, Training Loss: -0.5191\n",
      "Epoch 405/500, Training Loss: -0.5384\n",
      "Epoch 406/500, Training Loss: -0.5591\n",
      "Epoch 407/500, Training Loss: -0.5152\n",
      "Epoch 408/500, Training Loss: -0.5283\n",
      "Epoch 409/500, Training Loss: -0.5367\n",
      "Epoch 410/500, Training Loss: -0.5589\n",
      "Epoch 411/500, Training Loss: -0.5129\n",
      "Epoch 412/500, Training Loss: -0.5207\n",
      "Epoch 413/500, Training Loss: -0.5626\n",
      "Epoch 414/500, Training Loss: -0.5746\n",
      "Epoch 415/500, Training Loss: -0.5506\n",
      "Epoch 416/500, Training Loss: -0.5551\n",
      "Epoch 417/500, Training Loss: -0.5611\n",
      "Epoch 418/500, Training Loss: -0.5440\n",
      "Epoch 419/500, Training Loss: -0.5296\n",
      "Epoch 420/500, Training Loss: -0.5654\n",
      "Epoch 421/500, Training Loss: -0.5614\n",
      "Epoch 422/500, Training Loss: -0.5216\n",
      "Epoch 423/500, Training Loss: -0.5144\n",
      "Epoch 424/500, Training Loss: -0.5482\n",
      "Epoch 425/500, Training Loss: -0.5916\n",
      "Epoch 426/500, Training Loss: -0.5344\n",
      "Epoch 427/500, Training Loss: -0.5632\n",
      "Epoch 428/500, Training Loss: -0.5021\n",
      "Epoch 429/500, Training Loss: -0.5436\n",
      "Epoch 430/500, Training Loss: -0.5516\n",
      "Epoch 431/500, Training Loss: -0.5324\n",
      "Epoch 432/500, Training Loss: -0.5468\n",
      "Epoch 433/500, Training Loss: -0.5527\n",
      "Epoch 434/500, Training Loss: -0.5672\n",
      "Epoch 435/500, Training Loss: -0.5380\n",
      "Epoch 436/500, Training Loss: -0.5632\n",
      "Epoch 437/500, Training Loss: -0.5058\n",
      "Epoch 438/500, Training Loss: -0.5284\n",
      "Epoch 439/500, Training Loss: -0.5651\n",
      "Epoch 440/500, Training Loss: -0.5905\n",
      "Epoch 441/500, Training Loss: -0.4828\n",
      "Epoch 442/500, Training Loss: -0.5589\n",
      "Epoch 443/500, Training Loss: -0.5444\n",
      "Epoch 444/500, Training Loss: -0.5453\n",
      "Epoch 445/500, Training Loss: -0.5693\n",
      "Epoch 446/500, Training Loss: -0.5533\n",
      "Epoch 447/500, Training Loss: -0.5324\n",
      "Epoch 448/500, Training Loss: -0.5676\n",
      "Epoch 449/500, Training Loss: -0.5490\n",
      "Epoch 450/500, Training Loss: -0.5053\n",
      "Epoch 451/500, Training Loss: -0.5611\n",
      "Epoch 452/500, Training Loss: -0.5273\n",
      "Epoch 453/500, Training Loss: -0.5360\n",
      "Epoch 454/500, Training Loss: -0.5522\n",
      "Epoch 455/500, Training Loss: -0.5914\n",
      "Epoch 456/500, Training Loss: -0.5315\n",
      "Epoch 457/500, Training Loss: -0.5702\n",
      "Epoch 458/500, Training Loss: -0.5638\n",
      "Epoch 459/500, Training Loss: -0.5840\n",
      "Epoch 460/500, Training Loss: -0.5562\n",
      "Epoch 461/500, Training Loss: -0.5389\n",
      "Epoch 462/500, Training Loss: -0.5592\n",
      "Epoch 463/500, Training Loss: -0.5300\n",
      "Epoch 464/500, Training Loss: -0.5386\n",
      "Epoch 465/500, Training Loss: -0.5805\n",
      "Epoch 466/500, Training Loss: -0.5863\n",
      "Epoch 467/500, Training Loss: -0.5315\n",
      "Epoch 468/500, Training Loss: -0.5836\n",
      "Epoch 469/500, Training Loss: -0.5465\n",
      "Epoch 470/500, Training Loss: -0.5090\n",
      "Epoch 471/500, Training Loss: -0.5356\n",
      "Epoch 472/500, Training Loss: -0.5460\n",
      "Epoch 473/500, Training Loss: -0.5148\n",
      "Epoch 474/500, Training Loss: -0.5772\n",
      "Epoch 475/500, Training Loss: -0.5684\n",
      "Epoch 476/500, Training Loss: -0.5502\n",
      "Epoch 477/500, Training Loss: -0.5231\n",
      "Epoch 478/500, Training Loss: -0.5710\n",
      "Epoch 479/500, Training Loss: -0.5535\n",
      "Epoch 480/500, Training Loss: -0.5408\n",
      "Epoch 481/500, Training Loss: -0.5335\n",
      "Epoch 482/500, Training Loss: -0.5501\n",
      "Epoch 483/500, Training Loss: -0.5725\n",
      "Epoch 484/500, Training Loss: -0.5270\n",
      "Epoch 485/500, Training Loss: -0.5180\n",
      "Epoch 486/500, Training Loss: -0.5193\n",
      "Epoch 487/500, Training Loss: -0.4831\n",
      "Epoch 488/500, Training Loss: -0.5200\n",
      "Epoch 489/500, Training Loss: -0.5612\n",
      "Epoch 490/500, Training Loss: -0.5540\n",
      "Epoch 491/500, Training Loss: -0.5710\n",
      "Epoch 492/500, Training Loss: -0.5439\n",
      "Epoch 493/500, Training Loss: -0.5303\n",
      "Epoch 494/500, Training Loss: -0.5674\n",
      "Epoch 495/500, Training Loss: -0.5645\n",
      "Epoch 496/500, Training Loss: -0.5389\n",
      "Epoch 497/500, Training Loss: -0.5592\n",
      "Epoch 498/500, Training Loss: -0.5355\n",
      "Epoch 499/500, Training Loss: -0.5413\n",
      "Epoch 500/500, Training Loss: -0.5493\n",
      "Validation Loss: -0.5893\n",
      "Model 0, Fold 1: Validation Loss = -0.5893\n",
      "Epoch 1/500, Training Loss: -0.5074\n",
      "Epoch 2/500, Training Loss: -0.5137\n",
      "Epoch 3/500, Training Loss: -0.5322\n",
      "Epoch 4/500, Training Loss: -0.5400\n",
      "Epoch 5/500, Training Loss: -0.5321\n",
      "Epoch 6/500, Training Loss: -0.5404\n",
      "Epoch 7/500, Training Loss: -0.5832\n",
      "Epoch 8/500, Training Loss: -0.5517\n",
      "Epoch 9/500, Training Loss: -0.5120\n",
      "Epoch 10/500, Training Loss: -0.6114\n",
      "Epoch 11/500, Training Loss: -0.5335\n",
      "Epoch 12/500, Training Loss: -0.5500\n",
      "Epoch 13/500, Training Loss: -0.5779\n",
      "Epoch 14/500, Training Loss: -0.5661\n",
      "Epoch 15/500, Training Loss: -0.5051\n",
      "Epoch 16/500, Training Loss: -0.5573\n",
      "Epoch 17/500, Training Loss: -0.5821\n",
      "Epoch 18/500, Training Loss: -0.5593\n",
      "Epoch 19/500, Training Loss: -0.5909\n",
      "Epoch 20/500, Training Loss: -0.5800\n",
      "Epoch 21/500, Training Loss: -0.5506\n",
      "Epoch 22/500, Training Loss: -0.5838\n",
      "Epoch 23/500, Training Loss: -0.5642\n",
      "Epoch 24/500, Training Loss: -0.5820\n",
      "Epoch 25/500, Training Loss: -0.5659\n",
      "Epoch 26/500, Training Loss: -0.5428\n",
      "Epoch 27/500, Training Loss: -0.5879\n",
      "Epoch 28/500, Training Loss: -0.6245\n",
      "Epoch 29/500, Training Loss: -0.5656\n",
      "Epoch 30/500, Training Loss: -0.5203\n",
      "Epoch 31/500, Training Loss: -0.5947\n",
      "Epoch 32/500, Training Loss: -0.5532\n",
      "Epoch 33/500, Training Loss: -0.5372\n",
      "Epoch 34/500, Training Loss: -0.5819\n",
      "Epoch 35/500, Training Loss: -0.5598\n",
      "Epoch 36/500, Training Loss: -0.5498\n",
      "Epoch 37/500, Training Loss: -0.5753\n",
      "Epoch 38/500, Training Loss: -0.5718\n",
      "Epoch 39/500, Training Loss: -0.5707\n",
      "Epoch 40/500, Training Loss: -0.5351\n",
      "Epoch 41/500, Training Loss: -0.5178\n",
      "Epoch 42/500, Training Loss: -0.5466\n",
      "Epoch 43/500, Training Loss: -0.5351\n",
      "Epoch 44/500, Training Loss: -0.5833\n",
      "Epoch 45/500, Training Loss: -0.5517\n",
      "Epoch 46/500, Training Loss: -0.5687\n",
      "Epoch 47/500, Training Loss: -0.5466\n",
      "Epoch 48/500, Training Loss: -0.5463\n",
      "Epoch 49/500, Training Loss: -0.5442\n",
      "Epoch 50/500, Training Loss: -0.4967\n",
      "Epoch 51/500, Training Loss: -0.5545\n",
      "Epoch 52/500, Training Loss: -0.5370\n",
      "Epoch 53/500, Training Loss: -0.5613\n",
      "Epoch 54/500, Training Loss: -0.6107\n",
      "Epoch 55/500, Training Loss: -0.5730\n",
      "Epoch 56/500, Training Loss: -0.5758\n",
      "Epoch 57/500, Training Loss: -0.5469\n",
      "Epoch 58/500, Training Loss: -0.5761\n",
      "Epoch 59/500, Training Loss: -0.5751\n",
      "Epoch 60/500, Training Loss: -0.5238\n",
      "Epoch 61/500, Training Loss: -0.5716\n",
      "Epoch 62/500, Training Loss: -0.5625\n",
      "Epoch 63/500, Training Loss: -0.5454\n",
      "Epoch 64/500, Training Loss: -0.6028\n",
      "Epoch 65/500, Training Loss: -0.5890\n",
      "Epoch 66/500, Training Loss: -0.5189\n",
      "Epoch 67/500, Training Loss: -0.5710\n",
      "Epoch 68/500, Training Loss: -0.5675\n",
      "Epoch 69/500, Training Loss: -0.5451\n",
      "Epoch 70/500, Training Loss: -0.5382\n",
      "Epoch 71/500, Training Loss: -0.5383\n",
      "Epoch 72/500, Training Loss: -0.5429\n",
      "Epoch 73/500, Training Loss: -0.5772\n",
      "Epoch 74/500, Training Loss: -0.5698\n",
      "Epoch 75/500, Training Loss: -0.5394\n",
      "Epoch 76/500, Training Loss: -0.5835\n",
      "Epoch 77/500, Training Loss: -0.5683\n",
      "Epoch 78/500, Training Loss: -0.5569\n",
      "Epoch 79/500, Training Loss: -0.5338\n",
      "Epoch 80/500, Training Loss: -0.5493\n",
      "Epoch 81/500, Training Loss: -0.5289\n",
      "Epoch 82/500, Training Loss: -0.5309\n",
      "Epoch 83/500, Training Loss: -0.5790\n",
      "Epoch 84/500, Training Loss: -0.5352\n",
      "Epoch 85/500, Training Loss: -0.5545\n",
      "Epoch 86/500, Training Loss: -0.5909\n",
      "Epoch 87/500, Training Loss: -0.5779\n",
      "Epoch 88/500, Training Loss: -0.5234\n",
      "Epoch 89/500, Training Loss: -0.5602\n",
      "Epoch 90/500, Training Loss: -0.5728\n",
      "Epoch 91/500, Training Loss: -0.5366\n",
      "Epoch 92/500, Training Loss: -0.5908\n",
      "Epoch 93/500, Training Loss: -0.5445\n",
      "Epoch 94/500, Training Loss: -0.5417\n",
      "Epoch 95/500, Training Loss: -0.5766\n",
      "Epoch 96/500, Training Loss: -0.5545\n",
      "Epoch 97/500, Training Loss: -0.5429\n",
      "Epoch 98/500, Training Loss: -0.5724\n",
      "Epoch 99/500, Training Loss: -0.5403\n",
      "Epoch 100/500, Training Loss: -0.5481\n",
      "Epoch 101/500, Training Loss: -0.5268\n",
      "Epoch 102/500, Training Loss: -0.5849\n",
      "Epoch 103/500, Training Loss: -0.5570\n",
      "Epoch 104/500, Training Loss: -0.5886\n",
      "Epoch 105/500, Training Loss: -0.5615\n",
      "Epoch 106/500, Training Loss: -0.5209\n",
      "Epoch 107/500, Training Loss: -0.5624\n",
      "Epoch 108/500, Training Loss: -0.5405\n",
      "Epoch 109/500, Training Loss: -0.5614\n",
      "Epoch 110/500, Training Loss: -0.5534\n",
      "Epoch 111/500, Training Loss: -0.5272\n",
      "Epoch 112/500, Training Loss: -0.5651\n",
      "Epoch 113/500, Training Loss: -0.5631\n",
      "Epoch 114/500, Training Loss: -0.5361\n",
      "Epoch 115/500, Training Loss: -0.5812\n",
      "Epoch 116/500, Training Loss: -0.5832\n",
      "Epoch 117/500, Training Loss: -0.5214\n",
      "Epoch 118/500, Training Loss: -0.5741\n",
      "Epoch 119/500, Training Loss: -0.5630\n",
      "Epoch 120/500, Training Loss: -0.5556\n",
      "Epoch 121/500, Training Loss: -0.5768\n",
      "Epoch 122/500, Training Loss: -0.5526\n",
      "Epoch 123/500, Training Loss: -0.5679\n",
      "Epoch 124/500, Training Loss: -0.5307\n",
      "Epoch 125/500, Training Loss: -0.5229\n",
      "Epoch 126/500, Training Loss: -0.5821\n",
      "Epoch 127/500, Training Loss: -0.5370\n",
      "Epoch 128/500, Training Loss: -0.5676\n",
      "Epoch 129/500, Training Loss: -0.6087\n",
      "Epoch 130/500, Training Loss: -0.5584\n",
      "Epoch 131/500, Training Loss: -0.5727\n",
      "Epoch 132/500, Training Loss: -0.5787\n",
      "Epoch 133/500, Training Loss: -0.5950\n",
      "Epoch 134/500, Training Loss: -0.5634\n",
      "Epoch 135/500, Training Loss: -0.5352\n",
      "Epoch 136/500, Training Loss: -0.4933\n",
      "Epoch 137/500, Training Loss: -0.5470\n",
      "Epoch 138/500, Training Loss: -0.5535\n",
      "Epoch 139/500, Training Loss: -0.5683\n",
      "Epoch 140/500, Training Loss: -0.5301\n",
      "Epoch 141/500, Training Loss: -0.5742\n",
      "Epoch 142/500, Training Loss: -0.6244\n",
      "Epoch 143/500, Training Loss: -0.5273\n",
      "Epoch 144/500, Training Loss: -0.5411\n",
      "Epoch 145/500, Training Loss: -0.5592\n",
      "Epoch 146/500, Training Loss: -0.5547\n",
      "Epoch 147/500, Training Loss: -0.5705\n",
      "Epoch 148/500, Training Loss: -0.5910\n",
      "Epoch 149/500, Training Loss: -0.5499\n",
      "Epoch 150/500, Training Loss: -0.5789\n",
      "Epoch 151/500, Training Loss: -0.5799\n",
      "Epoch 152/500, Training Loss: -0.5713\n",
      "Epoch 153/500, Training Loss: -0.5902\n",
      "Epoch 154/500, Training Loss: -0.5702\n",
      "Epoch 155/500, Training Loss: -0.5337\n",
      "Epoch 156/500, Training Loss: -0.5645\n",
      "Epoch 157/500, Training Loss: -0.5781\n",
      "Epoch 158/500, Training Loss: -0.5854\n",
      "Epoch 159/500, Training Loss: -0.5891\n",
      "Epoch 160/500, Training Loss: -0.5592\n",
      "Epoch 161/500, Training Loss: -0.5520\n",
      "Epoch 162/500, Training Loss: -0.5597\n",
      "Epoch 163/500, Training Loss: -0.5462\n",
      "Epoch 164/500, Training Loss: -0.5302\n",
      "Epoch 165/500, Training Loss: -0.5641\n",
      "Epoch 166/500, Training Loss: -0.5560\n",
      "Epoch 167/500, Training Loss: -0.5924\n",
      "Epoch 168/500, Training Loss: -0.5696\n",
      "Epoch 169/500, Training Loss: -0.5487\n",
      "Epoch 170/500, Training Loss: -0.5838\n",
      "Epoch 171/500, Training Loss: -0.5601\n",
      "Epoch 172/500, Training Loss: -0.5467\n",
      "Epoch 173/500, Training Loss: -0.5974\n",
      "Epoch 174/500, Training Loss: -0.5629\n",
      "Epoch 175/500, Training Loss: -0.5785\n",
      "Epoch 176/500, Training Loss: -0.5813\n",
      "Epoch 177/500, Training Loss: -0.5713\n",
      "Epoch 178/500, Training Loss: -0.5593\n",
      "Epoch 179/500, Training Loss: -0.5482\n",
      "Epoch 180/500, Training Loss: -0.5417\n",
      "Epoch 181/500, Training Loss: -0.5657\n",
      "Epoch 182/500, Training Loss: -0.5410\n",
      "Epoch 183/500, Training Loss: -0.5456\n",
      "Epoch 184/500, Training Loss: -0.5396\n",
      "Epoch 185/500, Training Loss: -0.5798\n",
      "Epoch 186/500, Training Loss: -0.5804\n",
      "Epoch 187/500, Training Loss: -0.5436\n",
      "Epoch 188/500, Training Loss: -0.5786\n",
      "Epoch 189/500, Training Loss: -0.5530\n",
      "Epoch 190/500, Training Loss: -0.5385\n",
      "Epoch 191/500, Training Loss: -0.5386\n",
      "Epoch 192/500, Training Loss: -0.5518\n",
      "Epoch 193/500, Training Loss: -0.5818\n",
      "Epoch 194/500, Training Loss: -0.5791\n",
      "Epoch 195/500, Training Loss: -0.6012\n",
      "Epoch 196/500, Training Loss: -0.5820\n",
      "Epoch 197/500, Training Loss: -0.5851\n",
      "Epoch 198/500, Training Loss: -0.5497\n",
      "Epoch 199/500, Training Loss: -0.5560\n",
      "Epoch 200/500, Training Loss: -0.5264\n",
      "Epoch 201/500, Training Loss: -0.5889\n",
      "Epoch 202/500, Training Loss: -0.5567\n",
      "Epoch 203/500, Training Loss: -0.5559\n",
      "Epoch 204/500, Training Loss: -0.5566\n",
      "Epoch 205/500, Training Loss: -0.5804\n",
      "Epoch 206/500, Training Loss: -0.5700\n",
      "Epoch 207/500, Training Loss: -0.5466\n",
      "Epoch 208/500, Training Loss: -0.5446\n",
      "Epoch 209/500, Training Loss: -0.5826\n",
      "Epoch 210/500, Training Loss: -0.6143\n",
      "Epoch 211/500, Training Loss: -0.5681\n",
      "Epoch 212/500, Training Loss: -0.5648\n",
      "Epoch 213/500, Training Loss: -0.5417\n",
      "Epoch 214/500, Training Loss: -0.5643\n",
      "Epoch 215/500, Training Loss: -0.5516\n",
      "Epoch 216/500, Training Loss: -0.5612\n",
      "Epoch 217/500, Training Loss: -0.5250\n",
      "Epoch 218/500, Training Loss: -0.5581\n",
      "Epoch 219/500, Training Loss: -0.5961\n",
      "Epoch 220/500, Training Loss: -0.5446\n",
      "Epoch 221/500, Training Loss: -0.5355\n",
      "Epoch 222/500, Training Loss: -0.5215\n",
      "Epoch 223/500, Training Loss: -0.5623\n",
      "Epoch 224/500, Training Loss: -0.6115\n",
      "Epoch 225/500, Training Loss: -0.5408\n",
      "Epoch 226/500, Training Loss: -0.5423\n",
      "Epoch 227/500, Training Loss: -0.5878\n",
      "Epoch 228/500, Training Loss: -0.5773\n",
      "Epoch 229/500, Training Loss: -0.5466\n",
      "Epoch 230/500, Training Loss: -0.5681\n",
      "Epoch 231/500, Training Loss: -0.5344\n",
      "Epoch 232/500, Training Loss: -0.5715\n",
      "Epoch 233/500, Training Loss: -0.5766\n",
      "Epoch 234/500, Training Loss: -0.5498\n",
      "Epoch 235/500, Training Loss: -0.5550\n",
      "Epoch 236/500, Training Loss: -0.5987\n",
      "Epoch 237/500, Training Loss: -0.5494\n",
      "Epoch 238/500, Training Loss: -0.5215\n",
      "Epoch 239/500, Training Loss: -0.5619\n",
      "Epoch 240/500, Training Loss: -0.6090\n",
      "Epoch 241/500, Training Loss: -0.5412\n",
      "Epoch 242/500, Training Loss: -0.5414\n",
      "Epoch 243/500, Training Loss: -0.5386\n",
      "Epoch 244/500, Training Loss: -0.5638\n",
      "Epoch 245/500, Training Loss: -0.5865\n",
      "Epoch 246/500, Training Loss: -0.5779\n",
      "Epoch 247/500, Training Loss: -0.5619\n",
      "Epoch 248/500, Training Loss: -0.5800\n",
      "Epoch 249/500, Training Loss: -0.5620\n",
      "Epoch 250/500, Training Loss: -0.5435\n",
      "Epoch 251/500, Training Loss: -0.5707\n",
      "Epoch 252/500, Training Loss: -0.5715\n",
      "Epoch 253/500, Training Loss: -0.5622\n",
      "Epoch 254/500, Training Loss: -0.5315\n",
      "Epoch 255/500, Training Loss: -0.5540\n",
      "Epoch 256/500, Training Loss: -0.5575\n",
      "Epoch 257/500, Training Loss: -0.5923\n",
      "Epoch 258/500, Training Loss: -0.5596\n",
      "Epoch 259/500, Training Loss: -0.5851\n",
      "Epoch 260/500, Training Loss: -0.5048\n",
      "Epoch 261/500, Training Loss: -0.5462\n",
      "Epoch 262/500, Training Loss: -0.6226\n",
      "Epoch 263/500, Training Loss: -0.5633\n",
      "Epoch 264/500, Training Loss: -0.5404\n",
      "Epoch 265/500, Training Loss: -0.5754\n",
      "Epoch 266/500, Training Loss: -0.5545\n",
      "Epoch 267/500, Training Loss: -0.5292\n",
      "Epoch 268/500, Training Loss: -0.5721\n",
      "Epoch 269/500, Training Loss: -0.5346\n",
      "Epoch 270/500, Training Loss: -0.5658\n",
      "Epoch 271/500, Training Loss: -0.6067\n",
      "Epoch 272/500, Training Loss: -0.5616\n",
      "Epoch 273/500, Training Loss: -0.5348\n",
      "Epoch 274/500, Training Loss: -0.5580\n",
      "Epoch 275/500, Training Loss: -0.5718\n",
      "Epoch 276/500, Training Loss: -0.5411\n",
      "Epoch 277/500, Training Loss: -0.5503\n",
      "Epoch 278/500, Training Loss: -0.6132\n",
      "Epoch 279/500, Training Loss: -0.5464\n",
      "Epoch 280/500, Training Loss: -0.5931\n",
      "Epoch 281/500, Training Loss: -0.5458\n",
      "Epoch 282/500, Training Loss: -0.6080\n",
      "Epoch 283/500, Training Loss: -0.5588\n",
      "Epoch 284/500, Training Loss: -0.5643\n",
      "Epoch 285/500, Training Loss: -0.5639\n",
      "Epoch 286/500, Training Loss: -0.5791\n",
      "Epoch 287/500, Training Loss: -0.5299\n",
      "Epoch 288/500, Training Loss: -0.5450\n",
      "Epoch 289/500, Training Loss: -0.5186\n",
      "Epoch 290/500, Training Loss: -0.5504\n",
      "Epoch 291/500, Training Loss: -0.5468\n",
      "Epoch 292/500, Training Loss: -0.5833\n",
      "Epoch 293/500, Training Loss: -0.5522\n",
      "Epoch 294/500, Training Loss: -0.5444\n",
      "Epoch 295/500, Training Loss: -0.5448\n",
      "Epoch 296/500, Training Loss: -0.5408\n",
      "Epoch 297/500, Training Loss: -0.6254\n",
      "Epoch 298/500, Training Loss: -0.5592\n",
      "Epoch 299/500, Training Loss: -0.5537\n",
      "Epoch 300/500, Training Loss: -0.5106\n",
      "Epoch 301/500, Training Loss: -0.5958\n",
      "Epoch 302/500, Training Loss: -0.5888\n",
      "Epoch 303/500, Training Loss: -0.5948\n",
      "Epoch 304/500, Training Loss: -0.5948\n",
      "Epoch 305/500, Training Loss: -0.5515\n",
      "Epoch 306/500, Training Loss: -0.5714\n",
      "Epoch 307/500, Training Loss: -0.5486\n",
      "Epoch 308/500, Training Loss: -0.5775\n",
      "Epoch 309/500, Training Loss: -0.5770\n",
      "Epoch 310/500, Training Loss: -0.5196\n",
      "Epoch 311/500, Training Loss: -0.5622\n",
      "Epoch 312/500, Training Loss: -0.5279\n",
      "Epoch 313/500, Training Loss: -0.5947\n",
      "Epoch 314/500, Training Loss: -0.5556\n",
      "Epoch 315/500, Training Loss: -0.5548\n",
      "Epoch 316/500, Training Loss: -0.5669\n",
      "Epoch 317/500, Training Loss: -0.5531\n",
      "Epoch 318/500, Training Loss: -0.5749\n",
      "Epoch 319/500, Training Loss: -0.6095\n",
      "Epoch 320/500, Training Loss: -0.5785\n",
      "Epoch 321/500, Training Loss: -0.5913\n",
      "Epoch 322/500, Training Loss: -0.5613\n",
      "Epoch 323/500, Training Loss: -0.5345\n",
      "Epoch 324/500, Training Loss: -0.5691\n",
      "Epoch 325/500, Training Loss: -0.5198\n",
      "Epoch 326/500, Training Loss: -0.5257\n",
      "Epoch 327/500, Training Loss: -0.5639\n",
      "Epoch 328/500, Training Loss: -0.5702\n",
      "Epoch 329/500, Training Loss: -0.5276\n",
      "Epoch 330/500, Training Loss: -0.5391\n",
      "Epoch 331/500, Training Loss: -0.5667\n",
      "Epoch 332/500, Training Loss: -0.5620\n",
      "Epoch 333/500, Training Loss: -0.5728\n",
      "Epoch 334/500, Training Loss: -0.5775\n",
      "Epoch 335/500, Training Loss: -0.6128\n",
      "Epoch 336/500, Training Loss: -0.5504\n",
      "Epoch 337/500, Training Loss: -0.6225\n",
      "Epoch 338/500, Training Loss: -0.5522\n",
      "Epoch 339/500, Training Loss: -0.6108\n",
      "Epoch 340/500, Training Loss: -0.5906\n",
      "Epoch 341/500, Training Loss: -0.5814\n",
      "Epoch 342/500, Training Loss: -0.5582\n",
      "Epoch 343/500, Training Loss: -0.5709\n",
      "Epoch 344/500, Training Loss: -0.5507\n",
      "Epoch 345/500, Training Loss: -0.5482\n",
      "Epoch 346/500, Training Loss: -0.5509\n",
      "Epoch 347/500, Training Loss: -0.5604\n",
      "Epoch 348/500, Training Loss: -0.5671\n",
      "Epoch 349/500, Training Loss: -0.5591\n",
      "Epoch 350/500, Training Loss: -0.5477\n",
      "Epoch 351/500, Training Loss: -0.5714\n",
      "Epoch 352/500, Training Loss: -0.5722\n",
      "Epoch 353/500, Training Loss: -0.5742\n",
      "Epoch 354/500, Training Loss: -0.5921\n",
      "Epoch 355/500, Training Loss: -0.6265\n",
      "Epoch 356/500, Training Loss: -0.5526\n",
      "Epoch 357/500, Training Loss: -0.5823\n",
      "Epoch 358/500, Training Loss: -0.5473\n",
      "Epoch 359/500, Training Loss: -0.5467\n",
      "Epoch 360/500, Training Loss: -0.5454\n",
      "Epoch 361/500, Training Loss: -0.5199\n",
      "Epoch 362/500, Training Loss: -0.5358\n",
      "Epoch 363/500, Training Loss: -0.5747\n",
      "Epoch 364/500, Training Loss: -0.5609\n",
      "Epoch 365/500, Training Loss: -0.5591\n",
      "Epoch 366/500, Training Loss: -0.5519\n",
      "Epoch 367/500, Training Loss: -0.5229\n",
      "Epoch 368/500, Training Loss: -0.5672\n",
      "Epoch 369/500, Training Loss: -0.5587\n",
      "Epoch 370/500, Training Loss: -0.5476\n",
      "Epoch 371/500, Training Loss: -0.6123\n",
      "Epoch 372/500, Training Loss: -0.5633\n",
      "Epoch 373/500, Training Loss: -0.5785\n",
      "Epoch 374/500, Training Loss: -0.5677\n",
      "Epoch 375/500, Training Loss: -0.5362\n",
      "Epoch 376/500, Training Loss: -0.6020\n",
      "Epoch 377/500, Training Loss: -0.5467\n",
      "Epoch 378/500, Training Loss: -0.5499\n",
      "Epoch 379/500, Training Loss: -0.5751\n",
      "Epoch 380/500, Training Loss: -0.5450\n",
      "Epoch 381/500, Training Loss: -0.5751\n",
      "Epoch 382/500, Training Loss: -0.5736\n",
      "Epoch 383/500, Training Loss: -0.5754\n",
      "Epoch 384/500, Training Loss: -0.5466\n",
      "Epoch 385/500, Training Loss: -0.5455\n",
      "Epoch 386/500, Training Loss: -0.6091\n",
      "Epoch 387/500, Training Loss: -0.5893\n",
      "Epoch 388/500, Training Loss: -0.5754\n",
      "Epoch 389/500, Training Loss: -0.5478\n",
      "Epoch 390/500, Training Loss: -0.5622\n",
      "Epoch 391/500, Training Loss: -0.5687\n",
      "Epoch 392/500, Training Loss: -0.5486\n",
      "Epoch 393/500, Training Loss: -0.5514\n",
      "Epoch 394/500, Training Loss: -0.5636\n",
      "Epoch 395/500, Training Loss: -0.5488\n",
      "Epoch 396/500, Training Loss: -0.5656\n",
      "Epoch 397/500, Training Loss: -0.5842\n",
      "Epoch 398/500, Training Loss: -0.5451\n",
      "Epoch 399/500, Training Loss: -0.5584\n",
      "Epoch 400/500, Training Loss: -0.5846\n",
      "Epoch 401/500, Training Loss: -0.5499\n",
      "Epoch 402/500, Training Loss: -0.5768\n",
      "Epoch 403/500, Training Loss: -0.5319\n",
      "Epoch 404/500, Training Loss: -0.5532\n",
      "Epoch 405/500, Training Loss: -0.6229\n",
      "Epoch 406/500, Training Loss: -0.5619\n",
      "Epoch 407/500, Training Loss: -0.5584\n",
      "Epoch 408/500, Training Loss: -0.5541\n",
      "Epoch 409/500, Training Loss: -0.5738\n",
      "Epoch 410/500, Training Loss: -0.5608\n",
      "Epoch 411/500, Training Loss: -0.5946\n",
      "Epoch 412/500, Training Loss: -0.5248\n",
      "Epoch 413/500, Training Loss: -0.5825\n",
      "Epoch 414/500, Training Loss: -0.5297\n",
      "Epoch 415/500, Training Loss: -0.5531\n",
      "Epoch 416/500, Training Loss: -0.5656\n",
      "Epoch 417/500, Training Loss: -0.5620\n",
      "Epoch 418/500, Training Loss: -0.5782\n",
      "Epoch 419/500, Training Loss: -0.5894\n",
      "Epoch 420/500, Training Loss: -0.5403\n",
      "Epoch 421/500, Training Loss: -0.5133\n",
      "Epoch 422/500, Training Loss: -0.5795\n",
      "Epoch 423/500, Training Loss: -0.5581\n",
      "Epoch 424/500, Training Loss: -0.5527\n",
      "Epoch 425/500, Training Loss: -0.6007\n",
      "Epoch 426/500, Training Loss: -0.5625\n",
      "Epoch 427/500, Training Loss: -0.5729\n",
      "Epoch 428/500, Training Loss: -0.5613\n",
      "Epoch 429/500, Training Loss: -0.5564\n",
      "Epoch 430/500, Training Loss: -0.5621\n",
      "Epoch 431/500, Training Loss: -0.5679\n",
      "Epoch 432/500, Training Loss: -0.5705\n",
      "Epoch 433/500, Training Loss: -0.5103\n",
      "Epoch 434/500, Training Loss: -0.5865\n",
      "Epoch 435/500, Training Loss: -0.5414\n",
      "Epoch 436/500, Training Loss: -0.5796\n",
      "Epoch 437/500, Training Loss: -0.5588\n",
      "Epoch 438/500, Training Loss: -0.5343\n",
      "Epoch 439/500, Training Loss: -0.5919\n",
      "Epoch 440/500, Training Loss: -0.5252\n",
      "Epoch 441/500, Training Loss: -0.5707\n",
      "Epoch 442/500, Training Loss: -0.5794\n",
      "Epoch 443/500, Training Loss: -0.5233\n",
      "Epoch 444/500, Training Loss: -0.5538\n",
      "Epoch 445/500, Training Loss: -0.5884\n",
      "Epoch 446/500, Training Loss: -0.5439\n",
      "Epoch 447/500, Training Loss: -0.5431\n",
      "Epoch 448/500, Training Loss: -0.5455\n",
      "Epoch 449/500, Training Loss: -0.5884\n",
      "Epoch 450/500, Training Loss: -0.5505\n",
      "Epoch 451/500, Training Loss: -0.5715\n",
      "Epoch 452/500, Training Loss: -0.5371\n",
      "Epoch 453/500, Training Loss: -0.5351\n",
      "Epoch 454/500, Training Loss: -0.5922\n",
      "Epoch 455/500, Training Loss: -0.5599\n",
      "Epoch 456/500, Training Loss: -0.5669\n",
      "Epoch 457/500, Training Loss: -0.5080\n",
      "Epoch 458/500, Training Loss: -0.5740\n",
      "Epoch 459/500, Training Loss: -0.5262\n",
      "Epoch 460/500, Training Loss: -0.5940\n",
      "Epoch 461/500, Training Loss: -0.5212\n",
      "Epoch 462/500, Training Loss: -0.6162\n",
      "Epoch 463/500, Training Loss: -0.5411\n",
      "Epoch 464/500, Training Loss: -0.5733\n",
      "Epoch 465/500, Training Loss: -0.5439\n",
      "Epoch 466/500, Training Loss: -0.6334\n",
      "Epoch 467/500, Training Loss: -0.5458\n",
      "Epoch 468/500, Training Loss: -0.5363\n",
      "Epoch 469/500, Training Loss: -0.5694\n",
      "Epoch 470/500, Training Loss: -0.5101\n",
      "Epoch 471/500, Training Loss: -0.5693\n",
      "Epoch 472/500, Training Loss: -0.5576\n",
      "Epoch 473/500, Training Loss: -0.5720\n",
      "Epoch 474/500, Training Loss: -0.5264\n",
      "Epoch 475/500, Training Loss: -0.5964\n",
      "Epoch 476/500, Training Loss: -0.5928\n",
      "Epoch 477/500, Training Loss: -0.5329\n",
      "Epoch 478/500, Training Loss: -0.5907\n",
      "Epoch 479/500, Training Loss: -0.5546\n",
      "Epoch 480/500, Training Loss: -0.5429\n",
      "Epoch 481/500, Training Loss: -0.5619\n",
      "Epoch 482/500, Training Loss: -0.5800\n",
      "Epoch 483/500, Training Loss: -0.5671\n",
      "Epoch 484/500, Training Loss: -0.5413\n",
      "Epoch 485/500, Training Loss: -0.5807\n",
      "Epoch 486/500, Training Loss: -0.5587\n",
      "Epoch 487/500, Training Loss: -0.5578\n",
      "Epoch 488/500, Training Loss: -0.5727\n",
      "Epoch 489/500, Training Loss: -0.5538\n",
      "Epoch 490/500, Training Loss: -0.5569\n",
      "Epoch 491/500, Training Loss: -0.5755\n",
      "Epoch 492/500, Training Loss: -0.5753\n",
      "Epoch 493/500, Training Loss: -0.5695\n",
      "Epoch 494/500, Training Loss: -0.5986\n",
      "Epoch 495/500, Training Loss: -0.5756\n",
      "Epoch 496/500, Training Loss: -0.5506\n",
      "Epoch 497/500, Training Loss: -0.5649\n",
      "Epoch 498/500, Training Loss: -0.5699\n",
      "Epoch 499/500, Training Loss: -0.5692\n",
      "Epoch 500/500, Training Loss: -0.5601\n",
      "Validation Loss: -0.5285\n",
      "Model 1, Fold 0: Validation Loss = -0.5285\n",
      "Epoch 1/500, Training Loss: -0.4568\n",
      "Epoch 2/500, Training Loss: -0.5094\n",
      "Epoch 3/500, Training Loss: -0.5194\n",
      "Epoch 4/500, Training Loss: -0.5611\n",
      "Epoch 5/500, Training Loss: -0.5082\n",
      "Epoch 6/500, Training Loss: -0.5329\n",
      "Epoch 7/500, Training Loss: -0.5412\n",
      "Epoch 8/500, Training Loss: -0.5355\n",
      "Epoch 9/500, Training Loss: -0.5294\n",
      "Epoch 10/500, Training Loss: -0.5204\n",
      "Epoch 11/500, Training Loss: -0.5564\n",
      "Epoch 12/500, Training Loss: -0.5518\n",
      "Epoch 13/500, Training Loss: -0.5277\n",
      "Epoch 14/500, Training Loss: -0.5866\n",
      "Epoch 15/500, Training Loss: -0.5723\n",
      "Epoch 16/500, Training Loss: -0.5347\n",
      "Epoch 17/500, Training Loss: -0.5354\n",
      "Epoch 18/500, Training Loss: -0.5575\n",
      "Epoch 19/500, Training Loss: -0.5485\n",
      "Epoch 20/500, Training Loss: -0.5476\n",
      "Epoch 21/500, Training Loss: -0.5523\n",
      "Epoch 22/500, Training Loss: -0.5210\n",
      "Epoch 23/500, Training Loss: -0.5575\n",
      "Epoch 24/500, Training Loss: -0.5385\n",
      "Epoch 25/500, Training Loss: -0.5388\n",
      "Epoch 26/500, Training Loss: -0.5564\n",
      "Epoch 27/500, Training Loss: -0.5488\n",
      "Epoch 28/500, Training Loss: -0.5534\n",
      "Epoch 29/500, Training Loss: -0.5490\n",
      "Epoch 30/500, Training Loss: -0.5224\n",
      "Epoch 31/500, Training Loss: -0.5473\n",
      "Epoch 32/500, Training Loss: -0.5075\n",
      "Epoch 33/500, Training Loss: -0.5115\n",
      "Epoch 34/500, Training Loss: -0.5529\n",
      "Epoch 35/500, Training Loss: -0.5403\n",
      "Epoch 36/500, Training Loss: -0.5485\n",
      "Epoch 37/500, Training Loss: -0.4787\n",
      "Epoch 38/500, Training Loss: -0.5316\n",
      "Epoch 39/500, Training Loss: -0.5269\n",
      "Epoch 40/500, Training Loss: -0.5487\n",
      "Epoch 41/500, Training Loss: -0.5318\n",
      "Epoch 42/500, Training Loss: -0.5172\n",
      "Epoch 43/500, Training Loss: -0.5372\n",
      "Epoch 44/500, Training Loss: -0.4935\n",
      "Epoch 45/500, Training Loss: -0.5491\n",
      "Epoch 46/500, Training Loss: -0.5177\n",
      "Epoch 47/500, Training Loss: -0.5517\n",
      "Epoch 48/500, Training Loss: -0.5299\n",
      "Epoch 49/500, Training Loss: -0.5094\n",
      "Epoch 50/500, Training Loss: -0.5294\n",
      "Epoch 51/500, Training Loss: -0.5660\n",
      "Epoch 52/500, Training Loss: -0.5179\n",
      "Epoch 53/500, Training Loss: -0.5747\n",
      "Epoch 54/500, Training Loss: -0.5738\n",
      "Epoch 55/500, Training Loss: -0.5592\n",
      "Epoch 56/500, Training Loss: -0.5359\n",
      "Epoch 57/500, Training Loss: -0.5354\n",
      "Epoch 58/500, Training Loss: -0.5548\n",
      "Epoch 59/500, Training Loss: -0.5282\n",
      "Epoch 60/500, Training Loss: -0.5624\n",
      "Epoch 61/500, Training Loss: -0.5604\n",
      "Epoch 62/500, Training Loss: -0.5462\n",
      "Epoch 63/500, Training Loss: -0.5185\n",
      "Epoch 64/500, Training Loss: -0.5479\n",
      "Epoch 65/500, Training Loss: -0.5690\n",
      "Epoch 66/500, Training Loss: -0.5444\n",
      "Epoch 67/500, Training Loss: -0.5220\n",
      "Epoch 68/500, Training Loss: -0.5752\n",
      "Epoch 69/500, Training Loss: -0.5146\n",
      "Epoch 70/500, Training Loss: -0.5634\n",
      "Epoch 71/500, Training Loss: -0.5331\n",
      "Epoch 72/500, Training Loss: -0.5082\n",
      "Epoch 73/500, Training Loss: -0.5188\n",
      "Epoch 74/500, Training Loss: -0.5511\n",
      "Epoch 75/500, Training Loss: -0.5227\n",
      "Epoch 76/500, Training Loss: -0.5275\n",
      "Epoch 77/500, Training Loss: -0.5516\n",
      "Epoch 78/500, Training Loss: -0.5256\n",
      "Epoch 79/500, Training Loss: -0.5796\n",
      "Epoch 80/500, Training Loss: -0.5801\n",
      "Epoch 81/500, Training Loss: -0.5713\n",
      "Epoch 82/500, Training Loss: -0.5414\n",
      "Epoch 83/500, Training Loss: -0.5452\n",
      "Epoch 84/500, Training Loss: -0.5354\n",
      "Epoch 85/500, Training Loss: -0.5259\n",
      "Epoch 86/500, Training Loss: -0.5917\n",
      "Epoch 87/500, Training Loss: -0.5160\n",
      "Epoch 88/500, Training Loss: -0.5454\n",
      "Epoch 89/500, Training Loss: -0.5770\n",
      "Epoch 90/500, Training Loss: -0.5685\n",
      "Epoch 91/500, Training Loss: -0.5091\n",
      "Epoch 92/500, Training Loss: -0.5280\n",
      "Epoch 93/500, Training Loss: -0.5246\n",
      "Epoch 94/500, Training Loss: -0.5495\n",
      "Epoch 95/500, Training Loss: -0.5459\n",
      "Epoch 96/500, Training Loss: -0.5439\n",
      "Epoch 97/500, Training Loss: -0.5195\n",
      "Epoch 98/500, Training Loss: -0.5558\n",
      "Epoch 99/500, Training Loss: -0.5492\n",
      "Epoch 100/500, Training Loss: -0.5566\n",
      "Epoch 101/500, Training Loss: -0.5413\n",
      "Epoch 102/500, Training Loss: -0.5472\n",
      "Epoch 103/500, Training Loss: -0.5432\n",
      "Epoch 104/500, Training Loss: -0.5588\n",
      "Epoch 105/500, Training Loss: -0.5375\n",
      "Epoch 106/500, Training Loss: -0.5707\n",
      "Epoch 107/500, Training Loss: -0.5896\n",
      "Epoch 108/500, Training Loss: -0.5320\n",
      "Epoch 109/500, Training Loss: -0.5339\n",
      "Epoch 110/500, Training Loss: -0.5611\n",
      "Epoch 111/500, Training Loss: -0.5317\n",
      "Epoch 112/500, Training Loss: -0.5525\n",
      "Epoch 113/500, Training Loss: -0.5263\n",
      "Epoch 114/500, Training Loss: -0.5526\n",
      "Epoch 115/500, Training Loss: -0.5837\n",
      "Epoch 116/500, Training Loss: -0.5423\n",
      "Epoch 117/500, Training Loss: -0.5381\n",
      "Epoch 118/500, Training Loss: -0.5262\n",
      "Epoch 119/500, Training Loss: -0.5606\n",
      "Epoch 120/500, Training Loss: -0.4974\n",
      "Epoch 121/500, Training Loss: -0.5234\n",
      "Epoch 122/500, Training Loss: -0.5260\n",
      "Epoch 123/500, Training Loss: -0.4956\n",
      "Epoch 124/500, Training Loss: -0.5661\n",
      "Epoch 125/500, Training Loss: -0.5657\n",
      "Epoch 126/500, Training Loss: -0.5429\n",
      "Epoch 127/500, Training Loss: -0.5363\n",
      "Epoch 128/500, Training Loss: -0.5347\n",
      "Epoch 129/500, Training Loss: -0.5306\n",
      "Epoch 130/500, Training Loss: -0.5327\n",
      "Epoch 131/500, Training Loss: -0.5268\n",
      "Epoch 132/500, Training Loss: -0.5867\n",
      "Epoch 133/500, Training Loss: -0.5412\n",
      "Epoch 134/500, Training Loss: -0.5258\n",
      "Epoch 135/500, Training Loss: -0.5703\n",
      "Epoch 136/500, Training Loss: -0.5576\n",
      "Epoch 137/500, Training Loss: -0.5669\n",
      "Epoch 138/500, Training Loss: -0.5828\n",
      "Epoch 139/500, Training Loss: -0.4999\n",
      "Epoch 140/500, Training Loss: -0.5737\n",
      "Epoch 141/500, Training Loss: -0.5424\n",
      "Epoch 142/500, Training Loss: -0.5304\n",
      "Epoch 143/500, Training Loss: -0.5259\n",
      "Epoch 144/500, Training Loss: -0.5361\n",
      "Epoch 145/500, Training Loss: -0.5967\n",
      "Epoch 146/500, Training Loss: -0.5296\n",
      "Epoch 147/500, Training Loss: -0.5106\n",
      "Epoch 148/500, Training Loss: -0.5359\n",
      "Epoch 149/500, Training Loss: -0.5371\n",
      "Epoch 150/500, Training Loss: -0.5093\n",
      "Epoch 151/500, Training Loss: -0.5653\n",
      "Epoch 152/500, Training Loss: -0.5161\n",
      "Epoch 153/500, Training Loss: -0.5688\n",
      "Epoch 154/500, Training Loss: -0.5495\n",
      "Epoch 155/500, Training Loss: -0.4969\n",
      "Epoch 156/500, Training Loss: -0.5385\n",
      "Epoch 157/500, Training Loss: -0.5470\n",
      "Epoch 158/500, Training Loss: -0.5317\n",
      "Epoch 159/500, Training Loss: -0.5349\n",
      "Epoch 160/500, Training Loss: -0.5634\n",
      "Epoch 161/500, Training Loss: -0.5607\n",
      "Epoch 162/500, Training Loss: -0.5611\n",
      "Epoch 163/500, Training Loss: -0.5502\n",
      "Epoch 164/500, Training Loss: -0.5195\n",
      "Epoch 165/500, Training Loss: -0.5219\n",
      "Epoch 166/500, Training Loss: -0.5576\n",
      "Epoch 167/500, Training Loss: -0.5386\n",
      "Epoch 168/500, Training Loss: -0.5090\n",
      "Epoch 169/500, Training Loss: -0.5390\n",
      "Epoch 170/500, Training Loss: -0.5346\n",
      "Epoch 171/500, Training Loss: -0.5430\n",
      "Epoch 172/500, Training Loss: -0.5552\n",
      "Epoch 173/500, Training Loss: -0.5840\n",
      "Epoch 174/500, Training Loss: -0.5603\n",
      "Epoch 175/500, Training Loss: -0.5363\n",
      "Epoch 176/500, Training Loss: -0.5284\n",
      "Epoch 177/500, Training Loss: -0.4880\n",
      "Epoch 178/500, Training Loss: -0.5484\n",
      "Epoch 179/500, Training Loss: -0.5274\n",
      "Epoch 180/500, Training Loss: -0.5234\n",
      "Epoch 181/500, Training Loss: -0.6038\n",
      "Epoch 182/500, Training Loss: -0.5792\n",
      "Epoch 183/500, Training Loss: -0.5618\n",
      "Epoch 184/500, Training Loss: -0.5627\n",
      "Epoch 185/500, Training Loss: -0.5589\n",
      "Epoch 186/500, Training Loss: -0.5391\n",
      "Epoch 187/500, Training Loss: -0.5690\n",
      "Epoch 188/500, Training Loss: -0.5082\n",
      "Epoch 189/500, Training Loss: -0.5546\n",
      "Epoch 190/500, Training Loss: -0.5284\n",
      "Epoch 191/500, Training Loss: -0.5138\n",
      "Epoch 192/500, Training Loss: -0.5323\n",
      "Epoch 193/500, Training Loss: -0.5486\n",
      "Epoch 194/500, Training Loss: -0.5181\n",
      "Epoch 195/500, Training Loss: -0.5769\n",
      "Epoch 196/500, Training Loss: -0.5485\n",
      "Epoch 197/500, Training Loss: -0.5489\n",
      "Epoch 198/500, Training Loss: -0.5541\n",
      "Epoch 199/500, Training Loss: -0.5254\n",
      "Epoch 200/500, Training Loss: -0.5520\n",
      "Epoch 201/500, Training Loss: -0.5342\n",
      "Epoch 202/500, Training Loss: -0.5523\n",
      "Epoch 203/500, Training Loss: -0.4889\n",
      "Epoch 204/500, Training Loss: -0.5283\n",
      "Epoch 205/500, Training Loss: -0.5734\n",
      "Epoch 206/500, Training Loss: -0.5632\n",
      "Epoch 207/500, Training Loss: -0.5506\n",
      "Epoch 208/500, Training Loss: -0.5447\n",
      "Epoch 209/500, Training Loss: -0.5316\n",
      "Epoch 210/500, Training Loss: -0.5198\n",
      "Epoch 211/500, Training Loss: -0.5892\n",
      "Epoch 212/500, Training Loss: -0.5739\n",
      "Epoch 213/500, Training Loss: -0.5522\n",
      "Epoch 214/500, Training Loss: -0.5419\n",
      "Epoch 215/500, Training Loss: -0.5472\n",
      "Epoch 216/500, Training Loss: -0.5229\n",
      "Epoch 217/500, Training Loss: -0.5531\n",
      "Epoch 218/500, Training Loss: -0.5159\n",
      "Epoch 219/500, Training Loss: -0.5554\n",
      "Epoch 220/500, Training Loss: -0.5231\n",
      "Epoch 221/500, Training Loss: -0.5519\n",
      "Epoch 222/500, Training Loss: -0.5411\n",
      "Epoch 223/500, Training Loss: -0.5298\n",
      "Epoch 224/500, Training Loss: -0.5267\n",
      "Epoch 225/500, Training Loss: -0.5400\n",
      "Epoch 226/500, Training Loss: -0.5567\n",
      "Epoch 227/500, Training Loss: -0.5126\n",
      "Epoch 228/500, Training Loss: -0.5727\n",
      "Epoch 229/500, Training Loss: -0.5485\n",
      "Epoch 230/500, Training Loss: -0.5357\n",
      "Epoch 231/500, Training Loss: -0.5168\n",
      "Epoch 232/500, Training Loss: -0.5352\n",
      "Epoch 233/500, Training Loss: -0.5072\n",
      "Epoch 234/500, Training Loss: -0.5406\n",
      "Epoch 235/500, Training Loss: -0.5476\n",
      "Epoch 236/500, Training Loss: -0.5380\n",
      "Epoch 237/500, Training Loss: -0.5292\n",
      "Epoch 238/500, Training Loss: -0.5833\n",
      "Epoch 239/500, Training Loss: -0.5674\n",
      "Epoch 240/500, Training Loss: -0.5598\n",
      "Epoch 241/500, Training Loss: -0.5184\n",
      "Epoch 242/500, Training Loss: -0.5591\n",
      "Epoch 243/500, Training Loss: -0.5189\n",
      "Epoch 244/500, Training Loss: -0.5629\n",
      "Epoch 245/500, Training Loss: -0.5391\n",
      "Epoch 246/500, Training Loss: -0.5361\n",
      "Epoch 247/500, Training Loss: -0.5275\n",
      "Epoch 248/500, Training Loss: -0.5279\n",
      "Epoch 249/500, Training Loss: -0.5110\n",
      "Epoch 250/500, Training Loss: -0.5637\n",
      "Epoch 251/500, Training Loss: -0.5491\n",
      "Epoch 252/500, Training Loss: -0.5218\n",
      "Epoch 253/500, Training Loss: -0.5473\n",
      "Epoch 254/500, Training Loss: -0.5684\n",
      "Epoch 255/500, Training Loss: -0.5125\n",
      "Epoch 256/500, Training Loss: -0.5488\n",
      "Epoch 257/500, Training Loss: -0.5339\n",
      "Epoch 258/500, Training Loss: -0.5180\n",
      "Epoch 259/500, Training Loss: -0.5393\n",
      "Epoch 260/500, Training Loss: -0.5158\n",
      "Epoch 261/500, Training Loss: -0.5237\n",
      "Epoch 262/500, Training Loss: -0.5618\n",
      "Epoch 263/500, Training Loss: -0.5531\n",
      "Epoch 264/500, Training Loss: -0.5760\n",
      "Epoch 265/500, Training Loss: -0.5414\n",
      "Epoch 266/500, Training Loss: -0.4977\n",
      "Epoch 267/500, Training Loss: -0.5360\n",
      "Epoch 268/500, Training Loss: -0.5602\n",
      "Epoch 269/500, Training Loss: -0.5655\n",
      "Epoch 270/500, Training Loss: -0.5127\n",
      "Epoch 271/500, Training Loss: -0.5502\n",
      "Epoch 272/500, Training Loss: -0.5097\n",
      "Epoch 273/500, Training Loss: -0.5540\n",
      "Epoch 274/500, Training Loss: -0.5600\n",
      "Epoch 275/500, Training Loss: -0.5213\n",
      "Epoch 276/500, Training Loss: -0.5612\n",
      "Epoch 277/500, Training Loss: -0.5672\n",
      "Epoch 278/500, Training Loss: -0.5484\n",
      "Epoch 279/500, Training Loss: -0.5420\n",
      "Epoch 280/500, Training Loss: -0.5208\n",
      "Epoch 281/500, Training Loss: -0.5408\n",
      "Epoch 282/500, Training Loss: -0.5432\n",
      "Epoch 283/500, Training Loss: -0.5358\n",
      "Epoch 284/500, Training Loss: -0.5746\n",
      "Epoch 285/500, Training Loss: -0.5160\n",
      "Epoch 286/500, Training Loss: -0.5976\n",
      "Epoch 287/500, Training Loss: -0.5182\n",
      "Epoch 288/500, Training Loss: -0.5231\n",
      "Epoch 289/500, Training Loss: -0.5523\n",
      "Epoch 290/500, Training Loss: -0.5340\n",
      "Epoch 291/500, Training Loss: -0.5605\n",
      "Epoch 292/500, Training Loss: -0.5670\n",
      "Epoch 293/500, Training Loss: -0.5546\n",
      "Epoch 294/500, Training Loss: -0.5471\n",
      "Epoch 295/500, Training Loss: -0.5341\n",
      "Epoch 296/500, Training Loss: -0.5766\n",
      "Epoch 297/500, Training Loss: -0.5747\n",
      "Epoch 298/500, Training Loss: -0.5506\n",
      "Epoch 299/500, Training Loss: -0.5534\n",
      "Epoch 300/500, Training Loss: -0.5510\n",
      "Epoch 301/500, Training Loss: -0.5672\n",
      "Epoch 302/500, Training Loss: -0.5277\n",
      "Epoch 303/500, Training Loss: -0.5448\n",
      "Epoch 304/500, Training Loss: -0.5632\n",
      "Epoch 305/500, Training Loss: -0.5043\n",
      "Epoch 306/500, Training Loss: -0.5527\n",
      "Epoch 307/500, Training Loss: -0.5301\n",
      "Epoch 308/500, Training Loss: -0.5241\n",
      "Epoch 309/500, Training Loss: -0.5451\n",
      "Epoch 310/500, Training Loss: -0.5578\n",
      "Epoch 311/500, Training Loss: -0.5544\n",
      "Epoch 312/500, Training Loss: -0.5305\n",
      "Epoch 313/500, Training Loss: -0.5485\n",
      "Epoch 314/500, Training Loss: -0.5644\n",
      "Epoch 315/500, Training Loss: -0.5411\n",
      "Epoch 316/500, Training Loss: -0.5358\n",
      "Epoch 317/500, Training Loss: -0.5418\n",
      "Epoch 318/500, Training Loss: -0.5253\n",
      "Epoch 319/500, Training Loss: -0.5422\n",
      "Epoch 320/500, Training Loss: -0.5599\n",
      "Epoch 321/500, Training Loss: -0.5115\n",
      "Epoch 322/500, Training Loss: -0.5224\n",
      "Epoch 323/500, Training Loss: -0.5573\n",
      "Epoch 324/500, Training Loss: -0.5375\n",
      "Epoch 325/500, Training Loss: -0.5216\n",
      "Epoch 326/500, Training Loss: -0.5135\n",
      "Epoch 327/500, Training Loss: -0.4940\n",
      "Epoch 328/500, Training Loss: -0.5590\n",
      "Epoch 329/500, Training Loss: -0.5285\n",
      "Epoch 330/500, Training Loss: -0.5981\n",
      "Epoch 331/500, Training Loss: -0.5290\n",
      "Epoch 332/500, Training Loss: -0.5544\n",
      "Epoch 333/500, Training Loss: -0.5244\n",
      "Epoch 334/500, Training Loss: -0.5331\n",
      "Epoch 335/500, Training Loss: -0.5637\n",
      "Epoch 336/500, Training Loss: -0.5693\n",
      "Epoch 337/500, Training Loss: -0.4952\n",
      "Epoch 338/500, Training Loss: -0.5468\n",
      "Epoch 339/500, Training Loss: -0.5785\n",
      "Epoch 340/500, Training Loss: -0.5528\n",
      "Epoch 341/500, Training Loss: -0.5439\n",
      "Epoch 342/500, Training Loss: -0.5640\n",
      "Epoch 343/500, Training Loss: -0.5574\n",
      "Epoch 344/500, Training Loss: -0.5321\n",
      "Epoch 345/500, Training Loss: -0.5511\n",
      "Epoch 346/500, Training Loss: -0.5511\n",
      "Epoch 347/500, Training Loss: -0.5261\n",
      "Epoch 348/500, Training Loss: -0.5680\n",
      "Epoch 349/500, Training Loss: -0.5504\n",
      "Epoch 350/500, Training Loss: -0.5091\n",
      "Epoch 351/500, Training Loss: -0.5454\n",
      "Epoch 352/500, Training Loss: -0.5559\n",
      "Epoch 353/500, Training Loss: -0.5192\n",
      "Epoch 354/500, Training Loss: -0.5434\n",
      "Epoch 355/500, Training Loss: -0.5395\n",
      "Epoch 356/500, Training Loss: -0.5649\n",
      "Epoch 357/500, Training Loss: -0.5610\n",
      "Epoch 358/500, Training Loss: -0.5301\n",
      "Epoch 359/500, Training Loss: -0.5893\n",
      "Epoch 360/500, Training Loss: -0.5555\n",
      "Epoch 361/500, Training Loss: -0.5585\n",
      "Epoch 362/500, Training Loss: -0.5449\n",
      "Epoch 363/500, Training Loss: -0.5663\n",
      "Epoch 364/500, Training Loss: -0.5547\n",
      "Epoch 365/500, Training Loss: -0.5547\n",
      "Epoch 366/500, Training Loss: -0.5604\n",
      "Epoch 367/500, Training Loss: -0.5280\n",
      "Epoch 368/500, Training Loss: -0.5509\n",
      "Epoch 369/500, Training Loss: -0.5485\n",
      "Epoch 370/500, Training Loss: -0.5741\n",
      "Epoch 371/500, Training Loss: -0.5234\n",
      "Epoch 372/500, Training Loss: -0.5381\n",
      "Epoch 373/500, Training Loss: -0.5004\n",
      "Epoch 374/500, Training Loss: -0.5713\n",
      "Epoch 375/500, Training Loss: -0.6106\n",
      "Epoch 376/500, Training Loss: -0.5100\n",
      "Epoch 377/500, Training Loss: -0.6132\n",
      "Epoch 378/500, Training Loss: -0.5305\n",
      "Epoch 379/500, Training Loss: -0.5320\n",
      "Epoch 380/500, Training Loss: -0.5449\n",
      "Epoch 381/500, Training Loss: -0.5656\n",
      "Epoch 382/500, Training Loss: -0.5033\n",
      "Epoch 383/500, Training Loss: -0.5262\n",
      "Epoch 384/500, Training Loss: -0.5426\n",
      "Epoch 385/500, Training Loss: -0.5484\n",
      "Epoch 386/500, Training Loss: -0.5308\n",
      "Epoch 387/500, Training Loss: -0.5440\n",
      "Epoch 388/500, Training Loss: -0.5245\n",
      "Epoch 389/500, Training Loss: -0.5529\n",
      "Epoch 390/500, Training Loss: -0.5436\n",
      "Epoch 391/500, Training Loss: -0.5392\n",
      "Epoch 392/500, Training Loss: -0.5333\n",
      "Epoch 393/500, Training Loss: -0.5535\n",
      "Epoch 394/500, Training Loss: -0.5645\n",
      "Epoch 395/500, Training Loss: -0.5322\n",
      "Epoch 396/500, Training Loss: -0.5293\n",
      "Epoch 397/500, Training Loss: -0.5216\n",
      "Epoch 398/500, Training Loss: -0.5415\n",
      "Epoch 399/500, Training Loss: -0.5662\n",
      "Epoch 400/500, Training Loss: -0.5452\n",
      "Epoch 401/500, Training Loss: -0.5721\n",
      "Epoch 402/500, Training Loss: -0.5172\n",
      "Epoch 403/500, Training Loss: -0.5699\n",
      "Epoch 404/500, Training Loss: -0.5545\n",
      "Epoch 405/500, Training Loss: -0.5172\n",
      "Epoch 406/500, Training Loss: -0.5706\n",
      "Epoch 407/500, Training Loss: -0.5249\n",
      "Epoch 408/500, Training Loss: -0.5177\n",
      "Epoch 409/500, Training Loss: -0.5979\n",
      "Epoch 410/500, Training Loss: -0.4982\n",
      "Epoch 411/500, Training Loss: -0.5266\n",
      "Epoch 412/500, Training Loss: -0.5349\n",
      "Epoch 413/500, Training Loss: -0.5687\n",
      "Epoch 414/500, Training Loss: -0.5715\n",
      "Epoch 415/500, Training Loss: -0.5416\n",
      "Epoch 416/500, Training Loss: -0.4746\n",
      "Epoch 417/500, Training Loss: -0.4939\n",
      "Epoch 418/500, Training Loss: -0.5558\n",
      "Epoch 419/500, Training Loss: -0.5332\n",
      "Epoch 420/500, Training Loss: -0.5092\n",
      "Epoch 421/500, Training Loss: -0.5557\n",
      "Epoch 422/500, Training Loss: -0.5181\n",
      "Epoch 423/500, Training Loss: -0.5325\n",
      "Epoch 424/500, Training Loss: -0.5562\n",
      "Epoch 425/500, Training Loss: -0.5345\n",
      "Epoch 426/500, Training Loss: -0.5372\n",
      "Epoch 427/500, Training Loss: -0.5254\n",
      "Epoch 428/500, Training Loss: -0.5346\n",
      "Epoch 429/500, Training Loss: -0.5209\n",
      "Epoch 430/500, Training Loss: -0.5273\n",
      "Epoch 431/500, Training Loss: -0.5456\n",
      "Epoch 432/500, Training Loss: -0.5224\n",
      "Epoch 433/500, Training Loss: -0.5663\n",
      "Epoch 434/500, Training Loss: -0.5336\n",
      "Epoch 435/500, Training Loss: -0.4887\n",
      "Epoch 436/500, Training Loss: -0.5440\n",
      "Epoch 437/500, Training Loss: -0.5432\n",
      "Epoch 438/500, Training Loss: -0.5819\n",
      "Epoch 439/500, Training Loss: -0.5082\n",
      "Epoch 440/500, Training Loss: -0.5678\n",
      "Epoch 441/500, Training Loss: -0.5267\n",
      "Epoch 442/500, Training Loss: -0.5390\n",
      "Epoch 443/500, Training Loss: -0.5770\n",
      "Epoch 444/500, Training Loss: -0.5579\n",
      "Epoch 445/500, Training Loss: -0.5352\n",
      "Epoch 446/500, Training Loss: -0.5511\n",
      "Epoch 447/500, Training Loss: -0.5415\n",
      "Epoch 448/500, Training Loss: -0.5043\n",
      "Epoch 449/500, Training Loss: -0.5313\n",
      "Epoch 450/500, Training Loss: -0.5461\n",
      "Epoch 451/500, Training Loss: -0.5531\n",
      "Epoch 452/500, Training Loss: -0.5490\n",
      "Epoch 453/500, Training Loss: -0.5191\n",
      "Epoch 454/500, Training Loss: -0.5741\n",
      "Epoch 455/500, Training Loss: -0.5327\n",
      "Epoch 456/500, Training Loss: -0.5614\n",
      "Epoch 457/500, Training Loss: -0.5467\n",
      "Epoch 458/500, Training Loss: -0.5667\n",
      "Epoch 459/500, Training Loss: -0.5345\n",
      "Epoch 460/500, Training Loss: -0.5306\n",
      "Epoch 461/500, Training Loss: -0.5741\n",
      "Epoch 462/500, Training Loss: -0.5754\n",
      "Epoch 463/500, Training Loss: -0.5101\n",
      "Epoch 464/500, Training Loss: -0.5446\n",
      "Epoch 465/500, Training Loss: -0.5419\n",
      "Epoch 466/500, Training Loss: -0.4870\n",
      "Epoch 467/500, Training Loss: -0.5438\n",
      "Epoch 468/500, Training Loss: -0.5541\n",
      "Epoch 469/500, Training Loss: -0.5476\n",
      "Epoch 470/500, Training Loss: -0.5584\n",
      "Epoch 471/500, Training Loss: -0.5245\n",
      "Epoch 472/500, Training Loss: -0.5261\n",
      "Epoch 473/500, Training Loss: -0.5243\n",
      "Epoch 474/500, Training Loss: -0.5670\n",
      "Epoch 475/500, Training Loss: -0.5351\n",
      "Epoch 476/500, Training Loss: -0.5180\n",
      "Epoch 477/500, Training Loss: -0.5875\n",
      "Epoch 478/500, Training Loss: -0.5276\n",
      "Epoch 479/500, Training Loss: -0.5090\n",
      "Epoch 480/500, Training Loss: -0.5367\n",
      "Epoch 481/500, Training Loss: -0.5430\n",
      "Epoch 482/500, Training Loss: -0.5516\n",
      "Epoch 483/500, Training Loss: -0.5270\n",
      "Epoch 484/500, Training Loss: -0.5120\n",
      "Epoch 485/500, Training Loss: -0.5653\n",
      "Epoch 486/500, Training Loss: -0.5731\n",
      "Epoch 487/500, Training Loss: -0.5986\n",
      "Epoch 488/500, Training Loss: -0.5202\n",
      "Epoch 489/500, Training Loss: -0.5363\n",
      "Epoch 490/500, Training Loss: -0.5687\n",
      "Epoch 491/500, Training Loss: -0.5334\n",
      "Epoch 492/500, Training Loss: -0.5063\n",
      "Epoch 493/500, Training Loss: -0.5426\n",
      "Epoch 494/500, Training Loss: -0.5524\n",
      "Epoch 495/500, Training Loss: -0.5326\n",
      "Epoch 496/500, Training Loss: -0.5525\n",
      "Epoch 497/500, Training Loss: -0.5774\n",
      "Epoch 498/500, Training Loss: -0.5329\n",
      "Epoch 499/500, Training Loss: -0.5099\n",
      "Epoch 500/500, Training Loss: -0.4955\n",
      "Validation Loss: -0.5617\n",
      "Model 1, Fold 1: Validation Loss = -0.5617\n",
      "Epoch 1/500, Training Loss: -0.4016\n",
      "Epoch 2/500, Training Loss: -0.4902\n",
      "Epoch 3/500, Training Loss: -0.5052\n",
      "Epoch 4/500, Training Loss: -0.5375\n",
      "Epoch 5/500, Training Loss: -0.5601\n",
      "Epoch 6/500, Training Loss: -0.5647\n",
      "Epoch 7/500, Training Loss: -0.5531\n",
      "Epoch 8/500, Training Loss: -0.6036\n",
      "Epoch 9/500, Training Loss: -0.5535\n",
      "Epoch 10/500, Training Loss: -0.5790\n",
      "Epoch 11/500, Training Loss: -0.5410\n",
      "Epoch 12/500, Training Loss: -0.5417\n",
      "Epoch 13/500, Training Loss: -0.5480\n",
      "Epoch 14/500, Training Loss: -0.5741\n",
      "Epoch 15/500, Training Loss: -0.5423\n",
      "Epoch 16/500, Training Loss: -0.5322\n",
      "Epoch 17/500, Training Loss: -0.5130\n",
      "Epoch 18/500, Training Loss: -0.4768\n",
      "Epoch 19/500, Training Loss: -0.5851\n",
      "Epoch 20/500, Training Loss: -0.5419\n",
      "Epoch 21/500, Training Loss: -0.5949\n",
      "Epoch 22/500, Training Loss: -0.5110\n",
      "Epoch 23/500, Training Loss: -0.5800\n",
      "Epoch 24/500, Training Loss: -0.5195\n",
      "Epoch 25/500, Training Loss: -0.5849\n",
      "Epoch 26/500, Training Loss: -0.5577\n",
      "Epoch 27/500, Training Loss: -0.5306\n",
      "Epoch 28/500, Training Loss: -0.5604\n",
      "Epoch 29/500, Training Loss: -0.5161\n",
      "Epoch 30/500, Training Loss: -0.5734\n",
      "Epoch 31/500, Training Loss: -0.5700\n",
      "Epoch 32/500, Training Loss: -0.5856\n",
      "Epoch 33/500, Training Loss: -0.5768\n",
      "Epoch 34/500, Training Loss: -0.5660\n",
      "Epoch 35/500, Training Loss: -0.5388\n",
      "Epoch 36/500, Training Loss: -0.5528\n",
      "Epoch 37/500, Training Loss: -0.6154\n",
      "Epoch 38/500, Training Loss: -0.5427\n",
      "Epoch 39/500, Training Loss: -0.5941\n",
      "Epoch 40/500, Training Loss: -0.5230\n",
      "Epoch 41/500, Training Loss: -0.5667\n",
      "Epoch 42/500, Training Loss: -0.5643\n",
      "Epoch 43/500, Training Loss: -0.5301\n",
      "Epoch 44/500, Training Loss: -0.5538\n",
      "Epoch 45/500, Training Loss: -0.5758\n",
      "Epoch 46/500, Training Loss: -0.5967\n",
      "Epoch 47/500, Training Loss: -0.5317\n",
      "Epoch 48/500, Training Loss: -0.6092\n",
      "Epoch 49/500, Training Loss: -0.5446\n",
      "Epoch 50/500, Training Loss: -0.5608\n",
      "Epoch 51/500, Training Loss: -0.5592\n",
      "Epoch 52/500, Training Loss: -0.5557\n",
      "Epoch 53/500, Training Loss: -0.5222\n",
      "Epoch 54/500, Training Loss: -0.5703\n",
      "Epoch 55/500, Training Loss: -0.5163\n",
      "Epoch 56/500, Training Loss: -0.5364\n",
      "Epoch 57/500, Training Loss: -0.5275\n",
      "Epoch 58/500, Training Loss: -0.5273\n",
      "Epoch 59/500, Training Loss: -0.5643\n",
      "Epoch 60/500, Training Loss: -0.6027\n",
      "Epoch 61/500, Training Loss: -0.5587\n",
      "Epoch 62/500, Training Loss: -0.5764\n",
      "Epoch 63/500, Training Loss: -0.5564\n",
      "Epoch 64/500, Training Loss: -0.5386\n",
      "Epoch 65/500, Training Loss: -0.5841\n",
      "Epoch 66/500, Training Loss: -0.5995\n",
      "Epoch 67/500, Training Loss: -0.5608\n",
      "Epoch 68/500, Training Loss: -0.5847\n",
      "Epoch 69/500, Training Loss: -0.5776\n",
      "Epoch 70/500, Training Loss: -0.5034\n",
      "Epoch 71/500, Training Loss: -0.6088\n",
      "Epoch 72/500, Training Loss: -0.5775\n",
      "Epoch 73/500, Training Loss: -0.6111\n",
      "Epoch 74/500, Training Loss: -0.5261\n",
      "Epoch 75/500, Training Loss: -0.5911\n",
      "Epoch 76/500, Training Loss: -0.5625\n",
      "Epoch 77/500, Training Loss: -0.5491\n",
      "Epoch 78/500, Training Loss: -0.5215\n",
      "Epoch 79/500, Training Loss: -0.5593\n",
      "Epoch 80/500, Training Loss: -0.5696\n",
      "Epoch 81/500, Training Loss: -0.5663\n",
      "Epoch 82/500, Training Loss: -0.5315\n",
      "Epoch 83/500, Training Loss: -0.5423\n",
      "Epoch 84/500, Training Loss: -0.5249\n",
      "Epoch 85/500, Training Loss: -0.5741\n",
      "Epoch 86/500, Training Loss: -0.5525\n",
      "Epoch 87/500, Training Loss: -0.5544\n",
      "Epoch 88/500, Training Loss: -0.5720\n",
      "Epoch 89/500, Training Loss: -0.5532\n",
      "Epoch 90/500, Training Loss: -0.5813\n",
      "Epoch 91/500, Training Loss: -0.5928\n",
      "Epoch 92/500, Training Loss: -0.5558\n",
      "Epoch 93/500, Training Loss: -0.5976\n",
      "Epoch 94/500, Training Loss: -0.5492\n",
      "Epoch 95/500, Training Loss: -0.5259\n",
      "Epoch 96/500, Training Loss: -0.5715\n",
      "Epoch 97/500, Training Loss: -0.5493\n",
      "Epoch 98/500, Training Loss: -0.5687\n",
      "Epoch 99/500, Training Loss: -0.5729\n",
      "Epoch 100/500, Training Loss: -0.5767\n",
      "Epoch 101/500, Training Loss: -0.5310\n",
      "Epoch 102/500, Training Loss: -0.5513\n",
      "Epoch 103/500, Training Loss: -0.6077\n",
      "Epoch 104/500, Training Loss: -0.5732\n",
      "Epoch 105/500, Training Loss: -0.5570\n",
      "Epoch 106/500, Training Loss: -0.5588\n",
      "Epoch 107/500, Training Loss: -0.5626\n",
      "Epoch 108/500, Training Loss: -0.5898\n",
      "Epoch 109/500, Training Loss: -0.5546\n",
      "Epoch 110/500, Training Loss: -0.5886\n",
      "Epoch 111/500, Training Loss: -0.5580\n",
      "Epoch 112/500, Training Loss: -0.5944\n",
      "Epoch 113/500, Training Loss: -0.5807\n",
      "Epoch 114/500, Training Loss: -0.5653\n",
      "Epoch 115/500, Training Loss: -0.5581\n",
      "Epoch 116/500, Training Loss: -0.5379\n",
      "Epoch 117/500, Training Loss: -0.5753\n",
      "Epoch 118/500, Training Loss: -0.5992\n",
      "Epoch 119/500, Training Loss: -0.5842\n",
      "Epoch 120/500, Training Loss: -0.5704\n",
      "Epoch 121/500, Training Loss: -0.5795\n",
      "Epoch 122/500, Training Loss: -0.5786\n",
      "Epoch 123/500, Training Loss: -0.4995\n",
      "Epoch 124/500, Training Loss: -0.5864\n",
      "Epoch 125/500, Training Loss: -0.5459\n",
      "Epoch 126/500, Training Loss: -0.5718\n",
      "Epoch 127/500, Training Loss: -0.5700\n",
      "Epoch 128/500, Training Loss: -0.5198\n",
      "Epoch 129/500, Training Loss: -0.5824\n",
      "Epoch 130/500, Training Loss: -0.5596\n",
      "Epoch 131/500, Training Loss: -0.6260\n",
      "Epoch 132/500, Training Loss: -0.5536\n",
      "Epoch 133/500, Training Loss: -0.5625\n",
      "Epoch 134/500, Training Loss: -0.5889\n",
      "Epoch 135/500, Training Loss: -0.5570\n",
      "Epoch 136/500, Training Loss: -0.5552\n",
      "Epoch 137/500, Training Loss: -0.5251\n",
      "Epoch 138/500, Training Loss: -0.5772\n",
      "Epoch 139/500, Training Loss: -0.5256\n",
      "Epoch 140/500, Training Loss: -0.5331\n",
      "Epoch 141/500, Training Loss: -0.5666\n",
      "Epoch 142/500, Training Loss: -0.5793\n",
      "Epoch 143/500, Training Loss: -0.5399\n",
      "Epoch 144/500, Training Loss: -0.5392\n",
      "Epoch 145/500, Training Loss: -0.5553\n",
      "Epoch 146/500, Training Loss: -0.5548\n",
      "Epoch 147/500, Training Loss: -0.5647\n",
      "Epoch 148/500, Training Loss: -0.5867\n",
      "Epoch 149/500, Training Loss: -0.5598\n",
      "Epoch 150/500, Training Loss: -0.5782\n",
      "Epoch 151/500, Training Loss: -0.5390\n",
      "Epoch 152/500, Training Loss: -0.5775\n",
      "Epoch 153/500, Training Loss: -0.5420\n",
      "Epoch 154/500, Training Loss: -0.5394\n",
      "Epoch 155/500, Training Loss: -0.6108\n",
      "Epoch 156/500, Training Loss: -0.5320\n",
      "Epoch 157/500, Training Loss: -0.5662\n",
      "Epoch 158/500, Training Loss: -0.5777\n",
      "Epoch 159/500, Training Loss: -0.5715\n",
      "Epoch 160/500, Training Loss: -0.6052\n",
      "Epoch 161/500, Training Loss: -0.5509\n",
      "Epoch 162/500, Training Loss: -0.5831\n",
      "Epoch 163/500, Training Loss: -0.5637\n",
      "Epoch 164/500, Training Loss: -0.5548\n",
      "Epoch 165/500, Training Loss: -0.5943\n",
      "Epoch 166/500, Training Loss: -0.5853\n",
      "Epoch 167/500, Training Loss: -0.5448\n",
      "Epoch 168/500, Training Loss: -0.5689\n",
      "Epoch 169/500, Training Loss: -0.5770\n",
      "Epoch 170/500, Training Loss: -0.5621\n",
      "Epoch 171/500, Training Loss: -0.5797\n",
      "Epoch 172/500, Training Loss: -0.5476\n",
      "Epoch 173/500, Training Loss: -0.5999\n",
      "Epoch 174/500, Training Loss: -0.5504\n",
      "Epoch 175/500, Training Loss: -0.5530\n",
      "Epoch 176/500, Training Loss: -0.5425\n",
      "Epoch 177/500, Training Loss: -0.5414\n",
      "Epoch 178/500, Training Loss: -0.5406\n",
      "Epoch 179/500, Training Loss: -0.5673\n",
      "Epoch 180/500, Training Loss: -0.5715\n",
      "Epoch 181/500, Training Loss: -0.5843\n",
      "Epoch 182/500, Training Loss: -0.5533\n",
      "Epoch 183/500, Training Loss: -0.5759\n",
      "Epoch 184/500, Training Loss: -0.5824\n",
      "Epoch 185/500, Training Loss: -0.5419\n",
      "Epoch 186/500, Training Loss: -0.5807\n",
      "Epoch 187/500, Training Loss: -0.5521\n",
      "Epoch 188/500, Training Loss: -0.5922\n",
      "Epoch 189/500, Training Loss: -0.5466\n",
      "Epoch 190/500, Training Loss: -0.5707\n",
      "Epoch 191/500, Training Loss: -0.5575\n",
      "Epoch 192/500, Training Loss: -0.6014\n",
      "Epoch 193/500, Training Loss: -0.5786\n",
      "Epoch 194/500, Training Loss: -0.5864\n",
      "Epoch 195/500, Training Loss: -0.5731\n",
      "Epoch 196/500, Training Loss: -0.5440\n",
      "Epoch 197/500, Training Loss: -0.5998\n",
      "Epoch 198/500, Training Loss: -0.5842\n",
      "Epoch 199/500, Training Loss: -0.5472\n",
      "Epoch 200/500, Training Loss: -0.6071\n",
      "Epoch 201/500, Training Loss: -0.6056\n",
      "Epoch 202/500, Training Loss: -0.5562\n",
      "Epoch 203/500, Training Loss: -0.6022\n",
      "Epoch 204/500, Training Loss: -0.5856\n",
      "Epoch 205/500, Training Loss: -0.6083\n",
      "Epoch 206/500, Training Loss: -0.5726\n",
      "Epoch 207/500, Training Loss: -0.5320\n",
      "Epoch 208/500, Training Loss: -0.5836\n",
      "Epoch 209/500, Training Loss: -0.5723\n",
      "Epoch 210/500, Training Loss: -0.5869\n",
      "Epoch 211/500, Training Loss: -0.5864\n",
      "Epoch 212/500, Training Loss: -0.5720\n",
      "Epoch 213/500, Training Loss: -0.5643\n",
      "Epoch 214/500, Training Loss: -0.5784\n",
      "Epoch 215/500, Training Loss: -0.5578\n",
      "Epoch 216/500, Training Loss: -0.5775\n",
      "Epoch 217/500, Training Loss: -0.5831\n",
      "Epoch 218/500, Training Loss: -0.5732\n",
      "Epoch 219/500, Training Loss: -0.5909\n",
      "Epoch 220/500, Training Loss: -0.5260\n",
      "Epoch 221/500, Training Loss: -0.6083\n",
      "Epoch 222/500, Training Loss: -0.5895\n",
      "Epoch 223/500, Training Loss: -0.5510\n",
      "Epoch 224/500, Training Loss: -0.5166\n",
      "Epoch 225/500, Training Loss: -0.5448\n",
      "Epoch 226/500, Training Loss: -0.5721\n",
      "Epoch 227/500, Training Loss: -0.6011\n",
      "Epoch 228/500, Training Loss: -0.5629\n",
      "Epoch 229/500, Training Loss: -0.5554\n",
      "Epoch 230/500, Training Loss: -0.5522\n",
      "Epoch 231/500, Training Loss: -0.5569\n",
      "Epoch 232/500, Training Loss: -0.5751\n",
      "Epoch 233/500, Training Loss: -0.5811\n",
      "Epoch 234/500, Training Loss: -0.5466\n",
      "Epoch 235/500, Training Loss: -0.5663\n",
      "Epoch 236/500, Training Loss: -0.5706\n",
      "Epoch 237/500, Training Loss: -0.5612\n",
      "Epoch 238/500, Training Loss: -0.5837\n",
      "Epoch 239/500, Training Loss: -0.5461\n",
      "Epoch 240/500, Training Loss: -0.5827\n",
      "Epoch 241/500, Training Loss: -0.5726\n",
      "Epoch 242/500, Training Loss: -0.5818\n",
      "Epoch 243/500, Training Loss: -0.5568\n",
      "Epoch 244/500, Training Loss: -0.5381\n",
      "Epoch 245/500, Training Loss: -0.5244\n",
      "Epoch 246/500, Training Loss: -0.5677\n",
      "Epoch 247/500, Training Loss: -0.5881\n",
      "Epoch 248/500, Training Loss: -0.5823\n",
      "Epoch 249/500, Training Loss: -0.5716\n",
      "Epoch 250/500, Training Loss: -0.5739\n",
      "Epoch 251/500, Training Loss: -0.5295\n",
      "Epoch 252/500, Training Loss: -0.5455\n",
      "Epoch 253/500, Training Loss: -0.6044\n",
      "Epoch 254/500, Training Loss: -0.5489\n",
      "Epoch 255/500, Training Loss: -0.5844\n",
      "Epoch 256/500, Training Loss: -0.5449\n",
      "Epoch 257/500, Training Loss: -0.5869\n",
      "Epoch 258/500, Training Loss: -0.5348\n",
      "Epoch 259/500, Training Loss: -0.5750\n",
      "Epoch 260/500, Training Loss: -0.5477\n",
      "Epoch 261/500, Training Loss: -0.5740\n",
      "Epoch 262/500, Training Loss: -0.5718\n",
      "Epoch 263/500, Training Loss: -0.5701\n",
      "Epoch 264/500, Training Loss: -0.5444\n",
      "Epoch 265/500, Training Loss: -0.5889\n",
      "Epoch 266/500, Training Loss: -0.5565\n",
      "Epoch 267/500, Training Loss: -0.5600\n",
      "Epoch 268/500, Training Loss: -0.5683\n",
      "Epoch 269/500, Training Loss: -0.5625\n",
      "Epoch 270/500, Training Loss: -0.5481\n",
      "Epoch 271/500, Training Loss: -0.5696\n",
      "Epoch 272/500, Training Loss: -0.5483\n",
      "Epoch 273/500, Training Loss: -0.5741\n",
      "Epoch 274/500, Training Loss: -0.5730\n",
      "Epoch 275/500, Training Loss: -0.5690\n",
      "Epoch 276/500, Training Loss: -0.5759\n",
      "Epoch 277/500, Training Loss: -0.5794\n",
      "Epoch 278/500, Training Loss: -0.5451\n",
      "Epoch 279/500, Training Loss: -0.5368\n",
      "Epoch 280/500, Training Loss: -0.5721\n",
      "Epoch 281/500, Training Loss: -0.5606\n",
      "Epoch 282/500, Training Loss: -0.5303\n",
      "Epoch 283/500, Training Loss: -0.5981\n",
      "Epoch 284/500, Training Loss: -0.5665\n",
      "Epoch 285/500, Training Loss: -0.5864\n",
      "Epoch 286/500, Training Loss: -0.5992\n",
      "Epoch 287/500, Training Loss: -0.6028\n",
      "Epoch 288/500, Training Loss: -0.5610\n",
      "Epoch 289/500, Training Loss: -0.5729\n",
      "Epoch 290/500, Training Loss: -0.5797\n",
      "Epoch 291/500, Training Loss: -0.6007\n",
      "Epoch 292/500, Training Loss: -0.6012\n",
      "Epoch 293/500, Training Loss: -0.5948\n",
      "Epoch 294/500, Training Loss: -0.5745\n",
      "Epoch 295/500, Training Loss: -0.5917\n",
      "Epoch 296/500, Training Loss: -0.5487\n",
      "Epoch 297/500, Training Loss: -0.5664\n",
      "Epoch 298/500, Training Loss: -0.5814\n",
      "Epoch 299/500, Training Loss: -0.5625\n",
      "Epoch 300/500, Training Loss: -0.5466\n",
      "Epoch 301/500, Training Loss: -0.5609\n",
      "Epoch 302/500, Training Loss: -0.5490\n",
      "Epoch 303/500, Training Loss: -0.5961\n",
      "Epoch 304/500, Training Loss: -0.6200\n",
      "Epoch 305/500, Training Loss: -0.5704\n",
      "Epoch 306/500, Training Loss: -0.6179\n",
      "Epoch 307/500, Training Loss: -0.5577\n",
      "Epoch 308/500, Training Loss: -0.5629\n",
      "Epoch 309/500, Training Loss: -0.5510\n",
      "Epoch 310/500, Training Loss: -0.5507\n",
      "Epoch 311/500, Training Loss: -0.5626\n",
      "Epoch 312/500, Training Loss: -0.5739\n",
      "Epoch 313/500, Training Loss: -0.5459\n",
      "Epoch 314/500, Training Loss: -0.5654\n",
      "Epoch 315/500, Training Loss: -0.5901\n",
      "Epoch 316/500, Training Loss: -0.5338\n",
      "Epoch 317/500, Training Loss: -0.5664\n",
      "Epoch 318/500, Training Loss: -0.5270\n",
      "Epoch 319/500, Training Loss: -0.5575\n",
      "Epoch 320/500, Training Loss: -0.5342\n",
      "Epoch 321/500, Training Loss: -0.5157\n",
      "Epoch 322/500, Training Loss: -0.5707\n",
      "Epoch 323/500, Training Loss: -0.5624\n",
      "Epoch 324/500, Training Loss: -0.6129\n",
      "Epoch 325/500, Training Loss: -0.5604\n",
      "Epoch 326/500, Training Loss: -0.5459\n",
      "Epoch 327/500, Training Loss: -0.5741\n",
      "Epoch 328/500, Training Loss: -0.5798\n",
      "Epoch 329/500, Training Loss: -0.5486\n",
      "Epoch 330/500, Training Loss: -0.5408\n",
      "Epoch 331/500, Training Loss: -0.5738\n",
      "Epoch 332/500, Training Loss: -0.5386\n",
      "Epoch 333/500, Training Loss: -0.5783\n",
      "Epoch 334/500, Training Loss: -0.5685\n",
      "Epoch 335/500, Training Loss: -0.6123\n",
      "Epoch 336/500, Training Loss: -0.5533\n",
      "Epoch 337/500, Training Loss: -0.5489\n",
      "Epoch 338/500, Training Loss: -0.5420\n",
      "Epoch 339/500, Training Loss: -0.5913\n",
      "Epoch 340/500, Training Loss: -0.5426\n",
      "Epoch 341/500, Training Loss: -0.6046\n",
      "Epoch 342/500, Training Loss: -0.5718\n",
      "Epoch 343/500, Training Loss: -0.5706\n",
      "Epoch 344/500, Training Loss: -0.5762\n",
      "Epoch 345/500, Training Loss: -0.5394\n",
      "Epoch 346/500, Training Loss: -0.5927\n",
      "Epoch 347/500, Training Loss: -0.5252\n",
      "Epoch 348/500, Training Loss: -0.5285\n",
      "Epoch 349/500, Training Loss: -0.5587\n",
      "Epoch 350/500, Training Loss: -0.5757\n",
      "Epoch 351/500, Training Loss: -0.5425\n",
      "Epoch 352/500, Training Loss: -0.5629\n",
      "Epoch 353/500, Training Loss: -0.5008\n",
      "Epoch 354/500, Training Loss: -0.5405\n",
      "Epoch 355/500, Training Loss: -0.5494\n",
      "Epoch 356/500, Training Loss: -0.5518\n",
      "Epoch 357/500, Training Loss: -0.5427\n",
      "Epoch 358/500, Training Loss: -0.5843\n",
      "Epoch 359/500, Training Loss: -0.5325\n",
      "Epoch 360/500, Training Loss: -0.5926\n",
      "Epoch 361/500, Training Loss: -0.5787\n",
      "Epoch 362/500, Training Loss: -0.5737\n",
      "Epoch 363/500, Training Loss: -0.5702\n",
      "Epoch 364/500, Training Loss: -0.5697\n",
      "Epoch 365/500, Training Loss: -0.5847\n",
      "Epoch 366/500, Training Loss: -0.5907\n",
      "Epoch 367/500, Training Loss: -0.5776\n",
      "Epoch 368/500, Training Loss: -0.5761\n",
      "Epoch 369/500, Training Loss: -0.5789\n",
      "Epoch 370/500, Training Loss: -0.5482\n",
      "Epoch 371/500, Training Loss: -0.5480\n",
      "Epoch 372/500, Training Loss: -0.5547\n",
      "Epoch 373/500, Training Loss: -0.5398\n",
      "Epoch 374/500, Training Loss: -0.5934\n",
      "Epoch 375/500, Training Loss: -0.5531\n",
      "Epoch 376/500, Training Loss: -0.5576\n",
      "Epoch 377/500, Training Loss: -0.5544\n",
      "Epoch 378/500, Training Loss: -0.5290\n",
      "Epoch 379/500, Training Loss: -0.5384\n",
      "Epoch 380/500, Training Loss: -0.5464\n",
      "Epoch 381/500, Training Loss: -0.5450\n",
      "Epoch 382/500, Training Loss: -0.5353\n",
      "Epoch 383/500, Training Loss: -0.6048\n",
      "Epoch 384/500, Training Loss: -0.5763\n",
      "Epoch 385/500, Training Loss: -0.5466\n",
      "Epoch 386/500, Training Loss: -0.5920\n",
      "Epoch 387/500, Training Loss: -0.5672\n",
      "Epoch 388/500, Training Loss: -0.5395\n",
      "Epoch 389/500, Training Loss: -0.5481\n",
      "Epoch 390/500, Training Loss: -0.5584\n",
      "Epoch 391/500, Training Loss: -0.5699\n",
      "Epoch 392/500, Training Loss: -0.5458\n",
      "Epoch 393/500, Training Loss: -0.5691\n",
      "Epoch 394/500, Training Loss: -0.5851\n",
      "Epoch 395/500, Training Loss: -0.5728\n",
      "Epoch 396/500, Training Loss: -0.5646\n",
      "Epoch 397/500, Training Loss: -0.5869\n",
      "Epoch 398/500, Training Loss: -0.5983\n",
      "Epoch 399/500, Training Loss: -0.5580\n",
      "Epoch 400/500, Training Loss: -0.5575\n",
      "Epoch 401/500, Training Loss: -0.6177\n",
      "Epoch 402/500, Training Loss: -0.5947\n",
      "Epoch 403/500, Training Loss: -0.5708\n",
      "Epoch 404/500, Training Loss: -0.5815\n",
      "Epoch 405/500, Training Loss: -0.5699\n",
      "Epoch 406/500, Training Loss: -0.5569\n",
      "Epoch 407/500, Training Loss: -0.5840\n",
      "Epoch 408/500, Training Loss: -0.5776\n",
      "Epoch 409/500, Training Loss: -0.5786\n",
      "Epoch 410/500, Training Loss: -0.5963\n",
      "Epoch 411/500, Training Loss: -0.5798\n",
      "Epoch 412/500, Training Loss: -0.5720\n",
      "Epoch 413/500, Training Loss: -0.5901\n",
      "Epoch 414/500, Training Loss: -0.5746\n",
      "Epoch 415/500, Training Loss: -0.6137\n",
      "Epoch 416/500, Training Loss: -0.5466\n",
      "Epoch 417/500, Training Loss: -0.5796\n",
      "Epoch 418/500, Training Loss: -0.5755\n",
      "Epoch 419/500, Training Loss: -0.5748\n",
      "Epoch 420/500, Training Loss: -0.6192\n",
      "Epoch 421/500, Training Loss: -0.5670\n",
      "Epoch 422/500, Training Loss: -0.5820\n",
      "Epoch 423/500, Training Loss: -0.5953\n",
      "Epoch 424/500, Training Loss: -0.5878\n",
      "Epoch 425/500, Training Loss: -0.5798\n",
      "Epoch 426/500, Training Loss: -0.6096\n",
      "Epoch 427/500, Training Loss: -0.5917\n",
      "Epoch 428/500, Training Loss: -0.5465\n",
      "Epoch 429/500, Training Loss: -0.5680\n",
      "Epoch 430/500, Training Loss: -0.5592\n",
      "Epoch 431/500, Training Loss: -0.5664\n",
      "Epoch 432/500, Training Loss: -0.5884\n",
      "Epoch 433/500, Training Loss: -0.5924\n",
      "Epoch 434/500, Training Loss: -0.5718\n",
      "Epoch 435/500, Training Loss: -0.5950\n",
      "Epoch 436/500, Training Loss: -0.5762\n",
      "Epoch 437/500, Training Loss: -0.5591\n",
      "Epoch 438/500, Training Loss: -0.6006\n",
      "Epoch 439/500, Training Loss: -0.5934\n",
      "Epoch 440/500, Training Loss: -0.5248\n",
      "Epoch 441/500, Training Loss: -0.5837\n",
      "Epoch 442/500, Training Loss: -0.5607\n",
      "Epoch 443/500, Training Loss: -0.5747\n",
      "Epoch 444/500, Training Loss: -0.5520\n",
      "Epoch 445/500, Training Loss: -0.5852\n",
      "Epoch 446/500, Training Loss: -0.5721\n",
      "Epoch 447/500, Training Loss: -0.5515\n",
      "Epoch 448/500, Training Loss: -0.5963\n",
      "Epoch 449/500, Training Loss: -0.5814\n",
      "Epoch 450/500, Training Loss: -0.6084\n",
      "Epoch 451/500, Training Loss: -0.5197\n",
      "Epoch 452/500, Training Loss: -0.5347\n",
      "Epoch 453/500, Training Loss: -0.5540\n",
      "Epoch 454/500, Training Loss: -0.5754\n",
      "Epoch 455/500, Training Loss: -0.5644\n",
      "Epoch 456/500, Training Loss: -0.6269\n",
      "Epoch 457/500, Training Loss: -0.5670\n",
      "Epoch 458/500, Training Loss: -0.6006\n",
      "Epoch 459/500, Training Loss: -0.5474\n",
      "Epoch 460/500, Training Loss: -0.5752\n",
      "Epoch 461/500, Training Loss: -0.5651\n",
      "Epoch 462/500, Training Loss: -0.5861\n",
      "Epoch 463/500, Training Loss: -0.6015\n",
      "Epoch 464/500, Training Loss: -0.5490\n",
      "Epoch 465/500, Training Loss: -0.5493\n",
      "Epoch 466/500, Training Loss: -0.5901\n",
      "Epoch 467/500, Training Loss: -0.5627\n",
      "Epoch 468/500, Training Loss: -0.5612\n",
      "Epoch 469/500, Training Loss: -0.5752\n",
      "Epoch 470/500, Training Loss: -0.5749\n",
      "Epoch 471/500, Training Loss: -0.5700\n",
      "Epoch 472/500, Training Loss: -0.5407\n",
      "Epoch 473/500, Training Loss: -0.5417\n",
      "Epoch 474/500, Training Loss: -0.6004\n",
      "Epoch 475/500, Training Loss: -0.5977\n",
      "Epoch 476/500, Training Loss: -0.5738\n",
      "Epoch 477/500, Training Loss: -0.5497\n",
      "Epoch 478/500, Training Loss: -0.5838\n",
      "Epoch 479/500, Training Loss: -0.5778\n",
      "Epoch 480/500, Training Loss: -0.5573\n",
      "Epoch 481/500, Training Loss: -0.5652\n",
      "Epoch 482/500, Training Loss: -0.5672\n",
      "Epoch 483/500, Training Loss: -0.5809\n",
      "Epoch 484/500, Training Loss: -0.5526\n",
      "Epoch 485/500, Training Loss: -0.5689\n",
      "Epoch 486/500, Training Loss: -0.5467\n",
      "Epoch 487/500, Training Loss: -0.5623\n",
      "Epoch 488/500, Training Loss: -0.5767\n",
      "Epoch 489/500, Training Loss: -0.5858\n",
      "Epoch 490/500, Training Loss: -0.5776\n",
      "Epoch 491/500, Training Loss: -0.6069\n",
      "Epoch 492/500, Training Loss: -0.5591\n",
      "Epoch 493/500, Training Loss: -0.5747\n",
      "Epoch 494/500, Training Loss: -0.5941\n",
      "Epoch 495/500, Training Loss: -0.5610\n",
      "Epoch 496/500, Training Loss: -0.5853\n",
      "Epoch 497/500, Training Loss: -0.5515\n",
      "Epoch 498/500, Training Loss: -0.6008\n",
      "Epoch 499/500, Training Loss: -0.5989\n",
      "Epoch 500/500, Training Loss: -0.5574\n",
      "Validation Loss: -0.5414\n",
      "Model 2, Fold 0: Validation Loss = -0.5414\n",
      "Epoch 1/500, Training Loss: -0.3454\n",
      "Epoch 2/500, Training Loss: -0.4572\n",
      "Epoch 3/500, Training Loss: -0.5139\n",
      "Epoch 4/500, Training Loss: -0.5366\n",
      "Epoch 5/500, Training Loss: -0.5411\n",
      "Epoch 6/500, Training Loss: -0.5291\n",
      "Epoch 7/500, Training Loss: -0.5540\n",
      "Epoch 8/500, Training Loss: -0.4890\n",
      "Epoch 9/500, Training Loss: -0.5623\n",
      "Epoch 10/500, Training Loss: -0.5500\n",
      "Epoch 11/500, Training Loss: -0.5591\n",
      "Epoch 12/500, Training Loss: -0.5459\n",
      "Epoch 13/500, Training Loss: -0.5374\n",
      "Epoch 14/500, Training Loss: -0.5407\n",
      "Epoch 15/500, Training Loss: -0.5156\n",
      "Epoch 16/500, Training Loss: -0.5707\n",
      "Epoch 17/500, Training Loss: -0.5310\n",
      "Epoch 18/500, Training Loss: -0.5451\n",
      "Epoch 19/500, Training Loss: -0.5613\n",
      "Epoch 20/500, Training Loss: -0.5213\n",
      "Epoch 21/500, Training Loss: -0.5107\n",
      "Epoch 22/500, Training Loss: -0.5608\n",
      "Epoch 23/500, Training Loss: -0.5603\n",
      "Epoch 24/500, Training Loss: -0.5516\n",
      "Epoch 25/500, Training Loss: -0.5387\n",
      "Epoch 26/500, Training Loss: -0.5289\n",
      "Epoch 27/500, Training Loss: -0.5247\n",
      "Epoch 28/500, Training Loss: -0.5337\n",
      "Epoch 29/500, Training Loss: -0.4869\n",
      "Epoch 30/500, Training Loss: -0.5658\n",
      "Epoch 31/500, Training Loss: -0.4926\n",
      "Epoch 32/500, Training Loss: -0.5438\n",
      "Epoch 33/500, Training Loss: -0.5188\n",
      "Epoch 34/500, Training Loss: -0.5636\n",
      "Epoch 35/500, Training Loss: -0.5280\n",
      "Epoch 36/500, Training Loss: -0.5850\n",
      "Epoch 37/500, Training Loss: -0.5432\n",
      "Epoch 38/500, Training Loss: -0.5498\n",
      "Epoch 39/500, Training Loss: -0.5267\n",
      "Epoch 40/500, Training Loss: -0.5612\n",
      "Epoch 41/500, Training Loss: -0.5609\n",
      "Epoch 42/500, Training Loss: -0.5455\n",
      "Epoch 43/500, Training Loss: -0.5415\n",
      "Epoch 44/500, Training Loss: -0.5559\n",
      "Epoch 45/500, Training Loss: -0.4865\n",
      "Epoch 46/500, Training Loss: -0.5418\n",
      "Epoch 47/500, Training Loss: -0.5316\n",
      "Epoch 48/500, Training Loss: -0.5188\n",
      "Epoch 49/500, Training Loss: -0.5543\n",
      "Epoch 50/500, Training Loss: -0.5493\n",
      "Epoch 51/500, Training Loss: -0.4976\n",
      "Epoch 52/500, Training Loss: -0.5885\n",
      "Epoch 53/500, Training Loss: -0.5248\n",
      "Epoch 54/500, Training Loss: -0.5318\n",
      "Epoch 55/500, Training Loss: -0.5429\n",
      "Epoch 56/500, Training Loss: -0.5561\n",
      "Epoch 57/500, Training Loss: -0.5633\n",
      "Epoch 58/500, Training Loss: -0.5460\n",
      "Epoch 59/500, Training Loss: -0.5178\n",
      "Epoch 60/500, Training Loss: -0.5246\n",
      "Epoch 61/500, Training Loss: -0.5862\n",
      "Epoch 62/500, Training Loss: -0.5727\n",
      "Epoch 63/500, Training Loss: -0.5204\n",
      "Epoch 64/500, Training Loss: -0.5235\n",
      "Epoch 65/500, Training Loss: -0.5683\n",
      "Epoch 66/500, Training Loss: -0.5685\n",
      "Epoch 67/500, Training Loss: -0.5576\n",
      "Epoch 68/500, Training Loss: -0.5783\n",
      "Epoch 69/500, Training Loss: -0.5672\n",
      "Epoch 70/500, Training Loss: -0.5206\n",
      "Epoch 71/500, Training Loss: -0.5333\n",
      "Epoch 72/500, Training Loss: -0.5420\n",
      "Epoch 73/500, Training Loss: -0.5329\n",
      "Epoch 74/500, Training Loss: -0.5525\n",
      "Epoch 75/500, Training Loss: -0.5367\n",
      "Epoch 76/500, Training Loss: -0.5708\n",
      "Epoch 77/500, Training Loss: -0.5997\n",
      "Epoch 78/500, Training Loss: -0.5571\n",
      "Epoch 79/500, Training Loss: -0.5414\n",
      "Epoch 80/500, Training Loss: -0.5246\n",
      "Epoch 81/500, Training Loss: -0.5599\n",
      "Epoch 82/500, Training Loss: -0.5501\n",
      "Epoch 83/500, Training Loss: -0.5391\n",
      "Epoch 84/500, Training Loss: -0.5463\n",
      "Epoch 85/500, Training Loss: -0.5058\n",
      "Epoch 86/500, Training Loss: -0.5241\n",
      "Epoch 87/500, Training Loss: -0.5612\n",
      "Epoch 88/500, Training Loss: -0.5764\n",
      "Epoch 89/500, Training Loss: -0.5847\n",
      "Epoch 90/500, Training Loss: -0.5616\n",
      "Epoch 91/500, Training Loss: -0.5463\n",
      "Epoch 92/500, Training Loss: -0.4980\n",
      "Epoch 93/500, Training Loss: -0.5329\n",
      "Epoch 94/500, Training Loss: -0.5202\n",
      "Epoch 95/500, Training Loss: -0.5447\n",
      "Epoch 96/500, Training Loss: -0.5402\n",
      "Epoch 97/500, Training Loss: -0.5521\n",
      "Epoch 98/500, Training Loss: -0.5359\n",
      "Epoch 99/500, Training Loss: -0.5367\n",
      "Epoch 100/500, Training Loss: -0.4926\n",
      "Epoch 101/500, Training Loss: -0.5829\n",
      "Epoch 102/500, Training Loss: -0.5251\n",
      "Epoch 103/500, Training Loss: -0.5272\n",
      "Epoch 104/500, Training Loss: -0.5391\n",
      "Epoch 105/500, Training Loss: -0.5533\n",
      "Epoch 106/500, Training Loss: -0.5620\n",
      "Epoch 107/500, Training Loss: -0.5216\n",
      "Epoch 108/500, Training Loss: -0.5627\n",
      "Epoch 109/500, Training Loss: -0.5633\n",
      "Epoch 110/500, Training Loss: -0.5433\n",
      "Epoch 111/500, Training Loss: -0.5167\n",
      "Epoch 112/500, Training Loss: -0.5779\n",
      "Epoch 113/500, Training Loss: -0.5691\n",
      "Epoch 114/500, Training Loss: -0.5628\n",
      "Epoch 115/500, Training Loss: -0.5365\n",
      "Epoch 116/500, Training Loss: -0.5383\n",
      "Epoch 117/500, Training Loss: -0.5506\n",
      "Epoch 118/500, Training Loss: -0.5486\n",
      "Epoch 119/500, Training Loss: -0.5373\n",
      "Epoch 120/500, Training Loss: -0.5596\n",
      "Epoch 121/500, Training Loss: -0.5518\n",
      "Epoch 122/500, Training Loss: -0.5731\n",
      "Epoch 123/500, Training Loss: -0.5342\n",
      "Epoch 124/500, Training Loss: -0.5553\n",
      "Epoch 125/500, Training Loss: -0.5773\n",
      "Epoch 126/500, Training Loss: -0.5629\n",
      "Epoch 127/500, Training Loss: -0.5003\n",
      "Epoch 128/500, Training Loss: -0.6102\n",
      "Epoch 129/500, Training Loss: -0.5462\n",
      "Epoch 130/500, Training Loss: -0.5612\n",
      "Epoch 131/500, Training Loss: -0.5564\n",
      "Epoch 132/500, Training Loss: -0.5280\n",
      "Epoch 133/500, Training Loss: -0.5390\n",
      "Epoch 134/500, Training Loss: -0.5226\n",
      "Epoch 135/500, Training Loss: -0.5811\n",
      "Epoch 136/500, Training Loss: -0.5901\n",
      "Epoch 137/500, Training Loss: -0.5047\n",
      "Epoch 138/500, Training Loss: -0.5732\n",
      "Epoch 139/500, Training Loss: -0.5771\n",
      "Epoch 140/500, Training Loss: -0.5744\n",
      "Epoch 141/500, Training Loss: -0.5459\n",
      "Epoch 142/500, Training Loss: -0.5523\n",
      "Epoch 143/500, Training Loss: -0.5211\n",
      "Epoch 144/500, Training Loss: -0.5267\n",
      "Epoch 145/500, Training Loss: -0.5429\n",
      "Epoch 146/500, Training Loss: -0.5266\n",
      "Epoch 147/500, Training Loss: -0.5099\n",
      "Epoch 148/500, Training Loss: -0.5655\n",
      "Epoch 149/500, Training Loss: -0.5903\n",
      "Epoch 150/500, Training Loss: -0.5647\n",
      "Epoch 151/500, Training Loss: -0.5220\n",
      "Epoch 152/500, Training Loss: -0.5607\n",
      "Epoch 153/500, Training Loss: -0.5268\n",
      "Epoch 154/500, Training Loss: -0.5388\n",
      "Epoch 155/500, Training Loss: -0.5110\n",
      "Epoch 156/500, Training Loss: -0.5533\n",
      "Epoch 157/500, Training Loss: -0.5250\n",
      "Epoch 158/500, Training Loss: -0.5601\n",
      "Epoch 159/500, Training Loss: -0.5381\n",
      "Epoch 160/500, Training Loss: -0.5812\n",
      "Epoch 161/500, Training Loss: -0.5143\n",
      "Epoch 162/500, Training Loss: -0.5646\n",
      "Epoch 163/500, Training Loss: -0.5624\n",
      "Epoch 164/500, Training Loss: -0.5094\n",
      "Epoch 165/500, Training Loss: -0.5392\n",
      "Epoch 166/500, Training Loss: -0.5216\n",
      "Epoch 167/500, Training Loss: -0.5606\n",
      "Epoch 168/500, Training Loss: -0.5157\n",
      "Epoch 169/500, Training Loss: -0.5407\n",
      "Epoch 170/500, Training Loss: -0.5306\n",
      "Epoch 171/500, Training Loss: -0.5555\n",
      "Epoch 172/500, Training Loss: -0.5498\n",
      "Epoch 173/500, Training Loss: -0.5477\n",
      "Epoch 174/500, Training Loss: -0.5272\n",
      "Epoch 175/500, Training Loss: -0.5730\n",
      "Epoch 176/500, Training Loss: -0.5667\n",
      "Epoch 177/500, Training Loss: -0.5280\n",
      "Epoch 178/500, Training Loss: -0.5458\n",
      "Epoch 179/500, Training Loss: -0.5493\n",
      "Epoch 180/500, Training Loss: -0.5479\n",
      "Epoch 181/500, Training Loss: -0.5515\n",
      "Epoch 182/500, Training Loss: -0.5482\n",
      "Epoch 183/500, Training Loss: -0.5956\n",
      "Epoch 184/500, Training Loss: -0.5184\n",
      "Epoch 185/500, Training Loss: -0.5691\n",
      "Epoch 186/500, Training Loss: -0.5859\n",
      "Epoch 187/500, Training Loss: -0.5521\n",
      "Epoch 188/500, Training Loss: -0.5446\n",
      "Epoch 189/500, Training Loss: -0.5606\n",
      "Epoch 190/500, Training Loss: -0.5613\n",
      "Epoch 191/500, Training Loss: -0.5344\n",
      "Epoch 192/500, Training Loss: -0.5415\n",
      "Epoch 193/500, Training Loss: -0.5312\n",
      "Epoch 194/500, Training Loss: -0.5677\n",
      "Epoch 195/500, Training Loss: -0.5430\n",
      "Epoch 196/500, Training Loss: -0.5582\n",
      "Epoch 197/500, Training Loss: -0.5475\n",
      "Epoch 198/500, Training Loss: -0.5329\n",
      "Epoch 199/500, Training Loss: -0.5534\n",
      "Epoch 200/500, Training Loss: -0.5676\n",
      "Epoch 201/500, Training Loss: -0.5454\n",
      "Epoch 202/500, Training Loss: -0.5422\n",
      "Epoch 203/500, Training Loss: -0.5561\n",
      "Epoch 204/500, Training Loss: -0.5615\n",
      "Epoch 205/500, Training Loss: -0.5308\n",
      "Epoch 206/500, Training Loss: -0.5791\n",
      "Epoch 207/500, Training Loss: -0.5810\n",
      "Epoch 208/500, Training Loss: -0.5488\n",
      "Epoch 209/500, Training Loss: -0.5221\n",
      "Epoch 210/500, Training Loss: -0.5728\n",
      "Epoch 211/500, Training Loss: -0.5729\n",
      "Epoch 212/500, Training Loss: -0.5718\n",
      "Epoch 213/500, Training Loss: -0.5358\n",
      "Epoch 214/500, Training Loss: -0.5381\n",
      "Epoch 215/500, Training Loss: -0.5500\n",
      "Epoch 216/500, Training Loss: -0.5296\n",
      "Epoch 217/500, Training Loss: -0.5585\n",
      "Epoch 218/500, Training Loss: -0.5335\n",
      "Epoch 219/500, Training Loss: -0.5888\n",
      "Epoch 220/500, Training Loss: -0.5155\n",
      "Epoch 221/500, Training Loss: -0.5220\n",
      "Epoch 222/500, Training Loss: -0.5972\n",
      "Epoch 223/500, Training Loss: -0.5358\n",
      "Epoch 224/500, Training Loss: -0.5351\n",
      "Epoch 225/500, Training Loss: -0.5389\n",
      "Epoch 226/500, Training Loss: -0.5467\n",
      "Epoch 227/500, Training Loss: -0.5696\n",
      "Epoch 228/500, Training Loss: -0.5411\n",
      "Epoch 229/500, Training Loss: -0.5724\n",
      "Epoch 230/500, Training Loss: -0.5773\n",
      "Epoch 231/500, Training Loss: -0.5648\n",
      "Epoch 232/500, Training Loss: -0.5294\n",
      "Epoch 233/500, Training Loss: -0.4900\n",
      "Epoch 234/500, Training Loss: -0.5663\n",
      "Epoch 235/500, Training Loss: -0.5448\n",
      "Epoch 236/500, Training Loss: -0.5702\n",
      "Epoch 237/500, Training Loss: -0.5639\n",
      "Epoch 238/500, Training Loss: -0.5068\n",
      "Epoch 239/500, Training Loss: -0.5231\n",
      "Epoch 240/500, Training Loss: -0.5520\n",
      "Epoch 241/500, Training Loss: -0.5561\n",
      "Epoch 242/500, Training Loss: -0.5438\n",
      "Epoch 243/500, Training Loss: -0.5223\n",
      "Epoch 244/500, Training Loss: -0.5476\n",
      "Epoch 245/500, Training Loss: -0.5565\n",
      "Epoch 246/500, Training Loss: -0.5302\n",
      "Epoch 247/500, Training Loss: -0.5680\n",
      "Epoch 248/500, Training Loss: -0.5757\n",
      "Epoch 249/500, Training Loss: -0.5964\n",
      "Epoch 250/500, Training Loss: -0.5650\n",
      "Epoch 251/500, Training Loss: -0.5485\n",
      "Epoch 252/500, Training Loss: -0.6029\n",
      "Epoch 253/500, Training Loss: -0.5365\n",
      "Epoch 254/500, Training Loss: -0.5427\n",
      "Epoch 255/500, Training Loss: -0.5720\n",
      "Epoch 256/500, Training Loss: -0.5774\n",
      "Epoch 257/500, Training Loss: -0.5772\n",
      "Epoch 258/500, Training Loss: -0.5553\n",
      "Epoch 259/500, Training Loss: -0.5441\n",
      "Epoch 260/500, Training Loss: -0.5059\n",
      "Epoch 261/500, Training Loss: -0.5452\n",
      "Epoch 262/500, Training Loss: -0.5451\n",
      "Epoch 263/500, Training Loss: -0.5684\n",
      "Epoch 264/500, Training Loss: -0.5458\n",
      "Epoch 265/500, Training Loss: -0.5515\n",
      "Epoch 266/500, Training Loss: -0.5687\n",
      "Epoch 267/500, Training Loss: -0.5645\n",
      "Epoch 268/500, Training Loss: -0.5044\n",
      "Epoch 269/500, Training Loss: -0.5357\n",
      "Epoch 270/500, Training Loss: -0.5342\n",
      "Epoch 271/500, Training Loss: -0.5419\n",
      "Epoch 272/500, Training Loss: -0.5766\n",
      "Epoch 273/500, Training Loss: -0.5656\n",
      "Epoch 274/500, Training Loss: -0.5769\n",
      "Epoch 275/500, Training Loss: -0.5429\n",
      "Epoch 276/500, Training Loss: -0.5879\n",
      "Epoch 277/500, Training Loss: -0.5954\n",
      "Epoch 278/500, Training Loss: -0.5750\n",
      "Epoch 279/500, Training Loss: -0.5490\n",
      "Epoch 280/500, Training Loss: -0.5285\n",
      "Epoch 281/500, Training Loss: -0.5165\n",
      "Epoch 282/500, Training Loss: -0.6050\n",
      "Epoch 283/500, Training Loss: -0.5458\n",
      "Epoch 284/500, Training Loss: -0.5690\n",
      "Epoch 285/500, Training Loss: -0.5882\n",
      "Epoch 286/500, Training Loss: -0.5434\n",
      "Epoch 287/500, Training Loss: -0.5543\n",
      "Epoch 288/500, Training Loss: -0.4952\n",
      "Epoch 289/500, Training Loss: -0.5052\n",
      "Epoch 290/500, Training Loss: -0.5457\n",
      "Epoch 291/500, Training Loss: -0.5315\n",
      "Epoch 292/500, Training Loss: -0.5619\n",
      "Epoch 293/500, Training Loss: -0.5499\n",
      "Epoch 294/500, Training Loss: -0.5897\n",
      "Epoch 295/500, Training Loss: -0.5080\n",
      "Epoch 296/500, Training Loss: -0.5345\n",
      "Epoch 297/500, Training Loss: -0.5780\n",
      "Epoch 298/500, Training Loss: -0.5216\n",
      "Epoch 299/500, Training Loss: -0.5531\n",
      "Epoch 300/500, Training Loss: -0.5048\n",
      "Epoch 301/500, Training Loss: -0.5583\n",
      "Epoch 302/500, Training Loss: -0.5073\n",
      "Epoch 303/500, Training Loss: -0.5797\n",
      "Epoch 304/500, Training Loss: -0.5442\n",
      "Epoch 305/500, Training Loss: -0.5722\n",
      "Epoch 306/500, Training Loss: -0.5386\n",
      "Epoch 307/500, Training Loss: -0.5471\n",
      "Epoch 308/500, Training Loss: -0.5286\n",
      "Epoch 309/500, Training Loss: -0.5253\n",
      "Epoch 310/500, Training Loss: -0.5309\n",
      "Epoch 311/500, Training Loss: -0.5332\n",
      "Epoch 312/500, Training Loss: -0.5205\n",
      "Epoch 313/500, Training Loss: -0.5515\n",
      "Epoch 314/500, Training Loss: -0.5484\n",
      "Epoch 315/500, Training Loss: -0.5615\n",
      "Epoch 316/500, Training Loss: -0.5346\n",
      "Epoch 317/500, Training Loss: -0.5118\n",
      "Epoch 318/500, Training Loss: -0.5432\n",
      "Epoch 319/500, Training Loss: -0.5862\n",
      "Epoch 320/500, Training Loss: -0.5615\n",
      "Epoch 321/500, Training Loss: -0.4974\n",
      "Epoch 322/500, Training Loss: -0.5543\n",
      "Epoch 323/500, Training Loss: -0.5293\n",
      "Epoch 324/500, Training Loss: -0.5491\n",
      "Epoch 325/500, Training Loss: -0.5345\n",
      "Epoch 326/500, Training Loss: -0.5438\n",
      "Epoch 327/500, Training Loss: -0.5181\n",
      "Epoch 328/500, Training Loss: -0.5330\n",
      "Epoch 329/500, Training Loss: -0.5575\n",
      "Epoch 330/500, Training Loss: -0.5475\n",
      "Epoch 331/500, Training Loss: -0.5754\n",
      "Epoch 332/500, Training Loss: -0.5249\n",
      "Epoch 333/500, Training Loss: -0.5308\n",
      "Epoch 334/500, Training Loss: -0.5560\n",
      "Epoch 335/500, Training Loss: -0.5466\n",
      "Epoch 336/500, Training Loss: -0.5691\n",
      "Epoch 337/500, Training Loss: -0.5114\n",
      "Epoch 338/500, Training Loss: -0.5547\n",
      "Epoch 339/500, Training Loss: -0.5655\n",
      "Epoch 340/500, Training Loss: -0.5645\n",
      "Epoch 341/500, Training Loss: -0.5202\n",
      "Epoch 342/500, Training Loss: -0.5702\n",
      "Epoch 343/500, Training Loss: -0.4979\n",
      "Epoch 344/500, Training Loss: -0.5676\n",
      "Epoch 345/500, Training Loss: -0.5462\n",
      "Epoch 346/500, Training Loss: -0.5929\n",
      "Epoch 347/500, Training Loss: -0.5467\n",
      "Epoch 348/500, Training Loss: -0.5911\n",
      "Epoch 349/500, Training Loss: -0.5566\n",
      "Epoch 350/500, Training Loss: -0.5047\n",
      "Epoch 351/500, Training Loss: -0.5594\n",
      "Epoch 352/500, Training Loss: -0.5434\n",
      "Epoch 353/500, Training Loss: -0.5783\n",
      "Epoch 354/500, Training Loss: -0.5409\n",
      "Epoch 355/500, Training Loss: -0.5344\n",
      "Epoch 356/500, Training Loss: -0.5489\n",
      "Epoch 357/500, Training Loss: -0.5844\n",
      "Epoch 358/500, Training Loss: -0.5407\n",
      "Epoch 359/500, Training Loss: -0.5617\n",
      "Epoch 360/500, Training Loss: -0.5037\n",
      "Epoch 361/500, Training Loss: -0.5220\n",
      "Epoch 362/500, Training Loss: -0.5540\n",
      "Epoch 363/500, Training Loss: -0.5997\n",
      "Epoch 364/500, Training Loss: -0.5515\n",
      "Epoch 365/500, Training Loss: -0.5196\n",
      "Epoch 366/500, Training Loss: -0.5794\n",
      "Epoch 367/500, Training Loss: -0.5259\n",
      "Epoch 368/500, Training Loss: -0.5694\n",
      "Epoch 369/500, Training Loss: -0.5499\n",
      "Epoch 370/500, Training Loss: -0.5153\n",
      "Epoch 371/500, Training Loss: -0.5356\n",
      "Epoch 372/500, Training Loss: -0.5557\n",
      "Epoch 373/500, Training Loss: -0.5654\n",
      "Epoch 374/500, Training Loss: -0.5282\n",
      "Epoch 375/500, Training Loss: -0.5502\n",
      "Epoch 376/500, Training Loss: -0.5659\n",
      "Epoch 377/500, Training Loss: -0.5530\n",
      "Epoch 378/500, Training Loss: -0.5328\n",
      "Epoch 379/500, Training Loss: -0.5461\n",
      "Epoch 380/500, Training Loss: -0.5351\n",
      "Epoch 381/500, Training Loss: -0.5132\n",
      "Epoch 382/500, Training Loss: -0.5654\n",
      "Epoch 383/500, Training Loss: -0.5902\n",
      "Epoch 384/500, Training Loss: -0.5557\n",
      "Epoch 385/500, Training Loss: -0.5879\n",
      "Epoch 386/500, Training Loss: -0.5600\n",
      "Epoch 387/500, Training Loss: -0.5659\n",
      "Epoch 388/500, Training Loss: -0.5780\n",
      "Epoch 389/500, Training Loss: -0.5453\n",
      "Epoch 390/500, Training Loss: -0.5629\n",
      "Epoch 391/500, Training Loss: -0.5235\n",
      "Epoch 392/500, Training Loss: -0.5537\n",
      "Epoch 393/500, Training Loss: -0.5829\n",
      "Epoch 394/500, Training Loss: -0.5527\n",
      "Epoch 395/500, Training Loss: -0.5290\n",
      "Epoch 396/500, Training Loss: -0.5437\n",
      "Epoch 397/500, Training Loss: -0.5713\n",
      "Epoch 398/500, Training Loss: -0.5559\n",
      "Epoch 399/500, Training Loss: -0.5395\n",
      "Epoch 400/500, Training Loss: -0.5862\n",
      "Epoch 401/500, Training Loss: -0.5254\n",
      "Epoch 402/500, Training Loss: -0.5349\n",
      "Epoch 403/500, Training Loss: -0.5480\n",
      "Epoch 404/500, Training Loss: -0.5662\n",
      "Epoch 405/500, Training Loss: -0.5234\n",
      "Epoch 406/500, Training Loss: -0.5213\n",
      "Epoch 407/500, Training Loss: -0.5537\n",
      "Epoch 408/500, Training Loss: -0.5261\n",
      "Epoch 409/500, Training Loss: -0.5526\n",
      "Epoch 410/500, Training Loss: -0.5480\n",
      "Epoch 411/500, Training Loss: -0.5277\n",
      "Epoch 412/500, Training Loss: -0.5634\n",
      "Epoch 413/500, Training Loss: -0.5618\n",
      "Epoch 414/500, Training Loss: -0.5587\n",
      "Epoch 415/500, Training Loss: -0.5770\n",
      "Epoch 416/500, Training Loss: -0.5193\n",
      "Epoch 417/500, Training Loss: -0.5724\n",
      "Epoch 418/500, Training Loss: -0.5538\n",
      "Epoch 419/500, Training Loss: -0.5402\n",
      "Epoch 420/500, Training Loss: -0.5446\n",
      "Epoch 421/500, Training Loss: -0.5474\n",
      "Epoch 422/500, Training Loss: -0.5530\n",
      "Epoch 423/500, Training Loss: -0.5534\n",
      "Epoch 424/500, Training Loss: -0.5972\n",
      "Epoch 425/500, Training Loss: -0.5369\n",
      "Epoch 426/500, Training Loss: -0.5596\n",
      "Epoch 427/500, Training Loss: -0.5728\n",
      "Epoch 428/500, Training Loss: -0.5742\n",
      "Epoch 429/500, Training Loss: -0.5582\n",
      "Epoch 430/500, Training Loss: -0.5596\n",
      "Epoch 431/500, Training Loss: -0.5537\n",
      "Epoch 432/500, Training Loss: -0.5391\n",
      "Epoch 433/500, Training Loss: -0.5508\n",
      "Epoch 434/500, Training Loss: -0.5168\n",
      "Epoch 435/500, Training Loss: -0.5497\n",
      "Epoch 436/500, Training Loss: -0.5749\n",
      "Epoch 437/500, Training Loss: -0.5369\n",
      "Epoch 438/500, Training Loss: -0.5590\n",
      "Epoch 439/500, Training Loss: -0.5737\n",
      "Epoch 440/500, Training Loss: -0.5472\n",
      "Epoch 441/500, Training Loss: -0.5809\n",
      "Epoch 442/500, Training Loss: -0.5463\n",
      "Epoch 443/500, Training Loss: -0.5512\n",
      "Epoch 444/500, Training Loss: -0.5111\n",
      "Epoch 445/500, Training Loss: -0.5433\n",
      "Epoch 446/500, Training Loss: -0.5530\n",
      "Epoch 447/500, Training Loss: -0.5450\n",
      "Epoch 448/500, Training Loss: -0.5379\n",
      "Epoch 449/500, Training Loss: -0.5765\n",
      "Epoch 450/500, Training Loss: -0.5868\n",
      "Epoch 451/500, Training Loss: -0.5139\n",
      "Epoch 452/500, Training Loss: -0.5772\n",
      "Epoch 453/500, Training Loss: -0.5927\n",
      "Epoch 454/500, Training Loss: -0.5357\n",
      "Epoch 455/500, Training Loss: -0.5854\n",
      "Epoch 456/500, Training Loss: -0.5957\n",
      "Epoch 457/500, Training Loss: -0.5221\n",
      "Epoch 458/500, Training Loss: -0.5544\n",
      "Epoch 459/500, Training Loss: -0.5277\n",
      "Epoch 460/500, Training Loss: -0.5421\n",
      "Epoch 461/500, Training Loss: -0.4934\n",
      "Epoch 462/500, Training Loss: -0.5282\n",
      "Epoch 463/500, Training Loss: -0.5072\n",
      "Epoch 464/500, Training Loss: -0.5138\n",
      "Epoch 465/500, Training Loss: -0.5401\n",
      "Epoch 466/500, Training Loss: -0.5296\n",
      "Epoch 467/500, Training Loss: -0.5479\n",
      "Epoch 468/500, Training Loss: -0.5475\n",
      "Epoch 469/500, Training Loss: -0.5852\n",
      "Epoch 470/500, Training Loss: -0.5174\n",
      "Epoch 471/500, Training Loss: -0.5366\n",
      "Epoch 472/500, Training Loss: -0.5304\n",
      "Epoch 473/500, Training Loss: -0.5513\n",
      "Epoch 474/500, Training Loss: -0.5584\n",
      "Epoch 475/500, Training Loss: -0.5734\n",
      "Epoch 476/500, Training Loss: -0.5616\n",
      "Epoch 477/500, Training Loss: -0.5312\n",
      "Epoch 478/500, Training Loss: -0.5378\n",
      "Epoch 479/500, Training Loss: -0.5774\n",
      "Epoch 480/500, Training Loss: -0.5191\n",
      "Epoch 481/500, Training Loss: -0.5299\n",
      "Epoch 482/500, Training Loss: -0.5921\n",
      "Epoch 483/500, Training Loss: -0.5780\n",
      "Epoch 484/500, Training Loss: -0.5616\n",
      "Epoch 485/500, Training Loss: -0.5395\n",
      "Epoch 486/500, Training Loss: -0.6083\n",
      "Epoch 487/500, Training Loss: -0.5674\n",
      "Epoch 488/500, Training Loss: -0.5508\n",
      "Epoch 489/500, Training Loss: -0.5513\n",
      "Epoch 490/500, Training Loss: -0.5117\n",
      "Epoch 491/500, Training Loss: -0.5417\n",
      "Epoch 492/500, Training Loss: -0.5560\n",
      "Epoch 493/500, Training Loss: -0.5402\n",
      "Epoch 494/500, Training Loss: -0.5487\n",
      "Epoch 495/500, Training Loss: -0.5065\n",
      "Epoch 496/500, Training Loss: -0.5692\n",
      "Epoch 497/500, Training Loss: -0.5435\n",
      "Epoch 498/500, Training Loss: -0.5934\n",
      "Epoch 499/500, Training Loss: -0.5382\n",
      "Epoch 500/500, Training Loss: -0.5289\n",
      "Validation Loss: -0.5139\n",
      "Model 2, Fold 1: Validation Loss = -0.5139\n",
      "Epoch 1/500, Training Loss: -0.3338\n",
      "Epoch 2/500, Training Loss: -0.4836\n",
      "Epoch 3/500, Training Loss: -0.5732\n",
      "Epoch 4/500, Training Loss: -0.5552\n",
      "Epoch 5/500, Training Loss: -0.5612\n",
      "Epoch 6/500, Training Loss: -0.5000\n",
      "Epoch 7/500, Training Loss: -0.5951\n",
      "Epoch 8/500, Training Loss: -0.5617\n",
      "Epoch 9/500, Training Loss: -0.5425\n",
      "Epoch 10/500, Training Loss: -0.5091\n",
      "Epoch 11/500, Training Loss: -0.5934\n",
      "Epoch 12/500, Training Loss: -0.5250\n",
      "Epoch 13/500, Training Loss: -0.6140\n",
      "Epoch 14/500, Training Loss: -0.5318\n",
      "Epoch 15/500, Training Loss: -0.5696\n",
      "Epoch 16/500, Training Loss: -0.5663\n",
      "Epoch 17/500, Training Loss: -0.5214\n",
      "Epoch 18/500, Training Loss: -0.5052\n",
      "Epoch 19/500, Training Loss: -0.5565\n",
      "Epoch 20/500, Training Loss: -0.5009\n",
      "Epoch 21/500, Training Loss: -0.5701\n",
      "Epoch 22/500, Training Loss: -0.5234\n",
      "Epoch 23/500, Training Loss: -0.5060\n",
      "Epoch 24/500, Training Loss: -0.5965\n",
      "Epoch 25/500, Training Loss: -0.5253\n",
      "Epoch 26/500, Training Loss: -0.5895\n",
      "Epoch 27/500, Training Loss: -0.5845\n",
      "Epoch 28/500, Training Loss: -0.5504\n",
      "Epoch 29/500, Training Loss: -0.5992\n",
      "Epoch 30/500, Training Loss: -0.5682\n",
      "Epoch 31/500, Training Loss: -0.5618\n",
      "Epoch 32/500, Training Loss: -0.4896\n",
      "Epoch 33/500, Training Loss: -0.5945\n",
      "Epoch 34/500, Training Loss: -0.5419\n",
      "Epoch 35/500, Training Loss: -0.5797\n",
      "Epoch 36/500, Training Loss: -0.5285\n",
      "Epoch 37/500, Training Loss: -0.6083\n",
      "Epoch 38/500, Training Loss: -0.5743\n",
      "Epoch 39/500, Training Loss: -0.5624\n",
      "Epoch 40/500, Training Loss: -0.5290\n",
      "Epoch 41/500, Training Loss: -0.5253\n",
      "Epoch 42/500, Training Loss: -0.6483\n",
      "Epoch 43/500, Training Loss: -0.6135\n",
      "Epoch 44/500, Training Loss: -0.5587\n",
      "Epoch 45/500, Training Loss: -0.5945\n",
      "Epoch 46/500, Training Loss: -0.5799\n",
      "Epoch 47/500, Training Loss: -0.5485\n",
      "Epoch 48/500, Training Loss: -0.5587\n",
      "Epoch 49/500, Training Loss: -0.6066\n",
      "Epoch 50/500, Training Loss: -0.4776\n",
      "Epoch 51/500, Training Loss: -0.6173\n",
      "Epoch 52/500, Training Loss: -0.5863\n",
      "Epoch 53/500, Training Loss: -0.5553\n",
      "Epoch 54/500, Training Loss: -0.5881\n",
      "Epoch 55/500, Training Loss: -0.5742\n",
      "Epoch 56/500, Training Loss: -0.5929\n",
      "Epoch 57/500, Training Loss: -0.5960\n",
      "Epoch 58/500, Training Loss: -0.5377\n",
      "Epoch 59/500, Training Loss: -0.5601\n",
      "Epoch 60/500, Training Loss: -0.5185\n",
      "Epoch 61/500, Training Loss: -0.5315\n",
      "Epoch 62/500, Training Loss: -0.5950\n",
      "Epoch 63/500, Training Loss: -0.5931\n",
      "Epoch 64/500, Training Loss: -0.5358\n",
      "Epoch 65/500, Training Loss: -0.5605\n",
      "Epoch 66/500, Training Loss: -0.6078\n",
      "Epoch 67/500, Training Loss: -0.5338\n",
      "Epoch 68/500, Training Loss: -0.5189\n",
      "Epoch 69/500, Training Loss: -0.5629\n",
      "Epoch 70/500, Training Loss: -0.5334\n",
      "Epoch 71/500, Training Loss: -0.6204\n",
      "Epoch 72/500, Training Loss: -0.6059\n",
      "Epoch 73/500, Training Loss: -0.5880\n",
      "Epoch 74/500, Training Loss: -0.5789\n",
      "Epoch 75/500, Training Loss: -0.4810\n",
      "Epoch 76/500, Training Loss: -0.5689\n",
      "Epoch 77/500, Training Loss: -0.5927\n",
      "Epoch 78/500, Training Loss: -0.5329\n",
      "Epoch 79/500, Training Loss: -0.5206\n",
      "Epoch 80/500, Training Loss: -0.5541\n",
      "Epoch 81/500, Training Loss: -0.5147\n",
      "Epoch 82/500, Training Loss: -0.6547\n",
      "Epoch 83/500, Training Loss: -0.5104\n",
      "Epoch 84/500, Training Loss: -0.5953\n",
      "Epoch 85/500, Training Loss: -0.5664\n",
      "Epoch 86/500, Training Loss: -0.4876\n",
      "Epoch 87/500, Training Loss: -0.5589\n",
      "Epoch 88/500, Training Loss: -0.5034\n",
      "Epoch 89/500, Training Loss: -0.5489\n",
      "Epoch 90/500, Training Loss: -0.5970\n",
      "Epoch 91/500, Training Loss: -0.5984\n",
      "Epoch 92/500, Training Loss: -0.6002\n",
      "Epoch 93/500, Training Loss: -0.6088\n",
      "Epoch 94/500, Training Loss: -0.6014\n",
      "Epoch 95/500, Training Loss: -0.5988\n",
      "Epoch 96/500, Training Loss: -0.6105\n",
      "Epoch 97/500, Training Loss: -0.6125\n",
      "Epoch 98/500, Training Loss: -0.6061\n",
      "Epoch 99/500, Training Loss: -0.5120\n",
      "Epoch 100/500, Training Loss: -0.6448\n",
      "Epoch 101/500, Training Loss: -0.5565\n",
      "Epoch 102/500, Training Loss: -0.5348\n",
      "Epoch 103/500, Training Loss: -0.6094\n",
      "Epoch 104/500, Training Loss: -0.5908\n",
      "Epoch 105/500, Training Loss: -0.5592\n",
      "Epoch 106/500, Training Loss: -0.5375\n",
      "Epoch 107/500, Training Loss: -0.4875\n",
      "Epoch 108/500, Training Loss: -0.5525\n",
      "Epoch 109/500, Training Loss: -0.5940\n",
      "Epoch 110/500, Training Loss: -0.5713\n",
      "Epoch 111/500, Training Loss: -0.5780\n",
      "Epoch 112/500, Training Loss: -0.5342\n",
      "Epoch 113/500, Training Loss: -0.5231\n",
      "Epoch 114/500, Training Loss: -0.5819\n",
      "Epoch 115/500, Training Loss: -0.5542\n",
      "Epoch 116/500, Training Loss: -0.4824\n",
      "Epoch 117/500, Training Loss: -0.5125\n",
      "Epoch 118/500, Training Loss: -0.6037\n",
      "Epoch 119/500, Training Loss: -0.5644\n",
      "Epoch 120/500, Training Loss: -0.5585\n",
      "Epoch 121/500, Training Loss: -0.6015\n",
      "Epoch 122/500, Training Loss: -0.5927\n",
      "Epoch 123/500, Training Loss: -0.5686\n",
      "Epoch 124/500, Training Loss: -0.5437\n",
      "Epoch 125/500, Training Loss: -0.5533\n",
      "Epoch 126/500, Training Loss: -0.6213\n",
      "Epoch 127/500, Training Loss: -0.5925\n",
      "Epoch 128/500, Training Loss: -0.5581\n",
      "Epoch 129/500, Training Loss: -0.5085\n",
      "Epoch 130/500, Training Loss: -0.5338\n",
      "Epoch 131/500, Training Loss: -0.5209\n",
      "Epoch 132/500, Training Loss: -0.5950\n",
      "Epoch 133/500, Training Loss: -0.5758\n",
      "Epoch 134/500, Training Loss: -0.5481\n",
      "Epoch 135/500, Training Loss: -0.5505\n",
      "Epoch 136/500, Training Loss: -0.6295\n",
      "Epoch 137/500, Training Loss: -0.5807\n",
      "Epoch 138/500, Training Loss: -0.5234\n",
      "Epoch 139/500, Training Loss: -0.6057\n",
      "Epoch 140/500, Training Loss: -0.5718\n",
      "Epoch 141/500, Training Loss: -0.5582\n",
      "Epoch 142/500, Training Loss: -0.4971\n",
      "Epoch 143/500, Training Loss: -0.5370\n",
      "Epoch 144/500, Training Loss: -0.5614\n",
      "Epoch 145/500, Training Loss: -0.5473\n",
      "Epoch 146/500, Training Loss: -0.6009\n",
      "Epoch 147/500, Training Loss: -0.5139\n",
      "Epoch 148/500, Training Loss: -0.5562\n",
      "Epoch 149/500, Training Loss: -0.5395\n",
      "Epoch 150/500, Training Loss: -0.5929\n",
      "Epoch 151/500, Training Loss: -0.5486\n",
      "Epoch 152/500, Training Loss: -0.5719\n",
      "Epoch 153/500, Training Loss: -0.5252\n",
      "Epoch 154/500, Training Loss: -0.6192\n",
      "Epoch 155/500, Training Loss: -0.5678\n",
      "Epoch 156/500, Training Loss: -0.5587\n",
      "Epoch 157/500, Training Loss: -0.6043\n",
      "Epoch 158/500, Training Loss: -0.5394\n",
      "Epoch 159/500, Training Loss: -0.5533\n",
      "Epoch 160/500, Training Loss: -0.5593\n",
      "Epoch 161/500, Training Loss: -0.5932\n",
      "Epoch 162/500, Training Loss: -0.5600\n",
      "Epoch 163/500, Training Loss: -0.5127\n",
      "Epoch 164/500, Training Loss: -0.5097\n",
      "Epoch 165/500, Training Loss: -0.5842\n",
      "Epoch 166/500, Training Loss: -0.5728\n",
      "Epoch 167/500, Training Loss: -0.5093\n",
      "Epoch 168/500, Training Loss: -0.6142\n",
      "Epoch 169/500, Training Loss: -0.5429\n",
      "Epoch 170/500, Training Loss: -0.5203\n",
      "Epoch 171/500, Training Loss: -0.5601\n",
      "Epoch 172/500, Training Loss: -0.5141\n",
      "Epoch 173/500, Training Loss: -0.5150\n",
      "Epoch 174/500, Training Loss: -0.6195\n",
      "Epoch 175/500, Training Loss: -0.5327\n",
      "Epoch 176/500, Training Loss: -0.4960\n",
      "Epoch 177/500, Training Loss: -0.5969\n",
      "Epoch 178/500, Training Loss: -0.5263\n",
      "Epoch 179/500, Training Loss: -0.6182\n",
      "Epoch 180/500, Training Loss: -0.5479\n",
      "Epoch 181/500, Training Loss: -0.5114\n",
      "Epoch 182/500, Training Loss: -0.5090\n",
      "Epoch 183/500, Training Loss: -0.5471\n",
      "Epoch 184/500, Training Loss: -0.5415\n",
      "Epoch 185/500, Training Loss: -0.5393\n",
      "Epoch 186/500, Training Loss: -0.5294\n",
      "Epoch 187/500, Training Loss: -0.6045\n",
      "Epoch 188/500, Training Loss: -0.5414\n",
      "Epoch 189/500, Training Loss: -0.5512\n",
      "Epoch 190/500, Training Loss: -0.5519\n",
      "Epoch 191/500, Training Loss: -0.5714\n",
      "Epoch 192/500, Training Loss: -0.4932\n",
      "Epoch 193/500, Training Loss: -0.5717\n",
      "Epoch 194/500, Training Loss: -0.5937\n",
      "Epoch 195/500, Training Loss: -0.5217\n",
      "Epoch 196/500, Training Loss: -0.5474\n",
      "Epoch 197/500, Training Loss: -0.5906\n",
      "Epoch 198/500, Training Loss: -0.6236\n",
      "Epoch 199/500, Training Loss: -0.5729\n",
      "Epoch 200/500, Training Loss: -0.5307\n",
      "Epoch 201/500, Training Loss: -0.4983\n",
      "Epoch 202/500, Training Loss: -0.5379\n",
      "Epoch 203/500, Training Loss: -0.5719\n",
      "Epoch 204/500, Training Loss: -0.5890\n",
      "Epoch 205/500, Training Loss: -0.5356\n",
      "Epoch 206/500, Training Loss: -0.5029\n",
      "Epoch 207/500, Training Loss: -0.5785\n",
      "Epoch 208/500, Training Loss: -0.5438\n",
      "Epoch 209/500, Training Loss: -0.4930\n",
      "Epoch 210/500, Training Loss: -0.5583\n",
      "Epoch 211/500, Training Loss: -0.5810\n",
      "Epoch 212/500, Training Loss: -0.5940\n",
      "Epoch 213/500, Training Loss: -0.5807\n",
      "Epoch 214/500, Training Loss: -0.5416\n",
      "Epoch 215/500, Training Loss: -0.5454\n",
      "Epoch 216/500, Training Loss: -0.5763\n",
      "Epoch 217/500, Training Loss: -0.5738\n",
      "Epoch 218/500, Training Loss: -0.5351\n",
      "Epoch 219/500, Training Loss: -0.5998\n",
      "Epoch 220/500, Training Loss: -0.5833\n",
      "Epoch 221/500, Training Loss: -0.5330\n",
      "Epoch 222/500, Training Loss: -0.5116\n",
      "Epoch 223/500, Training Loss: -0.5866\n",
      "Epoch 224/500, Training Loss: -0.5311\n",
      "Epoch 225/500, Training Loss: -0.5467\n",
      "Epoch 226/500, Training Loss: -0.4963\n",
      "Epoch 227/500, Training Loss: -0.5188\n",
      "Epoch 228/500, Training Loss: -0.5452\n",
      "Epoch 229/500, Training Loss: -0.6063\n",
      "Epoch 230/500, Training Loss: -0.5463\n",
      "Epoch 231/500, Training Loss: -0.6006\n",
      "Epoch 232/500, Training Loss: -0.5580\n",
      "Epoch 233/500, Training Loss: -0.5064\n",
      "Epoch 234/500, Training Loss: -0.5627\n",
      "Epoch 235/500, Training Loss: -0.5776\n",
      "Epoch 236/500, Training Loss: -0.5681\n",
      "Epoch 237/500, Training Loss: -0.5210\n",
      "Epoch 238/500, Training Loss: -0.6040\n",
      "Epoch 239/500, Training Loss: -0.5678\n",
      "Epoch 240/500, Training Loss: -0.5782\n",
      "Epoch 241/500, Training Loss: -0.5418\n",
      "Epoch 242/500, Training Loss: -0.6491\n",
      "Epoch 243/500, Training Loss: -0.6002\n",
      "Epoch 244/500, Training Loss: -0.5608\n",
      "Epoch 245/500, Training Loss: -0.4989\n",
      "Epoch 246/500, Training Loss: -0.5741\n",
      "Epoch 247/500, Training Loss: -0.5267\n",
      "Epoch 248/500, Training Loss: -0.6481\n",
      "Epoch 249/500, Training Loss: -0.5470\n",
      "Epoch 250/500, Training Loss: -0.5693\n",
      "Epoch 251/500, Training Loss: -0.5667\n",
      "Epoch 252/500, Training Loss: -0.5791\n",
      "Epoch 253/500, Training Loss: -0.5710\n",
      "Epoch 254/500, Training Loss: -0.5083\n",
      "Epoch 255/500, Training Loss: -0.5710\n",
      "Epoch 256/500, Training Loss: -0.5958\n",
      "Epoch 257/500, Training Loss: -0.5313\n",
      "Epoch 258/500, Training Loss: -0.5509\n",
      "Epoch 259/500, Training Loss: -0.6061\n",
      "Epoch 260/500, Training Loss: -0.5377\n",
      "Epoch 261/500, Training Loss: -0.5530\n",
      "Epoch 262/500, Training Loss: -0.5927\n",
      "Epoch 263/500, Training Loss: -0.5508\n",
      "Epoch 264/500, Training Loss: -0.5324\n",
      "Epoch 265/500, Training Loss: -0.5632\n",
      "Epoch 266/500, Training Loss: -0.5926\n",
      "Epoch 267/500, Training Loss: -0.6278\n",
      "Epoch 268/500, Training Loss: -0.5603\n",
      "Epoch 269/500, Training Loss: -0.5730\n",
      "Epoch 270/500, Training Loss: -0.5704\n",
      "Epoch 271/500, Training Loss: -0.6272\n",
      "Epoch 272/500, Training Loss: -0.5402\n",
      "Epoch 273/500, Training Loss: -0.5209\n",
      "Epoch 274/500, Training Loss: -0.5240\n",
      "Epoch 275/500, Training Loss: -0.5406\n",
      "Epoch 276/500, Training Loss: -0.5462\n",
      "Epoch 277/500, Training Loss: -0.5975\n",
      "Epoch 278/500, Training Loss: -0.5328\n",
      "Epoch 279/500, Training Loss: -0.5623\n",
      "Epoch 280/500, Training Loss: -0.6172\n",
      "Epoch 281/500, Training Loss: -0.5635\n",
      "Epoch 282/500, Training Loss: -0.4999\n",
      "Epoch 283/500, Training Loss: -0.5173\n",
      "Epoch 284/500, Training Loss: -0.5729\n",
      "Epoch 285/500, Training Loss: -0.5596\n",
      "Epoch 286/500, Training Loss: -0.5357\n",
      "Epoch 287/500, Training Loss: -0.5571\n",
      "Epoch 288/500, Training Loss: -0.5955\n",
      "Epoch 289/500, Training Loss: -0.5302\n",
      "Epoch 290/500, Training Loss: -0.5667\n",
      "Epoch 291/500, Training Loss: -0.5541\n",
      "Epoch 292/500, Training Loss: -0.5974\n",
      "Epoch 293/500, Training Loss: -0.6008\n",
      "Epoch 294/500, Training Loss: -0.5229\n",
      "Epoch 295/500, Training Loss: -0.5569\n",
      "Epoch 296/500, Training Loss: -0.5835\n",
      "Epoch 297/500, Training Loss: -0.6215\n",
      "Epoch 298/500, Training Loss: -0.5378\n",
      "Epoch 299/500, Training Loss: -0.6163\n",
      "Epoch 300/500, Training Loss: -0.6009\n",
      "Epoch 301/500, Training Loss: -0.5668\n",
      "Epoch 302/500, Training Loss: -0.5973\n",
      "Epoch 303/500, Training Loss: -0.5888\n",
      "Epoch 304/500, Training Loss: -0.5345\n",
      "Epoch 305/500, Training Loss: -0.5588\n",
      "Epoch 306/500, Training Loss: -0.6145\n",
      "Epoch 307/500, Training Loss: -0.5822\n",
      "Epoch 308/500, Training Loss: -0.4972\n",
      "Epoch 309/500, Training Loss: -0.5717\n",
      "Epoch 310/500, Training Loss: -0.5830\n",
      "Epoch 311/500, Training Loss: -0.5701\n",
      "Epoch 312/500, Training Loss: -0.5350\n",
      "Epoch 313/500, Training Loss: -0.5370\n",
      "Epoch 314/500, Training Loss: -0.5588\n",
      "Epoch 315/500, Training Loss: -0.5704\n",
      "Epoch 316/500, Training Loss: -0.5532\n",
      "Epoch 317/500, Training Loss: -0.5778\n",
      "Epoch 318/500, Training Loss: -0.5882\n",
      "Epoch 319/500, Training Loss: -0.5459\n",
      "Epoch 320/500, Training Loss: -0.4770\n",
      "Epoch 321/500, Training Loss: -0.5659\n",
      "Epoch 322/500, Training Loss: -0.5988\n",
      "Epoch 323/500, Training Loss: -0.5878\n",
      "Epoch 324/500, Training Loss: -0.5350\n",
      "Epoch 325/500, Training Loss: -0.5708\n",
      "Epoch 326/500, Training Loss: -0.5621\n",
      "Epoch 327/500, Training Loss: -0.5470\n",
      "Epoch 328/500, Training Loss: -0.5377\n",
      "Epoch 329/500, Training Loss: -0.5932\n",
      "Epoch 330/500, Training Loss: -0.6119\n",
      "Epoch 331/500, Training Loss: -0.5808\n",
      "Epoch 332/500, Training Loss: -0.5600\n",
      "Epoch 333/500, Training Loss: -0.5758\n",
      "Epoch 334/500, Training Loss: -0.6091\n",
      "Epoch 335/500, Training Loss: -0.5571\n",
      "Epoch 336/500, Training Loss: -0.6086\n",
      "Epoch 337/500, Training Loss: -0.6232\n",
      "Epoch 338/500, Training Loss: -0.5674\n",
      "Epoch 339/500, Training Loss: -0.5440\n",
      "Epoch 340/500, Training Loss: -0.5595\n",
      "Epoch 341/500, Training Loss: -0.6085\n",
      "Epoch 342/500, Training Loss: -0.6050\n",
      "Epoch 343/500, Training Loss: -0.5657\n",
      "Epoch 344/500, Training Loss: -0.6089\n",
      "Epoch 345/500, Training Loss: -0.5768\n",
      "Epoch 346/500, Training Loss: -0.5357\n",
      "Epoch 347/500, Training Loss: -0.5305\n",
      "Epoch 348/500, Training Loss: -0.5595\n",
      "Epoch 349/500, Training Loss: -0.5701\n",
      "Epoch 350/500, Training Loss: -0.5258\n",
      "Epoch 351/500, Training Loss: -0.5736\n",
      "Epoch 352/500, Training Loss: -0.6203\n",
      "Epoch 353/500, Training Loss: -0.5977\n",
      "Epoch 354/500, Training Loss: -0.5308\n",
      "Epoch 355/500, Training Loss: -0.6000\n",
      "Epoch 356/500, Training Loss: -0.5569\n",
      "Epoch 357/500, Training Loss: -0.5579\n",
      "Epoch 358/500, Training Loss: -0.5925\n",
      "Epoch 359/500, Training Loss: -0.5372\n",
      "Epoch 360/500, Training Loss: -0.5339\n",
      "Epoch 361/500, Training Loss: -0.5725\n",
      "Epoch 362/500, Training Loss: -0.5782\n",
      "Epoch 363/500, Training Loss: -0.5961\n",
      "Epoch 364/500, Training Loss: -0.5434\n",
      "Epoch 365/500, Training Loss: -0.5860\n",
      "Epoch 366/500, Training Loss: -0.5517\n",
      "Epoch 367/500, Training Loss: -0.6067\n",
      "Epoch 368/500, Training Loss: -0.6050\n",
      "Epoch 369/500, Training Loss: -0.5884\n",
      "Epoch 370/500, Training Loss: -0.6310\n",
      "Epoch 371/500, Training Loss: -0.5784\n",
      "Epoch 372/500, Training Loss: -0.6128\n",
      "Epoch 373/500, Training Loss: -0.6006\n",
      "Epoch 374/500, Training Loss: -0.5886\n",
      "Epoch 375/500, Training Loss: -0.5642\n",
      "Epoch 376/500, Training Loss: -0.5521\n",
      "Epoch 377/500, Training Loss: -0.5493\n",
      "Epoch 378/500, Training Loss: -0.5846\n",
      "Epoch 379/500, Training Loss: -0.6189\n",
      "Epoch 380/500, Training Loss: -0.5624\n",
      "Epoch 381/500, Training Loss: -0.5909\n",
      "Epoch 382/500, Training Loss: -0.6147\n",
      "Epoch 383/500, Training Loss: -0.5354\n",
      "Epoch 384/500, Training Loss: -0.5665\n",
      "Epoch 385/500, Training Loss: -0.5815\n",
      "Epoch 386/500, Training Loss: -0.5261\n",
      "Epoch 387/500, Training Loss: -0.5886\n",
      "Epoch 388/500, Training Loss: -0.5698\n",
      "Epoch 389/500, Training Loss: -0.5665\n",
      "Epoch 390/500, Training Loss: -0.5950\n",
      "Epoch 391/500, Training Loss: -0.5650\n",
      "Epoch 392/500, Training Loss: -0.5881\n",
      "Epoch 393/500, Training Loss: -0.5527\n",
      "Epoch 394/500, Training Loss: -0.5833\n",
      "Epoch 395/500, Training Loss: -0.5996\n",
      "Epoch 396/500, Training Loss: -0.5188\n",
      "Epoch 397/500, Training Loss: -0.6040\n",
      "Epoch 398/500, Training Loss: -0.5979\n",
      "Epoch 399/500, Training Loss: -0.5752\n",
      "Epoch 400/500, Training Loss: -0.5991\n",
      "Epoch 401/500, Training Loss: -0.6031\n",
      "Epoch 402/500, Training Loss: -0.5908\n",
      "Epoch 403/500, Training Loss: -0.5727\n",
      "Epoch 404/500, Training Loss: -0.5496\n",
      "Epoch 405/500, Training Loss: -0.6054\n",
      "Epoch 406/500, Training Loss: -0.5916\n",
      "Epoch 407/500, Training Loss: -0.5204\n",
      "Epoch 408/500, Training Loss: -0.4974\n",
      "Epoch 409/500, Training Loss: -0.6227\n",
      "Epoch 410/500, Training Loss: -0.5519\n",
      "Epoch 411/500, Training Loss: -0.5561\n",
      "Epoch 412/500, Training Loss: -0.4939\n",
      "Epoch 413/500, Training Loss: -0.5243\n",
      "Epoch 414/500, Training Loss: -0.4795\n",
      "Epoch 415/500, Training Loss: -0.5567\n",
      "Epoch 416/500, Training Loss: -0.5616\n",
      "Epoch 417/500, Training Loss: -0.5424\n",
      "Epoch 418/500, Training Loss: -0.5487\n",
      "Epoch 419/500, Training Loss: -0.5111\n",
      "Epoch 420/500, Training Loss: -0.5941\n",
      "Epoch 421/500, Training Loss: -0.5979\n",
      "Epoch 422/500, Training Loss: -0.5508\n",
      "Epoch 423/500, Training Loss: -0.5672\n",
      "Epoch 424/500, Training Loss: -0.6054\n",
      "Epoch 425/500, Training Loss: -0.6033\n",
      "Epoch 426/500, Training Loss: -0.5603\n",
      "Epoch 427/500, Training Loss: -0.5710\n",
      "Epoch 428/500, Training Loss: -0.6077\n",
      "Epoch 429/500, Training Loss: -0.5630\n",
      "Epoch 430/500, Training Loss: -0.5297\n",
      "Epoch 431/500, Training Loss: -0.5320\n",
      "Epoch 432/500, Training Loss: -0.5367\n",
      "Epoch 433/500, Training Loss: -0.5648\n",
      "Epoch 434/500, Training Loss: -0.5867\n",
      "Epoch 435/500, Training Loss: -0.5219\n",
      "Epoch 436/500, Training Loss: -0.5935\n",
      "Epoch 437/500, Training Loss: -0.5461\n",
      "Epoch 438/500, Training Loss: -0.5974\n",
      "Epoch 439/500, Training Loss: -0.5492\n",
      "Epoch 440/500, Training Loss: -0.5744\n",
      "Epoch 441/500, Training Loss: -0.5579\n",
      "Epoch 442/500, Training Loss: -0.5310\n",
      "Epoch 443/500, Training Loss: -0.5519\n",
      "Epoch 444/500, Training Loss: -0.5900\n",
      "Epoch 445/500, Training Loss: -0.5402\n",
      "Epoch 446/500, Training Loss: -0.5926\n",
      "Epoch 447/500, Training Loss: -0.5609\n",
      "Epoch 448/500, Training Loss: -0.4853\n",
      "Epoch 449/500, Training Loss: -0.6032\n",
      "Epoch 450/500, Training Loss: -0.5455\n",
      "Epoch 451/500, Training Loss: -0.5828\n",
      "Epoch 452/500, Training Loss: -0.5100\n",
      "Epoch 453/500, Training Loss: -0.5465\n",
      "Epoch 454/500, Training Loss: -0.6259\n",
      "Epoch 455/500, Training Loss: -0.5845\n",
      "Epoch 456/500, Training Loss: -0.5704\n",
      "Epoch 457/500, Training Loss: -0.5707\n",
      "Epoch 458/500, Training Loss: -0.5190\n",
      "Epoch 459/500, Training Loss: -0.5098\n",
      "Epoch 460/500, Training Loss: -0.6322\n",
      "Epoch 461/500, Training Loss: -0.5673\n",
      "Epoch 462/500, Training Loss: -0.5799\n",
      "Epoch 463/500, Training Loss: -0.5167\n",
      "Epoch 464/500, Training Loss: -0.5561\n",
      "Epoch 465/500, Training Loss: -0.5983\n",
      "Epoch 466/500, Training Loss: -0.5757\n",
      "Epoch 467/500, Training Loss: -0.5636\n",
      "Epoch 468/500, Training Loss: -0.6182\n",
      "Epoch 469/500, Training Loss: -0.5845\n",
      "Epoch 470/500, Training Loss: -0.6418\n",
      "Epoch 471/500, Training Loss: -0.5973\n",
      "Epoch 472/500, Training Loss: -0.6488\n",
      "Epoch 473/500, Training Loss: -0.6320\n",
      "Epoch 474/500, Training Loss: -0.5963\n",
      "Epoch 475/500, Training Loss: -0.5275\n",
      "Epoch 476/500, Training Loss: -0.5951\n",
      "Epoch 477/500, Training Loss: -0.5660\n",
      "Epoch 478/500, Training Loss: -0.5079\n",
      "Epoch 479/500, Training Loss: -0.6067\n",
      "Epoch 480/500, Training Loss: -0.5564\n",
      "Epoch 481/500, Training Loss: -0.5535\n",
      "Epoch 482/500, Training Loss: -0.5605\n",
      "Epoch 483/500, Training Loss: -0.5559\n",
      "Epoch 484/500, Training Loss: -0.5408\n",
      "Epoch 485/500, Training Loss: -0.5887\n",
      "Epoch 486/500, Training Loss: -0.5489\n",
      "Epoch 487/500, Training Loss: -0.5679\n",
      "Epoch 488/500, Training Loss: -0.6131\n",
      "Epoch 489/500, Training Loss: -0.6190\n",
      "Epoch 490/500, Training Loss: -0.5894\n",
      "Epoch 491/500, Training Loss: -0.5577\n",
      "Epoch 492/500, Training Loss: -0.5357\n",
      "Epoch 493/500, Training Loss: -0.6566\n",
      "Epoch 494/500, Training Loss: -0.5570\n",
      "Epoch 495/500, Training Loss: -0.5665\n",
      "Epoch 496/500, Training Loss: -0.6052\n",
      "Epoch 497/500, Training Loss: -0.5786\n",
      "Epoch 498/500, Training Loss: -0.5699\n",
      "Epoch 499/500, Training Loss: -0.5270\n",
      "Epoch 500/500, Training Loss: -0.5644\n",
      "Validation Loss: -0.5203\n",
      "Model 3, Fold 0: Validation Loss = -0.5203\n",
      "Epoch 1/500, Training Loss: -0.3553\n",
      "Epoch 2/500, Training Loss: -0.4899\n",
      "Epoch 3/500, Training Loss: -0.4782\n",
      "Epoch 4/500, Training Loss: -0.4981\n",
      "Epoch 5/500, Training Loss: -0.4638\n",
      "Epoch 6/500, Training Loss: -0.4888\n",
      "Epoch 7/500, Training Loss: -0.5078\n",
      "Epoch 8/500, Training Loss: -0.5135\n",
      "Epoch 9/500, Training Loss: -0.5084\n",
      "Epoch 10/500, Training Loss: -0.5023\n",
      "Epoch 11/500, Training Loss: -0.5620\n",
      "Epoch 12/500, Training Loss: -0.5682\n",
      "Epoch 13/500, Training Loss: -0.5082\n",
      "Epoch 14/500, Training Loss: -0.5942\n",
      "Epoch 15/500, Training Loss: -0.5283\n",
      "Epoch 16/500, Training Loss: -0.5380\n",
      "Epoch 17/500, Training Loss: -0.5251\n",
      "Epoch 18/500, Training Loss: -0.5911\n",
      "Epoch 19/500, Training Loss: -0.4793\n",
      "Epoch 20/500, Training Loss: -0.5521\n",
      "Epoch 21/500, Training Loss: -0.5856\n",
      "Epoch 22/500, Training Loss: -0.5734\n",
      "Epoch 23/500, Training Loss: -0.6121\n",
      "Epoch 24/500, Training Loss: -0.5953\n",
      "Epoch 25/500, Training Loss: -0.5333\n",
      "Epoch 26/500, Training Loss: -0.6120\n",
      "Epoch 27/500, Training Loss: -0.5974\n",
      "Epoch 28/500, Training Loss: -0.5846\n",
      "Epoch 29/500, Training Loss: -0.6053\n",
      "Epoch 30/500, Training Loss: -0.5763\n",
      "Epoch 31/500, Training Loss: -0.5080\n",
      "Epoch 32/500, Training Loss: -0.4817\n",
      "Epoch 33/500, Training Loss: -0.5675\n",
      "Epoch 34/500, Training Loss: -0.5093\n",
      "Epoch 35/500, Training Loss: -0.5368\n",
      "Epoch 36/500, Training Loss: -0.5484\n",
      "Epoch 37/500, Training Loss: -0.4857\n",
      "Epoch 38/500, Training Loss: -0.5589\n",
      "Epoch 39/500, Training Loss: -0.5204\n",
      "Epoch 40/500, Training Loss: -0.5873\n",
      "Epoch 41/500, Training Loss: -0.5321\n",
      "Epoch 42/500, Training Loss: -0.4542\n",
      "Epoch 43/500, Training Loss: -0.5216\n",
      "Epoch 44/500, Training Loss: -0.5747\n",
      "Epoch 45/500, Training Loss: -0.5279\n",
      "Epoch 46/500, Training Loss: -0.4617\n",
      "Epoch 47/500, Training Loss: -0.5033\n",
      "Epoch 48/500, Training Loss: -0.5412\n",
      "Epoch 49/500, Training Loss: -0.5522\n",
      "Epoch 50/500, Training Loss: -0.5487\n",
      "Epoch 51/500, Training Loss: -0.5682\n",
      "Epoch 52/500, Training Loss: -0.4981\n",
      "Epoch 53/500, Training Loss: -0.5373\n",
      "Epoch 54/500, Training Loss: -0.4949\n",
      "Epoch 55/500, Training Loss: -0.5796\n",
      "Epoch 56/500, Training Loss: -0.5621\n",
      "Epoch 57/500, Training Loss: -0.5295\n",
      "Epoch 58/500, Training Loss: -0.5311\n",
      "Epoch 59/500, Training Loss: -0.5126\n",
      "Epoch 60/500, Training Loss: -0.6229\n",
      "Epoch 61/500, Training Loss: -0.5901\n",
      "Epoch 62/500, Training Loss: -0.5223\n",
      "Epoch 63/500, Training Loss: -0.5259\n",
      "Epoch 64/500, Training Loss: -0.4847\n",
      "Epoch 65/500, Training Loss: -0.5464\n",
      "Epoch 66/500, Training Loss: -0.6227\n",
      "Epoch 67/500, Training Loss: -0.5559\n",
      "Epoch 68/500, Training Loss: -0.5337\n",
      "Epoch 69/500, Training Loss: -0.5542\n",
      "Epoch 70/500, Training Loss: -0.5491\n",
      "Epoch 71/500, Training Loss: -0.5843\n",
      "Epoch 72/500, Training Loss: -0.4963\n",
      "Epoch 73/500, Training Loss: -0.5636\n",
      "Epoch 74/500, Training Loss: -0.5719\n",
      "Epoch 75/500, Training Loss: -0.4958\n",
      "Epoch 76/500, Training Loss: -0.5776\n",
      "Epoch 77/500, Training Loss: -0.5903\n",
      "Epoch 78/500, Training Loss: -0.5763\n",
      "Epoch 79/500, Training Loss: -0.4854\n",
      "Epoch 80/500, Training Loss: -0.5783\n",
      "Epoch 81/500, Training Loss: -0.5041\n",
      "Epoch 82/500, Training Loss: -0.5281\n",
      "Epoch 83/500, Training Loss: -0.4829\n",
      "Epoch 84/500, Training Loss: -0.5746\n",
      "Epoch 85/500, Training Loss: -0.5907\n",
      "Epoch 86/500, Training Loss: -0.5592\n",
      "Epoch 87/500, Training Loss: -0.5437\n",
      "Epoch 88/500, Training Loss: -0.5477\n",
      "Epoch 89/500, Training Loss: -0.5394\n",
      "Epoch 90/500, Training Loss: -0.5271\n",
      "Epoch 91/500, Training Loss: -0.5881\n",
      "Epoch 92/500, Training Loss: -0.6024\n",
      "Epoch 93/500, Training Loss: -0.4901\n",
      "Epoch 94/500, Training Loss: -0.5447\n",
      "Epoch 95/500, Training Loss: -0.5639\n",
      "Epoch 96/500, Training Loss: -0.5469\n",
      "Epoch 97/500, Training Loss: -0.4979\n",
      "Epoch 98/500, Training Loss: -0.5622\n",
      "Epoch 99/500, Training Loss: -0.5195\n",
      "Epoch 100/500, Training Loss: -0.5480\n",
      "Epoch 101/500, Training Loss: -0.5197\n",
      "Epoch 102/500, Training Loss: -0.5275\n",
      "Epoch 103/500, Training Loss: -0.5407\n",
      "Epoch 104/500, Training Loss: -0.4911\n",
      "Epoch 105/500, Training Loss: -0.5578\n",
      "Epoch 106/500, Training Loss: -0.5173\n",
      "Epoch 107/500, Training Loss: -0.5209\n",
      "Epoch 108/500, Training Loss: -0.5398\n",
      "Epoch 109/500, Training Loss: -0.5576\n",
      "Epoch 110/500, Training Loss: -0.5708\n",
      "Epoch 111/500, Training Loss: -0.5530\n",
      "Epoch 112/500, Training Loss: -0.5252\n",
      "Epoch 113/500, Training Loss: -0.5730\n",
      "Epoch 114/500, Training Loss: -0.4932\n",
      "Epoch 115/500, Training Loss: -0.5451\n",
      "Epoch 116/500, Training Loss: -0.5656\n",
      "Epoch 117/500, Training Loss: -0.5503\n",
      "Epoch 118/500, Training Loss: -0.5082\n",
      "Epoch 119/500, Training Loss: -0.5687\n",
      "Epoch 120/500, Training Loss: -0.5344\n",
      "Epoch 121/500, Training Loss: -0.5494\n",
      "Epoch 122/500, Training Loss: -0.5344\n",
      "Epoch 123/500, Training Loss: -0.5359\n",
      "Epoch 124/500, Training Loss: -0.5648\n",
      "Epoch 125/500, Training Loss: -0.5406\n",
      "Epoch 126/500, Training Loss: -0.5953\n",
      "Epoch 127/500, Training Loss: -0.5484\n",
      "Epoch 128/500, Training Loss: -0.5418\n",
      "Epoch 129/500, Training Loss: -0.6051\n",
      "Epoch 130/500, Training Loss: -0.4982\n",
      "Epoch 131/500, Training Loss: -0.5137\n",
      "Epoch 132/500, Training Loss: -0.5334\n",
      "Epoch 133/500, Training Loss: -0.5315\n",
      "Epoch 134/500, Training Loss: -0.5068\n",
      "Epoch 135/500, Training Loss: -0.5218\n",
      "Epoch 136/500, Training Loss: -0.5309\n",
      "Epoch 137/500, Training Loss: -0.5265\n",
      "Epoch 138/500, Training Loss: -0.5143\n",
      "Epoch 139/500, Training Loss: -0.6069\n",
      "Epoch 140/500, Training Loss: -0.5603\n",
      "Epoch 141/500, Training Loss: -0.5344\n",
      "Epoch 142/500, Training Loss: -0.5142\n",
      "Epoch 143/500, Training Loss: -0.6026\n",
      "Epoch 144/500, Training Loss: -0.6109\n",
      "Epoch 145/500, Training Loss: -0.5427\n",
      "Epoch 146/500, Training Loss: -0.5901\n",
      "Epoch 147/500, Training Loss: -0.6132\n",
      "Epoch 148/500, Training Loss: -0.5888\n",
      "Epoch 149/500, Training Loss: -0.5089\n",
      "Epoch 150/500, Training Loss: -0.5632\n",
      "Epoch 151/500, Training Loss: -0.5150\n",
      "Epoch 152/500, Training Loss: -0.5808\n",
      "Epoch 153/500, Training Loss: -0.5382\n",
      "Epoch 154/500, Training Loss: -0.5506\n",
      "Epoch 155/500, Training Loss: -0.4915\n",
      "Epoch 156/500, Training Loss: -0.4976\n",
      "Epoch 157/500, Training Loss: -0.4970\n",
      "Epoch 158/500, Training Loss: -0.5883\n",
      "Epoch 159/500, Training Loss: -0.5489\n",
      "Epoch 160/500, Training Loss: -0.4876\n",
      "Epoch 161/500, Training Loss: -0.6556\n",
      "Epoch 162/500, Training Loss: -0.4994\n",
      "Epoch 163/500, Training Loss: -0.5277\n",
      "Epoch 164/500, Training Loss: -0.5422\n",
      "Epoch 165/500, Training Loss: -0.5556\n",
      "Epoch 166/500, Training Loss: -0.6238\n",
      "Epoch 167/500, Training Loss: -0.5889\n",
      "Epoch 168/500, Training Loss: -0.5365\n",
      "Epoch 169/500, Training Loss: -0.5152\n",
      "Epoch 170/500, Training Loss: -0.5056\n",
      "Epoch 171/500, Training Loss: -0.5324\n",
      "Epoch 172/500, Training Loss: -0.4882\n",
      "Epoch 173/500, Training Loss: -0.5406\n",
      "Epoch 174/500, Training Loss: -0.5345\n",
      "Epoch 175/500, Training Loss: -0.5614\n",
      "Epoch 176/500, Training Loss: -0.5265\n",
      "Epoch 177/500, Training Loss: -0.6191\n",
      "Epoch 178/500, Training Loss: -0.5649\n",
      "Epoch 179/500, Training Loss: -0.5314\n",
      "Epoch 180/500, Training Loss: -0.6209\n",
      "Epoch 181/500, Training Loss: -0.5119\n",
      "Epoch 182/500, Training Loss: -0.5129\n",
      "Epoch 183/500, Training Loss: -0.5344\n",
      "Epoch 184/500, Training Loss: -0.5634\n",
      "Epoch 185/500, Training Loss: -0.5826\n",
      "Epoch 186/500, Training Loss: -0.5212\n",
      "Epoch 187/500, Training Loss: -0.5503\n",
      "Epoch 188/500, Training Loss: -0.5667\n",
      "Epoch 189/500, Training Loss: -0.5896\n",
      "Epoch 190/500, Training Loss: -0.5291\n",
      "Epoch 191/500, Training Loss: -0.5988\n",
      "Epoch 192/500, Training Loss: -0.5627\n",
      "Epoch 193/500, Training Loss: -0.5133\n",
      "Epoch 194/500, Training Loss: -0.5831\n",
      "Epoch 195/500, Training Loss: -0.5130\n",
      "Epoch 196/500, Training Loss: -0.5266\n",
      "Epoch 197/500, Training Loss: -0.5063\n",
      "Epoch 198/500, Training Loss: -0.5605\n",
      "Epoch 199/500, Training Loss: -0.5127\n",
      "Epoch 200/500, Training Loss: -0.5535\n",
      "Epoch 201/500, Training Loss: -0.5093\n",
      "Epoch 202/500, Training Loss: -0.6069\n",
      "Epoch 203/500, Training Loss: -0.5189\n",
      "Epoch 204/500, Training Loss: -0.5778\n",
      "Epoch 205/500, Training Loss: -0.5140\n",
      "Epoch 206/500, Training Loss: -0.5399\n",
      "Epoch 207/500, Training Loss: -0.5776\n",
      "Epoch 208/500, Training Loss: -0.5310\n",
      "Epoch 209/500, Training Loss: -0.5332\n",
      "Epoch 210/500, Training Loss: -0.5325\n",
      "Epoch 211/500, Training Loss: -0.5829\n",
      "Epoch 212/500, Training Loss: -0.5575\n",
      "Epoch 213/500, Training Loss: -0.5616\n",
      "Epoch 214/500, Training Loss: -0.4462\n",
      "Epoch 215/500, Training Loss: -0.5372\n",
      "Epoch 216/500, Training Loss: -0.5661\n",
      "Epoch 217/500, Training Loss: -0.6031\n",
      "Epoch 218/500, Training Loss: -0.5824\n",
      "Epoch 219/500, Training Loss: -0.5203\n",
      "Epoch 220/500, Training Loss: -0.5358\n",
      "Epoch 221/500, Training Loss: -0.5528\n",
      "Epoch 222/500, Training Loss: -0.5876\n",
      "Epoch 223/500, Training Loss: -0.5649\n",
      "Epoch 224/500, Training Loss: -0.5663\n",
      "Epoch 225/500, Training Loss: -0.5558\n",
      "Epoch 226/500, Training Loss: -0.5735\n",
      "Epoch 227/500, Training Loss: -0.5736\n",
      "Epoch 228/500, Training Loss: -0.5683\n",
      "Epoch 229/500, Training Loss: -0.5395\n",
      "Epoch 230/500, Training Loss: -0.5899\n",
      "Epoch 231/500, Training Loss: -0.5834\n",
      "Epoch 232/500, Training Loss: -0.5936\n",
      "Epoch 233/500, Training Loss: -0.5251\n",
      "Epoch 234/500, Training Loss: -0.5549\n",
      "Epoch 235/500, Training Loss: -0.5862\n",
      "Epoch 236/500, Training Loss: -0.5239\n",
      "Epoch 237/500, Training Loss: -0.5643\n",
      "Epoch 238/500, Training Loss: -0.5415\n",
      "Epoch 239/500, Training Loss: -0.4718\n",
      "Epoch 240/500, Training Loss: -0.4528\n",
      "Epoch 241/500, Training Loss: -0.5453\n",
      "Epoch 242/500, Training Loss: -0.5110\n",
      "Epoch 243/500, Training Loss: -0.5820\n",
      "Epoch 244/500, Training Loss: -0.5995\n",
      "Epoch 245/500, Training Loss: -0.5421\n",
      "Epoch 246/500, Training Loss: -0.4913\n",
      "Epoch 247/500, Training Loss: -0.5378\n",
      "Epoch 248/500, Training Loss: -0.5500\n",
      "Epoch 249/500, Training Loss: -0.5551\n",
      "Epoch 250/500, Training Loss: -0.5426\n",
      "Epoch 251/500, Training Loss: -0.5470\n",
      "Epoch 252/500, Training Loss: -0.5659\n",
      "Epoch 253/500, Training Loss: -0.5669\n",
      "Epoch 254/500, Training Loss: -0.5498\n",
      "Epoch 255/500, Training Loss: -0.5190\n",
      "Epoch 256/500, Training Loss: -0.5491\n",
      "Epoch 257/500, Training Loss: -0.5527\n",
      "Epoch 258/500, Training Loss: -0.5065\n",
      "Epoch 259/500, Training Loss: -0.5515\n",
      "Epoch 260/500, Training Loss: -0.5009\n",
      "Epoch 261/500, Training Loss: -0.5671\n",
      "Epoch 262/500, Training Loss: -0.5553\n",
      "Epoch 263/500, Training Loss: -0.5399\n",
      "Epoch 264/500, Training Loss: -0.4956\n",
      "Epoch 265/500, Training Loss: -0.5864\n",
      "Epoch 266/500, Training Loss: -0.4804\n",
      "Epoch 267/500, Training Loss: -0.5338\n",
      "Epoch 268/500, Training Loss: -0.6419\n",
      "Epoch 269/500, Training Loss: -0.5916\n",
      "Epoch 270/500, Training Loss: -0.5217\n",
      "Epoch 271/500, Training Loss: -0.5938\n",
      "Epoch 272/500, Training Loss: -0.5455\n",
      "Epoch 273/500, Training Loss: -0.6005\n",
      "Epoch 274/500, Training Loss: -0.5538\n",
      "Epoch 275/500, Training Loss: -0.5516\n",
      "Epoch 276/500, Training Loss: -0.5181\n",
      "Epoch 277/500, Training Loss: -0.6084\n",
      "Epoch 278/500, Training Loss: -0.5386\n",
      "Epoch 279/500, Training Loss: -0.5868\n",
      "Epoch 280/500, Training Loss: -0.4984\n",
      "Epoch 281/500, Training Loss: -0.5388\n",
      "Epoch 282/500, Training Loss: -0.5072\n",
      "Epoch 283/500, Training Loss: -0.4713\n",
      "Epoch 284/500, Training Loss: -0.5104\n",
      "Epoch 285/500, Training Loss: -0.5193\n",
      "Epoch 286/500, Training Loss: -0.6111\n",
      "Epoch 287/500, Training Loss: -0.5323\n",
      "Epoch 288/500, Training Loss: -0.6187\n",
      "Epoch 289/500, Training Loss: -0.5573\n",
      "Epoch 290/500, Training Loss: -0.5747\n",
      "Epoch 291/500, Training Loss: -0.5529\n",
      "Epoch 292/500, Training Loss: -0.4366\n",
      "Epoch 293/500, Training Loss: -0.5109\n",
      "Epoch 294/500, Training Loss: -0.5484\n",
      "Epoch 295/500, Training Loss: -0.4971\n",
      "Epoch 296/500, Training Loss: -0.4617\n",
      "Epoch 297/500, Training Loss: -0.5092\n",
      "Epoch 298/500, Training Loss: -0.5432\n",
      "Epoch 299/500, Training Loss: -0.6174\n",
      "Epoch 300/500, Training Loss: -0.4903\n",
      "Epoch 301/500, Training Loss: -0.5756\n",
      "Epoch 302/500, Training Loss: -0.5363\n",
      "Epoch 303/500, Training Loss: -0.6056\n",
      "Epoch 304/500, Training Loss: -0.5520\n",
      "Epoch 305/500, Training Loss: -0.6022\n",
      "Epoch 306/500, Training Loss: -0.5388\n",
      "Epoch 307/500, Training Loss: -0.5420\n",
      "Epoch 308/500, Training Loss: -0.5680\n",
      "Epoch 309/500, Training Loss: -0.5545\n",
      "Epoch 310/500, Training Loss: -0.5611\n",
      "Epoch 311/500, Training Loss: -0.6081\n",
      "Epoch 312/500, Training Loss: -0.5769\n",
      "Epoch 313/500, Training Loss: -0.5159\n",
      "Epoch 314/500, Training Loss: -0.5125\n",
      "Epoch 315/500, Training Loss: -0.5212\n",
      "Epoch 316/500, Training Loss: -0.5252\n",
      "Epoch 317/500, Training Loss: -0.5278\n",
      "Epoch 318/500, Training Loss: -0.5024\n",
      "Epoch 319/500, Training Loss: -0.5102\n",
      "Epoch 320/500, Training Loss: -0.5540\n",
      "Epoch 321/500, Training Loss: -0.5994\n",
      "Epoch 322/500, Training Loss: -0.5184\n",
      "Epoch 323/500, Training Loss: -0.5764\n",
      "Epoch 324/500, Training Loss: -0.5071\n",
      "Epoch 325/500, Training Loss: -0.5463\n",
      "Epoch 326/500, Training Loss: -0.6094\n",
      "Epoch 327/500, Training Loss: -0.5713\n",
      "Epoch 328/500, Training Loss: -0.4971\n",
      "Epoch 329/500, Training Loss: -0.5784\n",
      "Epoch 330/500, Training Loss: -0.5334\n",
      "Epoch 331/500, Training Loss: -0.5605\n",
      "Epoch 332/500, Training Loss: -0.5441\n",
      "Epoch 333/500, Training Loss: -0.5711\n",
      "Epoch 334/500, Training Loss: -0.4898\n",
      "Epoch 335/500, Training Loss: -0.5500\n",
      "Epoch 336/500, Training Loss: -0.5774\n",
      "Epoch 337/500, Training Loss: -0.5571\n",
      "Epoch 338/500, Training Loss: -0.5767\n",
      "Epoch 339/500, Training Loss: -0.4946\n",
      "Epoch 340/500, Training Loss: -0.5365\n",
      "Epoch 341/500, Training Loss: -0.6169\n",
      "Epoch 342/500, Training Loss: -0.5907\n",
      "Epoch 343/500, Training Loss: -0.5409\n",
      "Epoch 344/500, Training Loss: -0.5288\n",
      "Epoch 345/500, Training Loss: -0.5786\n",
      "Epoch 346/500, Training Loss: -0.5670\n",
      "Epoch 347/500, Training Loss: -0.5816\n",
      "Epoch 348/500, Training Loss: -0.5615\n",
      "Epoch 349/500, Training Loss: -0.5832\n",
      "Epoch 350/500, Training Loss: -0.5626\n",
      "Epoch 351/500, Training Loss: -0.5876\n",
      "Epoch 352/500, Training Loss: -0.5237\n",
      "Epoch 353/500, Training Loss: -0.5331\n",
      "Epoch 354/500, Training Loss: -0.5007\n",
      "Epoch 355/500, Training Loss: -0.4676\n",
      "Epoch 356/500, Training Loss: -0.5373\n",
      "Epoch 357/500, Training Loss: -0.5411\n",
      "Epoch 358/500, Training Loss: -0.5918\n",
      "Epoch 359/500, Training Loss: -0.5220\n",
      "Epoch 360/500, Training Loss: -0.5372\n",
      "Epoch 361/500, Training Loss: -0.5783\n",
      "Epoch 362/500, Training Loss: -0.5730\n",
      "Epoch 363/500, Training Loss: -0.5326\n",
      "Epoch 364/500, Training Loss: -0.5571\n",
      "Epoch 365/500, Training Loss: -0.5736\n",
      "Epoch 366/500, Training Loss: -0.5134\n",
      "Epoch 367/500, Training Loss: -0.5038\n",
      "Epoch 368/500, Training Loss: -0.5591\n",
      "Epoch 369/500, Training Loss: -0.4981\n",
      "Epoch 370/500, Training Loss: -0.5298\n",
      "Epoch 371/500, Training Loss: -0.6297\n",
      "Epoch 372/500, Training Loss: -0.5223\n",
      "Epoch 373/500, Training Loss: -0.5000\n",
      "Epoch 374/500, Training Loss: -0.5209\n",
      "Epoch 375/500, Training Loss: -0.5283\n",
      "Epoch 376/500, Training Loss: -0.4931\n",
      "Epoch 377/500, Training Loss: -0.5934\n",
      "Epoch 378/500, Training Loss: -0.5078\n",
      "Epoch 379/500, Training Loss: -0.4917\n",
      "Epoch 380/500, Training Loss: -0.5511\n",
      "Epoch 381/500, Training Loss: -0.5105\n",
      "Epoch 382/500, Training Loss: -0.5873\n",
      "Epoch 383/500, Training Loss: -0.5825\n",
      "Epoch 384/500, Training Loss: -0.6237\n",
      "Epoch 385/500, Training Loss: -0.5268\n",
      "Epoch 386/500, Training Loss: -0.5309\n",
      "Epoch 387/500, Training Loss: -0.5397\n",
      "Epoch 388/500, Training Loss: -0.6075\n",
      "Epoch 389/500, Training Loss: -0.5079\n",
      "Epoch 390/500, Training Loss: -0.5395\n",
      "Epoch 391/500, Training Loss: -0.5242\n",
      "Epoch 392/500, Training Loss: -0.5899\n",
      "Epoch 393/500, Training Loss: -0.5730\n",
      "Epoch 394/500, Training Loss: -0.5209\n",
      "Epoch 395/500, Training Loss: -0.5084\n",
      "Epoch 396/500, Training Loss: -0.5188\n",
      "Epoch 397/500, Training Loss: -0.5894\n",
      "Epoch 398/500, Training Loss: -0.5129\n",
      "Epoch 399/500, Training Loss: -0.5771\n",
      "Epoch 400/500, Training Loss: -0.5234\n",
      "Epoch 401/500, Training Loss: -0.5199\n",
      "Epoch 402/500, Training Loss: -0.5393\n",
      "Epoch 403/500, Training Loss: -0.4618\n",
      "Epoch 404/500, Training Loss: -0.5885\n",
      "Epoch 405/500, Training Loss: -0.5989\n",
      "Epoch 406/500, Training Loss: -0.5521\n",
      "Epoch 407/500, Training Loss: -0.5442\n",
      "Epoch 408/500, Training Loss: -0.5218\n",
      "Epoch 409/500, Training Loss: -0.5707\n",
      "Epoch 410/500, Training Loss: -0.5567\n",
      "Epoch 411/500, Training Loss: -0.5316\n",
      "Epoch 412/500, Training Loss: -0.5752\n",
      "Epoch 413/500, Training Loss: -0.5308\n",
      "Epoch 414/500, Training Loss: -0.5495\n",
      "Epoch 415/500, Training Loss: -0.5496\n",
      "Epoch 416/500, Training Loss: -0.5401\n",
      "Epoch 417/500, Training Loss: -0.5557\n",
      "Epoch 418/500, Training Loss: -0.5243\n",
      "Epoch 419/500, Training Loss: -0.5973\n",
      "Epoch 420/500, Training Loss: -0.5319\n",
      "Epoch 421/500, Training Loss: -0.5605\n",
      "Epoch 422/500, Training Loss: -0.5924\n",
      "Epoch 423/500, Training Loss: -0.5359\n",
      "Epoch 424/500, Training Loss: -0.5540\n",
      "Epoch 425/500, Training Loss: -0.4678\n",
      "Epoch 426/500, Training Loss: -0.5389\n",
      "Epoch 427/500, Training Loss: -0.5282\n",
      "Epoch 428/500, Training Loss: -0.5543\n",
      "Epoch 429/500, Training Loss: -0.5396\n",
      "Epoch 430/500, Training Loss: -0.5344\n",
      "Epoch 431/500, Training Loss: -0.5000\n",
      "Epoch 432/500, Training Loss: -0.5949\n",
      "Epoch 433/500, Training Loss: -0.5022\n",
      "Epoch 434/500, Training Loss: -0.4827\n",
      "Epoch 435/500, Training Loss: -0.5474\n",
      "Epoch 436/500, Training Loss: -0.4779\n",
      "Epoch 437/500, Training Loss: -0.5779\n",
      "Epoch 438/500, Training Loss: -0.5430\n",
      "Epoch 439/500, Training Loss: -0.5504\n",
      "Epoch 440/500, Training Loss: -0.5848\n",
      "Epoch 441/500, Training Loss: -0.5236\n",
      "Epoch 442/500, Training Loss: -0.5166\n",
      "Epoch 443/500, Training Loss: -0.5710\n",
      "Epoch 444/500, Training Loss: -0.5573\n",
      "Epoch 445/500, Training Loss: -0.5128\n",
      "Epoch 446/500, Training Loss: -0.5471\n",
      "Epoch 447/500, Training Loss: -0.5093\n",
      "Epoch 448/500, Training Loss: -0.5918\n",
      "Epoch 449/500, Training Loss: -0.5255\n",
      "Epoch 450/500, Training Loss: -0.5832\n",
      "Epoch 451/500, Training Loss: -0.5733\n",
      "Epoch 452/500, Training Loss: -0.5266\n",
      "Epoch 453/500, Training Loss: -0.6059\n",
      "Epoch 454/500, Training Loss: -0.5979\n",
      "Epoch 455/500, Training Loss: -0.5856\n",
      "Epoch 456/500, Training Loss: -0.5100\n",
      "Epoch 457/500, Training Loss: -0.4489\n",
      "Epoch 458/500, Training Loss: -0.5597\n",
      "Epoch 459/500, Training Loss: -0.5730\n",
      "Epoch 460/500, Training Loss: -0.6229\n",
      "Epoch 461/500, Training Loss: -0.5142\n",
      "Epoch 462/500, Training Loss: -0.5700\n",
      "Epoch 463/500, Training Loss: -0.4771\n",
      "Epoch 464/500, Training Loss: -0.5440\n",
      "Epoch 465/500, Training Loss: -0.5681\n",
      "Epoch 466/500, Training Loss: -0.5274\n",
      "Epoch 467/500, Training Loss: -0.5871\n",
      "Epoch 468/500, Training Loss: -0.5669\n",
      "Epoch 469/500, Training Loss: -0.5111\n",
      "Epoch 470/500, Training Loss: -0.5467\n",
      "Epoch 471/500, Training Loss: -0.4979\n",
      "Epoch 472/500, Training Loss: -0.5938\n",
      "Epoch 473/500, Training Loss: -0.5883\n",
      "Epoch 474/500, Training Loss: -0.5500\n",
      "Epoch 475/500, Training Loss: -0.5537\n",
      "Epoch 476/500, Training Loss: -0.5432\n",
      "Epoch 477/500, Training Loss: -0.5752\n",
      "Epoch 478/500, Training Loss: -0.5456\n",
      "Epoch 479/500, Training Loss: -0.4726\n",
      "Epoch 480/500, Training Loss: -0.5905\n",
      "Epoch 481/500, Training Loss: -0.5833\n",
      "Epoch 482/500, Training Loss: -0.6053\n",
      "Epoch 483/500, Training Loss: -0.5413\n",
      "Epoch 484/500, Training Loss: -0.5607\n",
      "Epoch 485/500, Training Loss: -0.5162\n",
      "Epoch 486/500, Training Loss: -0.5485\n",
      "Epoch 487/500, Training Loss: -0.6354\n",
      "Epoch 488/500, Training Loss: -0.5840\n",
      "Epoch 489/500, Training Loss: -0.4868\n",
      "Epoch 490/500, Training Loss: -0.5224\n",
      "Epoch 491/500, Training Loss: -0.5307\n",
      "Epoch 492/500, Training Loss: -0.5486\n",
      "Epoch 493/500, Training Loss: -0.6299\n",
      "Epoch 494/500, Training Loss: -0.5471\n",
      "Epoch 495/500, Training Loss: -0.5735\n",
      "Epoch 496/500, Training Loss: -0.4757\n",
      "Epoch 497/500, Training Loss: -0.5580\n",
      "Epoch 498/500, Training Loss: -0.5601\n",
      "Epoch 499/500, Training Loss: -0.5348\n",
      "Epoch 500/500, Training Loss: -0.4822\n",
      "Validation Loss: -0.5523\n",
      "Model 3, Fold 1: Validation Loss = -0.5523\n",
      "Best overall model index: 0 with average validation loss -0.5753\n",
      "Retraining best model on full dataset...\n",
      "Epoch 1/500, Training Loss: -0.1854\n",
      "Epoch 2/500, Training Loss: -0.2467\n",
      "Epoch 3/500, Training Loss: -0.2868\n",
      "Epoch 4/500, Training Loss: -0.3148\n",
      "Epoch 5/500, Training Loss: -0.3775\n",
      "Epoch 6/500, Training Loss: -0.3873\n",
      "Epoch 7/500, Training Loss: -0.4569\n",
      "Epoch 8/500, Training Loss: -0.4606\n",
      "Epoch 9/500, Training Loss: -0.4946\n",
      "Epoch 10/500, Training Loss: -0.5373\n",
      "Epoch 11/500, Training Loss: -0.5349\n",
      "Epoch 12/500, Training Loss: -0.5197\n",
      "Epoch 13/500, Training Loss: -0.5471\n",
      "Epoch 14/500, Training Loss: -0.5694\n",
      "Epoch 15/500, Training Loss: -0.5322\n",
      "Epoch 16/500, Training Loss: -0.5603\n",
      "Epoch 17/500, Training Loss: -0.5237\n",
      "Epoch 18/500, Training Loss: -0.5597\n",
      "Epoch 19/500, Training Loss: -0.5500\n",
      "Epoch 20/500, Training Loss: -0.5485\n",
      "Epoch 21/500, Training Loss: -0.5386\n",
      "Epoch 22/500, Training Loss: -0.5301\n",
      "Epoch 23/500, Training Loss: -0.5534\n",
      "Epoch 24/500, Training Loss: -0.5353\n",
      "Epoch 25/500, Training Loss: -0.5598\n",
      "Epoch 26/500, Training Loss: -0.5184\n",
      "Epoch 27/500, Training Loss: -0.5542\n",
      "Epoch 28/500, Training Loss: -0.5304\n",
      "Epoch 29/500, Training Loss: -0.5589\n",
      "Epoch 30/500, Training Loss: -0.5442\n",
      "Epoch 31/500, Training Loss: -0.5495\n",
      "Epoch 32/500, Training Loss: -0.5591\n",
      "Epoch 33/500, Training Loss: -0.5350\n",
      "Epoch 34/500, Training Loss: -0.5392\n",
      "Epoch 35/500, Training Loss: -0.5588\n",
      "Epoch 36/500, Training Loss: -0.5475\n",
      "Epoch 37/500, Training Loss: -0.5538\n",
      "Epoch 38/500, Training Loss: -0.5595\n",
      "Epoch 39/500, Training Loss: -0.5447\n",
      "Epoch 40/500, Training Loss: -0.5430\n",
      "Epoch 41/500, Training Loss: -0.5351\n",
      "Epoch 42/500, Training Loss: -0.5529\n",
      "Epoch 43/500, Training Loss: -0.5377\n",
      "Epoch 44/500, Training Loss: -0.5788\n",
      "Epoch 45/500, Training Loss: -0.5591\n",
      "Epoch 46/500, Training Loss: -0.5786\n",
      "Epoch 47/500, Training Loss: -0.5563\n",
      "Epoch 48/500, Training Loss: -0.5658\n",
      "Epoch 49/500, Training Loss: -0.5569\n",
      "Epoch 50/500, Training Loss: -0.5414\n",
      "Epoch 51/500, Training Loss: -0.5093\n",
      "Epoch 52/500, Training Loss: -0.5713\n",
      "Epoch 53/500, Training Loss: -0.5217\n",
      "Epoch 54/500, Training Loss: -0.5478\n",
      "Epoch 55/500, Training Loss: -0.5479\n",
      "Epoch 56/500, Training Loss: -0.5520\n",
      "Epoch 57/500, Training Loss: -0.5408\n",
      "Epoch 58/500, Training Loss: -0.5304\n",
      "Epoch 59/500, Training Loss: -0.5758\n",
      "Epoch 60/500, Training Loss: -0.5400\n",
      "Epoch 61/500, Training Loss: -0.5551\n",
      "Epoch 62/500, Training Loss: -0.5694\n",
      "Epoch 63/500, Training Loss: -0.5627\n",
      "Epoch 64/500, Training Loss: -0.5495\n",
      "Epoch 65/500, Training Loss: -0.5337\n",
      "Epoch 66/500, Training Loss: -0.5702\n",
      "Epoch 67/500, Training Loss: -0.5276\n",
      "Epoch 68/500, Training Loss: -0.5497\n",
      "Epoch 69/500, Training Loss: -0.5423\n",
      "Epoch 70/500, Training Loss: -0.5323\n",
      "Epoch 71/500, Training Loss: -0.5692\n",
      "Epoch 72/500, Training Loss: -0.5750\n",
      "Epoch 73/500, Training Loss: -0.5642\n",
      "Epoch 74/500, Training Loss: -0.5707\n",
      "Epoch 75/500, Training Loss: -0.5340\n",
      "Epoch 76/500, Training Loss: -0.5388\n",
      "Epoch 77/500, Training Loss: -0.5657\n",
      "Epoch 78/500, Training Loss: -0.5549\n",
      "Epoch 79/500, Training Loss: -0.5482\n",
      "Epoch 80/500, Training Loss: -0.5656\n",
      "Epoch 81/500, Training Loss: -0.5307\n",
      "Epoch 82/500, Training Loss: -0.5886\n",
      "Epoch 83/500, Training Loss: -0.5487\n",
      "Epoch 84/500, Training Loss: -0.5718\n",
      "Epoch 85/500, Training Loss: -0.5539\n",
      "Epoch 86/500, Training Loss: -0.5619\n",
      "Epoch 87/500, Training Loss: -0.5534\n",
      "Epoch 88/500, Training Loss: -0.5394\n",
      "Epoch 89/500, Training Loss: -0.5428\n",
      "Epoch 90/500, Training Loss: -0.5805\n",
      "Epoch 91/500, Training Loss: -0.5452\n",
      "Epoch 92/500, Training Loss: -0.5577\n",
      "Epoch 93/500, Training Loss: -0.5534\n",
      "Epoch 94/500, Training Loss: -0.5360\n",
      "Epoch 95/500, Training Loss: -0.5591\n",
      "Epoch 96/500, Training Loss: -0.5435\n",
      "Epoch 97/500, Training Loss: -0.5401\n",
      "Epoch 98/500, Training Loss: -0.5328\n",
      "Epoch 99/500, Training Loss: -0.5561\n",
      "Epoch 100/500, Training Loss: -0.5681\n",
      "Epoch 101/500, Training Loss: -0.5640\n",
      "Epoch 102/500, Training Loss: -0.5290\n",
      "Epoch 103/500, Training Loss: -0.5602\n",
      "Epoch 104/500, Training Loss: -0.5715\n",
      "Epoch 105/500, Training Loss: -0.5558\n",
      "Epoch 106/500, Training Loss: -0.5615\n",
      "Epoch 107/500, Training Loss: -0.5454\n",
      "Epoch 108/500, Training Loss: -0.5360\n",
      "Epoch 109/500, Training Loss: -0.5445\n",
      "Epoch 110/500, Training Loss: -0.5711\n",
      "Epoch 111/500, Training Loss: -0.5359\n",
      "Epoch 112/500, Training Loss: -0.5375\n",
      "Epoch 113/500, Training Loss: -0.5565\n",
      "Epoch 114/500, Training Loss: -0.5608\n",
      "Epoch 115/500, Training Loss: -0.5401\n",
      "Epoch 116/500, Training Loss: -0.5278\n",
      "Epoch 117/500, Training Loss: -0.5613\n",
      "Epoch 118/500, Training Loss: -0.5512\n",
      "Epoch 119/500, Training Loss: -0.5464\n",
      "Epoch 120/500, Training Loss: -0.5595\n",
      "Epoch 121/500, Training Loss: -0.5520\n",
      "Epoch 122/500, Training Loss: -0.5293\n",
      "Epoch 123/500, Training Loss: -0.5544\n",
      "Epoch 124/500, Training Loss: -0.5496\n",
      "Epoch 125/500, Training Loss: -0.5387\n",
      "Epoch 126/500, Training Loss: -0.5687\n",
      "Epoch 127/500, Training Loss: -0.5669\n",
      "Epoch 128/500, Training Loss: -0.5456\n",
      "Epoch 129/500, Training Loss: -0.5549\n",
      "Epoch 130/500, Training Loss: -0.5454\n",
      "Epoch 131/500, Training Loss: -0.5387\n",
      "Epoch 132/500, Training Loss: -0.5591\n",
      "Epoch 133/500, Training Loss: -0.5430\n",
      "Epoch 134/500, Training Loss: -0.5560\n",
      "Epoch 135/500, Training Loss: -0.5564\n",
      "Epoch 136/500, Training Loss: -0.5181\n",
      "Epoch 137/500, Training Loss: -0.5525\n",
      "Epoch 138/500, Training Loss: -0.5324\n",
      "Epoch 139/500, Training Loss: -0.5520\n",
      "Epoch 140/500, Training Loss: -0.5495\n",
      "Epoch 141/500, Training Loss: -0.5557\n",
      "Epoch 142/500, Training Loss: -0.5567\n",
      "Epoch 143/500, Training Loss: -0.5736\n",
      "Epoch 144/500, Training Loss: -0.5675\n",
      "Epoch 145/500, Training Loss: -0.5374\n",
      "Epoch 146/500, Training Loss: -0.5385\n",
      "Epoch 147/500, Training Loss: -0.5619\n",
      "Epoch 148/500, Training Loss: -0.5309\n",
      "Epoch 149/500, Training Loss: -0.5648\n",
      "Epoch 150/500, Training Loss: -0.5511\n",
      "Epoch 151/500, Training Loss: -0.5780\n",
      "Epoch 152/500, Training Loss: -0.5487\n",
      "Epoch 153/500, Training Loss: -0.5187\n",
      "Epoch 154/500, Training Loss: -0.5472\n",
      "Epoch 155/500, Training Loss: -0.5161\n",
      "Epoch 156/500, Training Loss: -0.5578\n",
      "Epoch 157/500, Training Loss: -0.5314\n",
      "Epoch 158/500, Training Loss: -0.5602\n",
      "Epoch 159/500, Training Loss: -0.5348\n",
      "Epoch 160/500, Training Loss: -0.5667\n",
      "Epoch 161/500, Training Loss: -0.5522\n",
      "Epoch 162/500, Training Loss: -0.5194\n",
      "Epoch 163/500, Training Loss: -0.5648\n",
      "Epoch 164/500, Training Loss: -0.5702\n",
      "Epoch 165/500, Training Loss: -0.5536\n",
      "Epoch 166/500, Training Loss: -0.5387\n",
      "Epoch 167/500, Training Loss: -0.5372\n",
      "Epoch 168/500, Training Loss: -0.5647\n",
      "Epoch 169/500, Training Loss: -0.5550\n",
      "Epoch 170/500, Training Loss: -0.5747\n",
      "Epoch 171/500, Training Loss: -0.5460\n",
      "Epoch 172/500, Training Loss: -0.5758\n",
      "Epoch 173/500, Training Loss: -0.5759\n",
      "Epoch 174/500, Training Loss: -0.5450\n",
      "Epoch 175/500, Training Loss: -0.5428\n",
      "Epoch 176/500, Training Loss: -0.5280\n",
      "Epoch 177/500, Training Loss: -0.5495\n",
      "Epoch 178/500, Training Loss: -0.5593\n",
      "Epoch 179/500, Training Loss: -0.5283\n",
      "Epoch 180/500, Training Loss: -0.5492\n",
      "Epoch 181/500, Training Loss: -0.5368\n",
      "Epoch 182/500, Training Loss: -0.5442\n",
      "Epoch 183/500, Training Loss: -0.5490\n",
      "Epoch 184/500, Training Loss: -0.5475\n",
      "Epoch 185/500, Training Loss: -0.5405\n",
      "Epoch 186/500, Training Loss: -0.5624\n",
      "Epoch 187/500, Training Loss: -0.5626\n",
      "Epoch 188/500, Training Loss: -0.5399\n",
      "Epoch 189/500, Training Loss: -0.5407\n",
      "Epoch 190/500, Training Loss: -0.5475\n",
      "Epoch 191/500, Training Loss: -0.5638\n",
      "Epoch 192/500, Training Loss: -0.5543\n",
      "Epoch 193/500, Training Loss: -0.5194\n",
      "Epoch 194/500, Training Loss: -0.5583\n",
      "Epoch 195/500, Training Loss: -0.5392\n",
      "Epoch 196/500, Training Loss: -0.5357\n",
      "Epoch 197/500, Training Loss: -0.5682\n",
      "Epoch 198/500, Training Loss: -0.5347\n",
      "Epoch 199/500, Training Loss: -0.5582\n",
      "Epoch 200/500, Training Loss: -0.5491\n",
      "Epoch 201/500, Training Loss: -0.5533\n",
      "Epoch 202/500, Training Loss: -0.5469\n",
      "Epoch 203/500, Training Loss: -0.5393\n",
      "Epoch 204/500, Training Loss: -0.5462\n",
      "Epoch 205/500, Training Loss: -0.5741\n",
      "Epoch 206/500, Training Loss: -0.5810\n",
      "Epoch 207/500, Training Loss: -0.5222\n",
      "Epoch 208/500, Training Loss: -0.5533\n",
      "Epoch 209/500, Training Loss: -0.5492\n",
      "Epoch 210/500, Training Loss: -0.5477\n",
      "Epoch 211/500, Training Loss: -0.5415\n",
      "Epoch 212/500, Training Loss: -0.5485\n",
      "Epoch 213/500, Training Loss: -0.5550\n",
      "Epoch 214/500, Training Loss: -0.5408\n",
      "Epoch 215/500, Training Loss: -0.5552\n",
      "Epoch 216/500, Training Loss: -0.5268\n",
      "Epoch 217/500, Training Loss: -0.5456\n",
      "Epoch 218/500, Training Loss: -0.5718\n",
      "Epoch 219/500, Training Loss: -0.5366\n",
      "Epoch 220/500, Training Loss: -0.5289\n",
      "Epoch 221/500, Training Loss: -0.5683\n",
      "Epoch 222/500, Training Loss: -0.5346\n",
      "Epoch 223/500, Training Loss: -0.5324\n",
      "Epoch 224/500, Training Loss: -0.5925\n",
      "Epoch 225/500, Training Loss: -0.5402\n",
      "Epoch 226/500, Training Loss: -0.5501\n",
      "Epoch 227/500, Training Loss: -0.5389\n",
      "Epoch 228/500, Training Loss: -0.5490\n",
      "Epoch 229/500, Training Loss: -0.5393\n",
      "Epoch 230/500, Training Loss: -0.5488\n",
      "Epoch 231/500, Training Loss: -0.5409\n",
      "Epoch 232/500, Training Loss: -0.5189\n",
      "Epoch 233/500, Training Loss: -0.5432\n",
      "Epoch 234/500, Training Loss: -0.5500\n",
      "Epoch 235/500, Training Loss: -0.5442\n",
      "Epoch 236/500, Training Loss: -0.5858\n",
      "Epoch 237/500, Training Loss: -0.5539\n",
      "Epoch 238/500, Training Loss: -0.5448\n",
      "Epoch 239/500, Training Loss: -0.5163\n",
      "Epoch 240/500, Training Loss: -0.5428\n",
      "Epoch 241/500, Training Loss: -0.5697\n",
      "Epoch 242/500, Training Loss: -0.5537\n",
      "Epoch 243/500, Training Loss: -0.5860\n",
      "Epoch 244/500, Training Loss: -0.5615\n",
      "Epoch 245/500, Training Loss: -0.5510\n",
      "Epoch 246/500, Training Loss: -0.5715\n",
      "Epoch 247/500, Training Loss: -0.5288\n",
      "Epoch 248/500, Training Loss: -0.5292\n",
      "Epoch 249/500, Training Loss: -0.5654\n",
      "Epoch 250/500, Training Loss: -0.5433\n",
      "Epoch 251/500, Training Loss: -0.5573\n",
      "Epoch 252/500, Training Loss: -0.5379\n",
      "Epoch 253/500, Training Loss: -0.5756\n",
      "Epoch 254/500, Training Loss: -0.5825\n",
      "Epoch 255/500, Training Loss: -0.5465\n",
      "Epoch 256/500, Training Loss: -0.5693\n",
      "Epoch 257/500, Training Loss: -0.5480\n",
      "Epoch 258/500, Training Loss: -0.5619\n",
      "Epoch 259/500, Training Loss: -0.5559\n",
      "Epoch 260/500, Training Loss: -0.5742\n",
      "Epoch 261/500, Training Loss: -0.5316\n",
      "Epoch 262/500, Training Loss: -0.5430\n",
      "Epoch 263/500, Training Loss: -0.5702\n",
      "Epoch 264/500, Training Loss: -0.5458\n",
      "Epoch 265/500, Training Loss: -0.5556\n",
      "Epoch 266/500, Training Loss: -0.5521\n",
      "Epoch 267/500, Training Loss: -0.5461\n",
      "Epoch 268/500, Training Loss: -0.5163\n",
      "Epoch 269/500, Training Loss: -0.5385\n",
      "Epoch 270/500, Training Loss: -0.5477\n",
      "Epoch 271/500, Training Loss: -0.5417\n",
      "Epoch 272/500, Training Loss: -0.5642\n",
      "Epoch 273/500, Training Loss: -0.5322\n",
      "Epoch 274/500, Training Loss: -0.5487\n",
      "Epoch 275/500, Training Loss: -0.5214\n",
      "Epoch 276/500, Training Loss: -0.5444\n",
      "Epoch 277/500, Training Loss: -0.5325\n",
      "Epoch 278/500, Training Loss: -0.5810\n",
      "Epoch 279/500, Training Loss: -0.5518\n",
      "Epoch 280/500, Training Loss: -0.5575\n",
      "Epoch 281/500, Training Loss: -0.5836\n",
      "Epoch 282/500, Training Loss: -0.5437\n",
      "Epoch 283/500, Training Loss: -0.5315\n",
      "Epoch 284/500, Training Loss: -0.5359\n",
      "Epoch 285/500, Training Loss: -0.5540\n",
      "Epoch 286/500, Training Loss: -0.5610\n",
      "Epoch 287/500, Training Loss: -0.5629\n",
      "Epoch 288/500, Training Loss: -0.5716\n",
      "Epoch 289/500, Training Loss: -0.5140\n",
      "Epoch 290/500, Training Loss: -0.5642\n",
      "Epoch 291/500, Training Loss: -0.5658\n",
      "Epoch 292/500, Training Loss: -0.5360\n",
      "Epoch 293/500, Training Loss: -0.5197\n",
      "Epoch 294/500, Training Loss: -0.5712\n",
      "Epoch 295/500, Training Loss: -0.5361\n",
      "Epoch 296/500, Training Loss: -0.5247\n",
      "Epoch 297/500, Training Loss: -0.5992\n",
      "Epoch 298/500, Training Loss: -0.5567\n",
      "Epoch 299/500, Training Loss: -0.5527\n",
      "Epoch 300/500, Training Loss: -0.5636\n",
      "Epoch 301/500, Training Loss: -0.5553\n",
      "Epoch 302/500, Training Loss: -0.5389\n",
      "Epoch 303/500, Training Loss: -0.5404\n",
      "Epoch 304/500, Training Loss: -0.5556\n",
      "Epoch 305/500, Training Loss: -0.5561\n",
      "Epoch 306/500, Training Loss: -0.5376\n",
      "Epoch 307/500, Training Loss: -0.5550\n",
      "Epoch 308/500, Training Loss: -0.5628\n",
      "Epoch 309/500, Training Loss: -0.5669\n",
      "Epoch 310/500, Training Loss: -0.5512\n",
      "Epoch 311/500, Training Loss: -0.5682\n",
      "Epoch 312/500, Training Loss: -0.5502\n",
      "Epoch 313/500, Training Loss: -0.5171\n",
      "Epoch 314/500, Training Loss: -0.5290\n",
      "Epoch 315/500, Training Loss: -0.5265\n",
      "Epoch 316/500, Training Loss: -0.5393\n",
      "Epoch 317/500, Training Loss: -0.5305\n",
      "Epoch 318/500, Training Loss: -0.5400\n",
      "Epoch 319/500, Training Loss: -0.5375\n",
      "Epoch 320/500, Training Loss: -0.5456\n",
      "Epoch 321/500, Training Loss: -0.5363\n",
      "Epoch 322/500, Training Loss: -0.5599\n",
      "Epoch 323/500, Training Loss: -0.5764\n",
      "Epoch 324/500, Training Loss: -0.5621\n",
      "Epoch 325/500, Training Loss: -0.5527\n",
      "Epoch 326/500, Training Loss: -0.5344\n",
      "Epoch 327/500, Training Loss: -0.5548\n",
      "Epoch 328/500, Training Loss: -0.5444\n",
      "Epoch 329/500, Training Loss: -0.5318\n",
      "Epoch 330/500, Training Loss: -0.5379\n",
      "Epoch 331/500, Training Loss: -0.5359\n",
      "Epoch 332/500, Training Loss: -0.5363\n",
      "Epoch 333/500, Training Loss: -0.5662\n",
      "Epoch 334/500, Training Loss: -0.5494\n",
      "Epoch 335/500, Training Loss: -0.5493\n",
      "Epoch 336/500, Training Loss: -0.5522\n",
      "Epoch 337/500, Training Loss: -0.5308\n",
      "Epoch 338/500, Training Loss: -0.5145\n",
      "Epoch 339/500, Training Loss: -0.5652\n",
      "Epoch 340/500, Training Loss: -0.5531\n",
      "Epoch 341/500, Training Loss: -0.5722\n",
      "Epoch 342/500, Training Loss: -0.5474\n",
      "Epoch 343/500, Training Loss: -0.5383\n",
      "Epoch 344/500, Training Loss: -0.5348\n",
      "Epoch 345/500, Training Loss: -0.5664\n",
      "Epoch 346/500, Training Loss: -0.5395\n",
      "Epoch 347/500, Training Loss: -0.5435\n",
      "Epoch 348/500, Training Loss: -0.5351\n",
      "Epoch 349/500, Training Loss: -0.5588\n",
      "Epoch 350/500, Training Loss: -0.5481\n",
      "Epoch 351/500, Training Loss: -0.5501\n",
      "Epoch 352/500, Training Loss: -0.5748\n",
      "Epoch 353/500, Training Loss: -0.5637\n",
      "Epoch 354/500, Training Loss: -0.5442\n",
      "Epoch 355/500, Training Loss: -0.5279\n",
      "Epoch 356/500, Training Loss: -0.5283\n",
      "Epoch 357/500, Training Loss: -0.5607\n",
      "Epoch 358/500, Training Loss: -0.5374\n",
      "Epoch 359/500, Training Loss: -0.5497\n",
      "Epoch 360/500, Training Loss: -0.5563\n",
      "Epoch 361/500, Training Loss: -0.5531\n",
      "Epoch 362/500, Training Loss: -0.5350\n",
      "Epoch 363/500, Training Loss: -0.5551\n",
      "Epoch 364/500, Training Loss: -0.5403\n",
      "Epoch 365/500, Training Loss: -0.5321\n",
      "Epoch 366/500, Training Loss: -0.5301\n",
      "Epoch 367/500, Training Loss: -0.5539\n",
      "Epoch 368/500, Training Loss: -0.5389\n",
      "Epoch 369/500, Training Loss: -0.5429\n",
      "Epoch 370/500, Training Loss: -0.5699\n",
      "Epoch 371/500, Training Loss: -0.5365\n",
      "Epoch 372/500, Training Loss: -0.5687\n",
      "Epoch 373/500, Training Loss: -0.5492\n",
      "Epoch 374/500, Training Loss: -0.5181\n",
      "Epoch 375/500, Training Loss: -0.5554\n",
      "Epoch 376/500, Training Loss: -0.5505\n",
      "Epoch 377/500, Training Loss: -0.5770\n",
      "Epoch 378/500, Training Loss: -0.5527\n",
      "Epoch 379/500, Training Loss: -0.5269\n",
      "Epoch 380/500, Training Loss: -0.5482\n",
      "Epoch 381/500, Training Loss: -0.5289\n",
      "Epoch 382/500, Training Loss: -0.5430\n",
      "Epoch 383/500, Training Loss: -0.5483\n",
      "Epoch 384/500, Training Loss: -0.5733\n",
      "Epoch 385/500, Training Loss: -0.5846\n",
      "Epoch 386/500, Training Loss: -0.5190\n",
      "Epoch 387/500, Training Loss: -0.5478\n",
      "Epoch 388/500, Training Loss: -0.5463\n",
      "Epoch 389/500, Training Loss: -0.5581\n",
      "Epoch 390/500, Training Loss: -0.5557\n",
      "Epoch 391/500, Training Loss: -0.5563\n",
      "Epoch 392/500, Training Loss: -0.5327\n",
      "Epoch 393/500, Training Loss: -0.5808\n",
      "Epoch 394/500, Training Loss: -0.5610\n",
      "Epoch 395/500, Training Loss: -0.5187\n",
      "Epoch 396/500, Training Loss: -0.5347\n",
      "Epoch 397/500, Training Loss: -0.5504\n",
      "Epoch 398/500, Training Loss: -0.5382\n",
      "Epoch 399/500, Training Loss: -0.5588\n",
      "Epoch 400/500, Training Loss: -0.5419\n",
      "Epoch 401/500, Training Loss: -0.5412\n",
      "Epoch 402/500, Training Loss: -0.5076\n",
      "Epoch 403/500, Training Loss: -0.5419\n",
      "Epoch 404/500, Training Loss: -0.5482\n",
      "Epoch 405/500, Training Loss: -0.5703\n",
      "Epoch 406/500, Training Loss: -0.5167\n",
      "Epoch 407/500, Training Loss: -0.5502\n",
      "Epoch 408/500, Training Loss: -0.5298\n",
      "Epoch 409/500, Training Loss: -0.5428\n",
      "Epoch 410/500, Training Loss: -0.5261\n",
      "Epoch 411/500, Training Loss: -0.5355\n",
      "Epoch 412/500, Training Loss: -0.5473\n",
      "Epoch 413/500, Training Loss: -0.5274\n",
      "Epoch 414/500, Training Loss: -0.5777\n",
      "Epoch 415/500, Training Loss: -0.5166\n",
      "Epoch 416/500, Training Loss: -0.5487\n",
      "Epoch 417/500, Training Loss: -0.5327\n",
      "Epoch 418/500, Training Loss: -0.5054\n",
      "Epoch 419/500, Training Loss: -0.5486\n",
      "Epoch 420/500, Training Loss: -0.5226\n",
      "Epoch 421/500, Training Loss: -0.5308\n",
      "Epoch 422/500, Training Loss: -0.5544\n",
      "Epoch 423/500, Training Loss: -0.5477\n",
      "Epoch 424/500, Training Loss: -0.5589\n",
      "Epoch 425/500, Training Loss: -0.5539\n",
      "Epoch 426/500, Training Loss: -0.5363\n",
      "Epoch 427/500, Training Loss: -0.5686\n",
      "Epoch 428/500, Training Loss: -0.5439\n",
      "Epoch 429/500, Training Loss: -0.5234\n",
      "Epoch 430/500, Training Loss: -0.5570\n",
      "Epoch 431/500, Training Loss: -0.5662\n",
      "Epoch 432/500, Training Loss: -0.5590\n",
      "Epoch 433/500, Training Loss: -0.5292\n",
      "Epoch 434/500, Training Loss: -0.5301\n",
      "Epoch 435/500, Training Loss: -0.5512\n",
      "Epoch 436/500, Training Loss: -0.5565\n",
      "Epoch 437/500, Training Loss: -0.5549\n",
      "Epoch 438/500, Training Loss: -0.5617\n",
      "Epoch 439/500, Training Loss: -0.5378\n",
      "Epoch 440/500, Training Loss: -0.5697\n",
      "Epoch 441/500, Training Loss: -0.5570\n",
      "Epoch 442/500, Training Loss: -0.5678\n",
      "Epoch 443/500, Training Loss: -0.5642\n",
      "Epoch 444/500, Training Loss: -0.5436\n",
      "Epoch 445/500, Training Loss: -0.5572\n",
      "Epoch 446/500, Training Loss: -0.5441\n",
      "Epoch 447/500, Training Loss: -0.5575\n",
      "Epoch 448/500, Training Loss: -0.5472\n",
      "Epoch 449/500, Training Loss: -0.5291\n",
      "Epoch 450/500, Training Loss: -0.5304\n",
      "Epoch 451/500, Training Loss: -0.5472\n",
      "Epoch 452/500, Training Loss: -0.5544\n",
      "Epoch 453/500, Training Loss: -0.5372\n",
      "Epoch 454/500, Training Loss: -0.5442\n",
      "Epoch 455/500, Training Loss: -0.5471\n",
      "Epoch 456/500, Training Loss: -0.5397\n",
      "Epoch 457/500, Training Loss: -0.5681\n",
      "Epoch 458/500, Training Loss: -0.5561\n",
      "Epoch 459/500, Training Loss: -0.5828\n",
      "Epoch 460/500, Training Loss: -0.5419\n",
      "Epoch 461/500, Training Loss: -0.5210\n",
      "Epoch 462/500, Training Loss: -0.5486\n",
      "Epoch 463/500, Training Loss: -0.5439\n",
      "Epoch 464/500, Training Loss: -0.5562\n",
      "Epoch 465/500, Training Loss: -0.5676\n",
      "Epoch 466/500, Training Loss: -0.5555\n",
      "Epoch 467/500, Training Loss: -0.5642\n",
      "Epoch 468/500, Training Loss: -0.5750\n",
      "Epoch 469/500, Training Loss: -0.5834\n",
      "Epoch 470/500, Training Loss: -0.5676\n",
      "Epoch 471/500, Training Loss: -0.5591\n",
      "Epoch 472/500, Training Loss: -0.5294\n",
      "Epoch 473/500, Training Loss: -0.5374\n",
      "Epoch 474/500, Training Loss: -0.5368\n",
      "Epoch 475/500, Training Loss: -0.5732\n",
      "Epoch 476/500, Training Loss: -0.5549\n",
      "Epoch 477/500, Training Loss: -0.5484\n",
      "Epoch 478/500, Training Loss: -0.5349\n",
      "Epoch 479/500, Training Loss: -0.5435\n",
      "Epoch 480/500, Training Loss: -0.5784\n",
      "Epoch 481/500, Training Loss: -0.5579\n",
      "Epoch 482/500, Training Loss: -0.5652\n",
      "Epoch 483/500, Training Loss: -0.5873\n",
      "Epoch 484/500, Training Loss: -0.5380\n",
      "Epoch 485/500, Training Loss: -0.5182\n",
      "Epoch 486/500, Training Loss: -0.5874\n",
      "Epoch 487/500, Training Loss: -0.5553\n",
      "Epoch 488/500, Training Loss: -0.5447\n",
      "Epoch 489/500, Training Loss: -0.5493\n",
      "Epoch 490/500, Training Loss: -0.5301\n",
      "Epoch 491/500, Training Loss: -0.5660\n",
      "Epoch 492/500, Training Loss: -0.5636\n",
      "Epoch 493/500, Training Loss: -0.5407\n",
      "Epoch 494/500, Training Loss: -0.5592\n",
      "Epoch 495/500, Training Loss: -0.5453\n",
      "Epoch 496/500, Training Loss: -0.5230\n"
     ]
    }
   ],
   "source": [
    "# Training with urr\n",
    "kernel = [gaussian_kernel()] * 2\n",
    "loss_factory = CocycleLossFactory(kernel)\n",
    "loss= loss_factory.build_loss(\"URR\", X, Y, subsamples=10**4)\n",
    "final_model_urr_l, (best_index_urr_l, val_loss_urr_l) = validate(\n",
    "        models_urr_laplace,\n",
    "        loss,\n",
    "        X,\n",
    "        Y,\n",
    "        loss_val=loss,\n",
    "        method=\"CV\",\n",
    "        train_val_split=0.5,\n",
    "        opt_kwargs=opt_config,\n",
    "        hyper_kwargs=hyper_args,\n",
    "        choose_best_model=\"overall\",\n",
    "        retrain=True,\n",
    "    )\n",
    "print(f\"Best overall model index: {best_index_urr} with average validation loss {val_loss_urr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c3de577-40f5-4ad3-991f-3adb56d3e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1]) torch.Size([1000, 1])\n",
      "L2      KS_int=0.114  CF_RMSE=0.221\n",
      "L1      KS_int=0.054  CF_RMSE=0.047\n",
      "URR L2  KS_int=0.109  CF_RMSE=0.177\n",
      "URR L1  KS_int=0.054  CF_RMSE=0.095\n",
      "CMMD_V  KS_int=0.045  CF_RMSE=0.022\n",
      "CMMD_U  KS_int=0.045  CF_RMSE=0.020\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) KS and RMSE helpers (pure Torch)\n",
    "# -------------------------------------------------------------------\n",
    "def ks_statistic(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a = a.flatten(); b = b.flatten()\n",
    "    a_s, _ = torch.sort(a); b_s, _ = torch.sort(b)\n",
    "    all_vs = torch.cat([a_s, b_s]).unique()\n",
    "    cdf_a = torch.bucketize(all_vs, a_s, right=True).float() / a_s.numel()\n",
    "    cdf_b = torch.bucketize(all_vs, b_s, right=True).float() / b_s.numel()\n",
    "    return (torch.abs(cdf_a - cdf_b).max()).item()\n",
    "\n",
    "def rmse(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    return torch.sqrt(((a - b)**2).mean()).item()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Evaluation loop\n",
    "# -------------------------------------------------------------------\n",
    "def evaluate_models(\n",
    "    models_dict: dict,\n",
    "    index_dict: dict,\n",
    "    X: torch.Tensor,\n",
    "    Y: torch.Tensor,\n",
    "    B: torch.Tensor,\n",
    "    U: torch.Tensor,\n",
    "    sig_noise_ratio: float,\n",
    "    seed: int = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    models_dict: { name: (model, model_type) }\n",
    "      model_type in {'cocycle','hsic','cmmd_v','cmmd_u','l2','l1','urr'}\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        name: {\n",
    "          'KS_int':   KS between Y_model(X+1) and Y_true(X+1),\n",
    "          'KS_cf':    KS between (Y_cf-Y) and (Y_true-Y),\n",
    "          'CF_RMSE':  RMSE between (Y_cf-Y) and (Y_true-Y)\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    _, P = Y.shape\n",
    "    device = X.device\n",
    "\n",
    "    # “true” interventional outcome\n",
    "    Xp      = X + 1.0  \n",
    "    Y_true  = Xp + U            # (N,P)\n",
    "    ΔY_true = torch.ones((N,P))          # (N,P)\n",
    "\n",
    "    results = {}\n",
    "    for name, (model, mtype) in models_dict.items():\n",
    "        # ---- interventional estimate ----\n",
    "        if mtype in ('hsic','cmmdv','cmmdu'):\n",
    "            Y_int = model.cocycle(Xp, X, Y)             # (N,P)\n",
    "        else:  # l2, l1, urr\n",
    "            Uhat = model.base_distribution.sample((N,))\n",
    "            out = model.transformation(Xp, Uhat)           # either y or (y,logdet)\n",
    "            Y_int = out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "        KS_int = ks_statistic(Y_int[:,0],   Y_true[:,0])\n",
    "\n",
    "        # ---- counterfactual via cocycle ----\n",
    "        model.transformer.logdet = False\n",
    "        Y_cf    = model.cocycle(Xp, X, Y)     # (N,P)\n",
    "        ΔY_model= Y_cf - Y                    # (N,P)\n",
    "\n",
    "        RMSE_cf = rmse(    ΔY_model[:,0],   ΔY_true[:,0])\n",
    "\n",
    "        results[name] = {\n",
    "            'KS_int':  KS_int,\n",
    "            'CF_RMSE': RMSE_cf,\n",
    "            'index': index_dict[name][0]\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Example usage\n",
    "# -------------------------------------------------------------------\n",
    "my_models = {\n",
    "    'L2' : (final_model_l2, 'l2'),\n",
    "    'L1' : (final_model_l1, 'l1'),\n",
    "    'URR L2': (final_model_urr,'urr'),\n",
    "    'URR L1': (final_model_urr_l,'urr'),\n",
    "    'CMMD_V': (final_model_cmmdv,'cmmdv'),\n",
    "    'CMMD_U': (final_model_cmmdu,'cmmdu'),\n",
    "}\n",
    "\n",
    "my_indexes = {\n",
    "    'L2' : (best_index_l2, 'l2'),\n",
    "    'L1' : (best_index_l1, 'l1'),\n",
    "    'URR L2': (best_index_urr,'urr'),\n",
    "    'URR L1': (best_index_urr_l,'urr'),\n",
    "    'CMMD_V': (best_index_cmmdv,'cmmdv'),\n",
    "    'CMMD_U': (best_index_cmmdu,'cmmdu'),\n",
    "}\n",
    "\n",
    "final_model_l2.inverse_transformation(X,Y).detach()\n",
    "\n",
    "metrics = evaluate_models(\n",
    "    my_models,\n",
    "    my_indexes,\n",
    "    X, Y,\n",
    "    B=torch.ones((1,1)), \n",
    "    U = Uint,\n",
    "    sig_noise_ratio=sig_noise_ratio,\n",
    "    seed=2025\n",
    ")\n",
    "\n",
    "# Print nicely\n",
    "for name, m in metrics.items():\n",
    "     print(f\"{name:6s}  KS_int={m['KS_int']:.3f}  CF_RMSE={m['CF_RMSE']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
