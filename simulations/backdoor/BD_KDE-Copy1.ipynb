{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67c7bd58-2d8b-47bb-8f83-e0320d12d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.distributions import Normal,Uniform,Gamma,Laplace,OneHotCategorical\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from causal_cocycle.kernels import *\n",
    "from causal_cocycle.kde import *\n",
    "from causal_cocycle.regression_functionals import *\n",
    "from causal_cocycle.distribution_estimation import *\n",
    "from causal_cocycle.helper_functions import propensity_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICSCORE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314768ae-d62f-48ba-95f2-9c30e62f1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP functions\n",
    "def cdf(X,t):\n",
    "    return ((X<= t.T)*1).float().mean(0)\n",
    "\n",
    "class IG:\n",
    "    \n",
    "    def __init__(self,alpha,beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def sample(self,size):\n",
    "        return 1/Gamma(self.alpha,self.beta).sample(size)\n",
    "\n",
    "class Mixture1D:\n",
    "    \n",
    "    def __init__(self,base_dists,probabilities,noints,scales):\n",
    "        self.dists = base_dists\n",
    "        self.probabilities = probabilities\n",
    "        self.noints = noints\n",
    "        self.scales = scales\n",
    "        \n",
    "    def sample(self,size):\n",
    "        C = OneHotCategorical(probabilities).sample(size)[:,0]\n",
    "        Z = torch.zeros((size[0],len(probabilities)))\n",
    "        for i in range(len(self.dists)):\n",
    "            Z[:,i] = self.noints[i]+self.scales[i]*self.dists[i].sample(size).T\n",
    "        return (Z*C).sum(1)[:,None]          \n",
    "\n",
    "def policy(V, flip_prob = 0.00):\n",
    "    Z = (V.mean(1)*len(V.T)**0.5)[:,None]\n",
    "    X_correct =  (Z<-1)*0+(Z>=-1)*(Z<1)*1 + (Z>=1)*2\n",
    "    flips = (Uniform(0,1).sample((len(V),1))<flip_prob)*1\n",
    "    return X_correct*(1-flips) + torch.randint(3, (len(V),1))*flips\n",
    "\n",
    "def new_policy(V, flip_prob = 0.00):\n",
    "    Z = (V.mean(1)*len(V.T)**0.5)[:,None]\n",
    "    X_correct =  (Z<-1)*0+(Z>=-1)*1\n",
    "    flips = (Uniform(0,1).sample((len(V),1))<flip_prob)*1\n",
    "    return X_correct*(1-flips) + torch.randint(2, (len(V),1))*flips\n",
    "\n",
    "def shift(V,policy,coeffs):\n",
    "    t = policy(V)\n",
    "    z = V @ coeffs\n",
    "    return 1/(1+torch.exp(z)) + ((t==0)*torch.exp(-0.1*(z+3)**2) + \n",
    "                                 (t==1)*torch.exp(-0.1*(z-0)**2)*0.75 + \n",
    "                                 (t==2)*torch.exp(-0.1*(z-3)**2)*0.5)\n",
    "\n",
    "def scale(V,coeffs):\n",
    "    z = V @ coeffs\n",
    "    return 0.1*(torch.exp(-1/10*(z+2)**2*(z-2)**2)+1)\n",
    "\n",
    "def DGP(N,D,policy,covariate_corr = 0, \n",
    "        covariate_dist = Normal(0,1),\n",
    "        noise_dist = Normal(0,1)):\n",
    "    Sigma = (1-covariate_corr)*torch.eye(D)+covariate_corr*torch.ones((D,D))\n",
    "    A = torch.linalg.cholesky(Sigma)\n",
    "    Z = covariate_dist.sample((N,D)) @ A.T\n",
    "    U = noise_dist.sample((N,1))\n",
    "    Y = shift(Z,policy,coeffs) + scale(Z,coeffs)*U\n",
    "    X = torch.column_stack((policy(Z),Z))\n",
    "    return Z,X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1971311d-a36d-414d-9f98-faccc9e38696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP set up\n",
    "N = 10**4\n",
    "D = 10\n",
    "Zcorr = 0.0\n",
    "flip_prob = 0.05\n",
    "coeffs = 1/torch.linspace(1,D,D)[:,None]**1\n",
    "coeffs *= 1/coeffs.sum()\n",
    "means = torch.tensor([[-2, 0]]).T # means for mixture U dist\n",
    "scales = torch.tensor([[-1.0, 1.0]]).T  # variances for mixture U dist\n",
    "probabilities = torch.tensor([1/2,1/2]) # mixture probs for mixture U dist\n",
    "base_dists = [IG(10,10),IG(1,1)]\n",
    "noise_dist = Mixture1D(base_dists,probabilities,means,scales)\n",
    "Zdist = Normal(0,1.5)\n",
    "feature = lambda x,t: (torch.sigmoid(x)<=t.T).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "109a75da-75d2-4507-853c-2c06a96be92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method + opt set up\n",
    "train_val_split = 0.5\n",
    "functional = KRR_functional\n",
    "kernel = gaussian_kernel\n",
    "subsample = True\n",
    "subsamples = 1000\n",
    "lr = 0.025\n",
    "nfold = 3\n",
    "tol = 1e-3\n",
    "ls_method = \"med_heuristic\"\n",
    "hyper_grid = 2**torch.linspace(-10,0,10)\n",
    "t_train = torch.linspace(0,1,100)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7214612-c8a3-4b55-b301-ca78b3fb8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP\n",
    "ntrain = int(train_val_split*N)\n",
    "Z,X,Y = DGP(N,D,partial(policy,flip_prob = flip_prob),Zcorr,Zdist,noise_dist)\n",
    "Ztrain,Xtrain,Ytrain = Z[:ntrain],X[:ntrain],Y[:ntrain]\n",
    "Ztest,Xtest,Ytest = Z[ntrain:],X[ntrain:],Y[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a641773-bdfa-421e-a072-1250c20c0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319708/925322502.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k_x = kernel(lengthscale = torch.tensor(ls,requires_grad = True))\n"
     ]
    }
   ],
   "source": [
    "# Getting data to train on\n",
    "inputs_train = Xtrain\n",
    "outputs_train = feature(Ytrain,t_train).float()\n",
    "\n",
    "# Getting median heuristic\n",
    "Distances= torch.tril((inputs_train[...,None]-inputs_train[...,None].T)**2)\n",
    "ls = (Distances[Distances!=0].median()/2).sqrt() * torch.ones(D+1)\n",
    "                      \n",
    "# Defining model\n",
    "k_x = kernel(lengthscale = torch.tensor(ls,requires_grad = True))\n",
    "regressor = functional(k_x)\n",
    "CE = Conditional_Expectation_Regressor(regressor)\n",
    "\n",
    "# If not med heuristic gradient-descent on hypers\n",
    "if ls_method != \"med_heuristic\":\n",
    "    losses = CE.optimise(inputs_train,outputs_train,\n",
    "                    learn_rate = lr, \n",
    "                    subsample = subsample,\n",
    "                    subsamples = subsamples,\n",
    "                    nfold = nfold,\n",
    "                    tol = tol)\n",
    "# Or, as long as there is a hyper to opt, do cvgridsearch\n",
    "else:\n",
    "    if len(regressor.hyperparameters)>1:\n",
    "        CE.CVgridsearch(inputs_train,outputs_train,\n",
    "                        nfold = nfold, \n",
    "                        subsample = subsample,\n",
    "                        subsamples = subsamples,\n",
    "                        hyper_grid = hyper_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da176518-dedd-4cee-a355-2c75ffe03734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from interventional distribution\n",
    "ntest = 5000\n",
    "nintsample = ntest\n",
    "Zintdist = Normal(1,1.5)\n",
    "Zshift,Xshift,Yshift = DGP(nintsample,D,partial(policy),Zcorr,Zintdist,noise_dist)\n",
    "Zint,Xint,Yint = DGP(nintsample,D,partial(new_policy),Zcorr,Zintdist,noise_dist)\n",
    "Zshift_train,Xshift_train = Zshift[:ntest],Xshift[:ntest]\n",
    "Xint_train = Xint[:ntest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c466bd6e-4f1e-4412-a334-774146cae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf values\n",
    "t = torch.linspace(0,1,1000)[:,None]\n",
    "#outputs_train = feature(Yshift,t).float()\n",
    "#inputs_train = Xshift_train\n",
    "inputs_train = Xtrain\n",
    "outputs_train = feature(Ytrain,t).float()\n",
    "\n",
    "# Getting CDF\n",
    "kde_cdf_int = CE.forward(outputs_train,inputs_train,Xint_train).mean(0).detach()\n",
    "kde_cdf_shift = CE.forward(outputs_train,inputs_train,Xshift_train).mean(0).detach()\n",
    "\n",
    "# True cdf\n",
    "true_cdf_int = feature(Yint,t).mean(0)\n",
    "true_cdf_shift = feature(Yshift,t).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c34482f-a4ee-4eb0-b210-89aa12db6254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0416)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kde_cdf_int-true_cdf_int).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8660d1f-01ce-4d2c-a2ff-b854bc393244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training propensity score models\n",
    "Propensity_score_model_est = []\n",
    "Propensity_score_model_policy = []\n",
    "Propensity_score_model_new_policy = []\n",
    "\n",
    "# Estimating mistae probabilities\n",
    "Xtrue = policy(Ztrain)\n",
    "states = torch.unique(X[:,0]).int()\n",
    "nstate = len(states)\n",
    "P = torch.zeros((nstate,nstate))\n",
    "for i in range(nstate):\n",
    "    for j in range(nstate):\n",
    "        P[i,j] = ((Xtrain[:,0]==states[i])*(Xtrue[:,0]==states[j])).float().sum()\n",
    "P *= 1/P.sum(0)\n",
    "\n",
    "propensity_model_est = propensity_score(P,policy)\n",
    "propensity_model_new_policy = propensity_score(torch.eye(len(P)),new_policy)  \n",
    "propensity_model_policy = propensity_score(torch.eye(len(P)),policy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af76e467-3df9-40d0-af2d-5f834e93508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 , loss =  tensor(68978.4219)\n",
      "iter 10 , loss =  tensor(68913.3906)\n",
      "iter 20 , loss =  tensor(68898.0156)\n",
      "iter 30 , loss =  tensor(68892.9688)\n",
      "iter 40 , loss =  tensor(68891.3359)\n",
      "iter 50 , loss =  tensor(68890.8906)\n",
      "iter 60 , loss =  tensor(68890.8516)\n",
      "iter 70 , loss =  tensor(68890.8281)\n",
      "iter 80 , loss =  tensor(68890.7969)\n",
      "iter 90 , loss =  tensor(68890.7812)\n",
      "iter 0 , loss =  tensor(68978.7656)\n",
      "iter 10 , loss =  tensor(68914.7656)\n",
      "iter 20 , loss =  tensor(68904.9766)\n",
      "iter 30 , loss =  tensor(68901.3984)\n",
      "iter 40 , loss =  tensor(68900.2422)\n",
      "iter 50 , loss =  tensor(68899.7188)\n",
      "iter 60 , loss =  tensor(68899.5625)\n",
      "iter 70 , loss =  tensor(68899.5312)\n",
      "iter 80 , loss =  tensor(68899.5391)\n",
      "iter 90 , loss =  tensor(68899.5391)\n"
     ]
    }
   ],
   "source": [
    "# Training density models\n",
    "kde_learn_rate = 0.1\n",
    "kde_miniter = 100\n",
    "kde_maxiter = 100\n",
    "kde_tol = 1e-2\n",
    "kde_nfold = 3\n",
    "kde_reg = 1e-6\n",
    "\n",
    "Densities_Z = []\n",
    "Densities_Z_shift = []\n",
    "    \n",
    "    \n",
    "kernel = inverse_gaussian_kernel(lengthscale = torch.ones(D),scale = 1.0)\n",
    "density_z = KDE(kernel)\n",
    "losses = density_z.optimise(Ztrain,kde_learn_rate,kde_miniter,kde_maxiter,kde_tol,kde_nfold,kde_reg)\n",
    "\n",
    "kernel_shift = inverse_gaussian_kernel(lengthscale = torch.ones(D),scale = 1.0)\n",
    "density_zshift = KDE(kernel_shift)\n",
    "losses_shift = density_zshift.optimise(Zshift_train,kde_learn_rate,kde_miniter,kde_maxiter,kde_tol,kde_nfold,kde_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f199b8b-ae56-4df6-98b2-afb0474c4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True densities\n",
    "class normal:\n",
    "\n",
    "    def __init__(self,mu,sigma2):\n",
    "        self.mu = mu\n",
    "        self.var = sigma2\n",
    "\n",
    "    def forward(self,data,X):\n",
    "        return Normal(self.mu,self.var).log_prob(data).sum(1).exp()\n",
    "\n",
    "#density_zshift = normal(1,1.5)\n",
    "#density_z = normal(0,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1e3759c-dd67-4033-9371-044b784ad83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting IPW estimator\n",
    "weights_shift = ((propensity_model_policy(Xtest,Ztest)*\n",
    "                density_zshift.forward(Ztest,Zshift_train))/\n",
    "                (propensity_model_est(Xtest,Ztest)*\n",
    "                density_z.forward(Ztest,Ztrain))).detach()\n",
    "\n",
    "weights_int = ((propensity_model_new_policy(Xtest,Ztest)*\n",
    "                density_zshift.forward(Ztest,Zshift_train))/\n",
    "                (propensity_model_est(Xtest,Ztest)*\n",
    "                density_z.forward(Ztest,Ztrain))).detach()\n",
    "\n",
    "weights_shift *= len(weights_shift)/weights_shift.sum()\n",
    "weights_int *= len(weights_shift)/weights_int.sum()\n",
    "\n",
    "IPW_cdf_shift = (weights_shift[:,None]*feature(Ytest,t)).mean(0)\n",
    "IPW_cdf_int = (weights_int[:,None]*feature(Ytest,t)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e2d0129-9e1e-4bdb-8f05-9ab661bc1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting DR estimator (start by adding on IPW term to outcome model\n",
    "kde_DR_cdf_shift = kde_cdf_shift + IPW_cdf_shift\n",
    "kde_DR_cdf_int = kde_cdf_int + IPW_cdf_int\n",
    "\n",
    "# Updating DR estimator\n",
    "kde_DR_cdf_shift -= (weights_shift[:,None]*CE.forward(outputs,inputs_train,Xtest)).mean(0)\n",
    "kde_DR_cdf_int-= (weights_int[:,None]*CE.forward(outputs,inputs_train,Xtest)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "280997ea-f3f8-425a-b5ab-354b2e36c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.3319e-07)\n",
      "tensor(5.9968e-06, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1849e-06)\n",
      "tensor(4.5334e-06, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((kde_cdf_shift -true_cdf_shift).abs().mean())\n",
    "print((kde_DR_cdf_shift -true_cdf_shift).abs().mean())\n",
    "\n",
    "print((kde_cdf_int -true_cdf_int).abs().mean())\n",
    "print((kde_DR_cdf_int -true_cdf_int).abs().mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
