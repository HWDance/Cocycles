{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c7bd58-2d8b-47bb-8f83-e0320d12d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.distributions import Normal,Uniform,Gamma,Laplace,OneHotCategorical\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from causal_cocycle.model import cocycle_model,flow_model,flow_outcome_model\n",
    "from causal_cocycle.optimise import *\n",
    "from causal_cocycle.loss_functions import Loss\n",
    "from causal_cocycle.conditioners import Empty_Conditioner,Constant_Conditioner,Lin_Conditioner,NN_RELU_Conditioner\n",
    "from causal_cocycle.transformers import Transformer,Shift_layer,Scale_layer,RQS_layer,Inverse_layer\n",
    "from causal_cocycle.helper_functions import likelihood_loss,mmd,propensity_score\n",
    "from causal_cocycle.kernels import *\n",
    "from causal_cocycle.kde import *\n",
    "\n",
    "#Shorthand function calls\n",
    "def NN(i,o=2,width=128,layers=2):\n",
    "    return NN_RELU_Conditioner(width = width,\n",
    "                                     layers = layers, \n",
    "                                     input_dims =  i, \n",
    "                                     output_dims = o,\n",
    "                                     bias = True)\n",
    "def C(rows,cols,value):\n",
    "    return Constant_Conditioner(init = torch.ones((rows,cols))*value)\n",
    "\n",
    "T = partial(Transformer,logdet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314768ae-d62f-48ba-95f2-9c30e62f1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP functions\n",
    "def cdf(X,t):\n",
    "    return ((X<= t.T)*1).float().mean(0)\n",
    "\n",
    "class IG:\n",
    "    \n",
    "    def __init__(self,alpha,beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def sample(self,size):\n",
    "        return 1/Gamma(self.alpha,self.beta).sample(size)\n",
    "\n",
    "class Mixture1D:\n",
    "    \n",
    "    def __init__(self,base_dists,probabilities,noints,scales):\n",
    "        self.dists = base_dists\n",
    "        self.probabilities = probabilities\n",
    "        self.noints = noints\n",
    "        self.scales = scales\n",
    "        \n",
    "    def sample(self,size):\n",
    "        C = OneHotCategorical(probabilities).sample(size)[:,0]\n",
    "        Z = torch.zeros((size[0],len(probabilities)))\n",
    "        for i in range(len(self.dists)):\n",
    "            Z[:,i] = self.noints[i]+self.scales[i]*self.dists[i].sample(size).T\n",
    "        return (Z*C).sum(1)[:,None]          \n",
    "\n",
    "def policy(V, flip_prob = 0.00):\n",
    "    Z = (V.mean(1)*len(V.T)**0.5)[:,None]\n",
    "    X_correct =  (Z<-1)*0+(Z>=-1)*(Z<1)*1 + (Z>=1)*2\n",
    "    flips = (Uniform(0,1).sample((len(V),1))<flip_prob)*1\n",
    "    return X_correct*(1-flips) + torch.randint(3, (len(V),1))*flips\n",
    "\n",
    "def new_policy(V, flip_prob = 0.00):\n",
    "    Z = (V.mean(1)*len(V.T)**0.5)[:,None]\n",
    "    X_correct =  (Z<-1)*0+(Z>=-1)*1\n",
    "    flips = (Uniform(0,1).sample((len(V),1))<flip_prob)*1\n",
    "    return X_correct*(1-flips) + torch.randint(2, (len(V),1))*flips\n",
    "\n",
    "def shift(V,policy,coeffs):\n",
    "    t = policy(V)\n",
    "    z = V @ coeffs\n",
    "    return 1/(1+torch.exp(z)) + ((t==0)*torch.exp(-0.1*(z+3)**2) + \n",
    "                                 (t==1)*torch.exp(-0.1*(z-0)**2)*0.75 + \n",
    "                                 (t==2)*torch.exp(-0.1*(z-3)**2)*0.5)\n",
    "\n",
    "def scale(V,coeffs):\n",
    "    z = V @ coeffs\n",
    "    return 0.1*(torch.exp(-1/10*(z+2)**2*(z-2)**2)+1)\n",
    "\n",
    "def DGP(N,D,policy,covariate_corr = 0, \n",
    "        covariate_dist = Normal(0,1),\n",
    "        noise_dist = Normal(0,1)):\n",
    "    Sigma = (1-covariate_corr)*torch.eye(D)+covariate_corr*torch.ones((D,D))\n",
    "    A = torch.linalg.cholesky(Sigma)\n",
    "    Z = covariate_dist.sample((N,D)) @ A.T\n",
    "    U = noise_dist.sample((N,1))\n",
    "    Y = shift(Z,policy,coeffs) + scale(Z,coeffs)*U\n",
    "    X = torch.column_stack((policy(Z),Z))\n",
    "    return Z,X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1971311d-a36d-414d-9f98-faccc9e38696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP set up\n",
    "N = 10**4\n",
    "D = 10\n",
    "Zcorr = 0.0\n",
    "flip_prob = 0.05\n",
    "coeffs = 1/torch.linspace(1,D,D)[:,None]**1\n",
    "coeffs *= 1/coeffs.sum()\n",
    "means = torch.tensor([[-2, 0]]).T # means for mixture U dist\n",
    "scales = torch.tensor([[-1.0, 1.0]]).T  # variances for mixture U dist\n",
    "probabilities = torch.tensor([1/2,1/2]) # mixture probs for mixture U dist\n",
    "base_dists = [IG(10,10),IG(1,1)]\n",
    "noise_dist = Mixture1D(base_dists,probabilities,means,scales)\n",
    "Zdist = Normal(0,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "109a75da-75d2-4507-853c-2c06a96be92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method + opt set up\n",
    "base_distribution = Normal(0,1)\n",
    "batch_size = 64\n",
    "validation_method = \"fixed\"\n",
    "layers = 2\n",
    "width = 128\n",
    "train_val_split = 0.5\n",
    "learn_rate = [1e-3]\n",
    "scheduler = True\n",
    "maxiter = 10000\n",
    "miniter = 10000\n",
    "weight_decay = 1e-3\n",
    "RQS_bins = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e54d7d2-ba57-4c06-92aa-8b42de1329d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training optimiser args\n",
    "opt_args = [\"learn_rate\",\n",
    "            \"scheduler\",\n",
    "            \"batch_size\",\n",
    "            \"maxiter\",\n",
    "            \"miniter\",\n",
    "            \"weight_decay\",\n",
    "            \"print_\"]\n",
    "opt_argvals = [learn_rate,\n",
    "              scheduler,\n",
    "              batch_size,\n",
    "             maxiter,\n",
    "              miniter,\n",
    "              weight_decay,\n",
    "              True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2bfc501-6bfd-410d-a8e4-b6c4f9b3e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying models for cross-validation\n",
    "\n",
    "# Specifying list of hypers to construct models from\n",
    "hypers = [\"weight_decay\"]\n",
    "hypers_list = [[0,1e-3,1e-2,1e-1,1]]\n",
    "conditioners_list = [[NN(D+1,1,width,layers),C(1,1,1),C(1,3*RQS_bins + 2,3)],\n",
    "                          [NN(D+1,1,width,layers),NN(D+1,1,width,layers),C(1,3*RQS_bins + 2,3)],\n",
    "                          [NN(D+1,1,width,layers),NN(D+1,1,width,layers),NN(D+1,3*RQS_bins + 2,width,layers)]]\n",
    "transformers_list = [Transformer([Shift_layer(),Scale_layer(),RQS_layer(RQS_bins)],logdet = True),\n",
    "                           Transformer([Shift_layer(),Scale_layer(),RQS_layer(RQS_bins)],logdet = True),\n",
    "                           Transformer([Shift_layer(),Scale_layer(),RQS_layer(RQS_bins)],logdet = True)]\n",
    "\n",
    "# Constructing all model combinations\n",
    "models_validation = []\n",
    "hyper_argvals = []\n",
    "for m in range(len(conditioners_list)):\n",
    "    for hyper in hypers_list:\n",
    "        for hyper_value in hyper:\n",
    "            models_validation.append(flow_model(conditioners_list[m],transformers_list[m]))\n",
    "            hyper_argvals.append([hyper_value])\n",
    "hyper_args = [hypers]*len(hyper_argvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7214612-c8a3-4b55-b301-ca78b3fb8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP\n",
    "ntrain = int(train_val_split*N)\n",
    "Z,X,Y = DGP(N,D,partial(policy,flip_prob = flip_prob),Zcorr,Zdist,noise_dist)\n",
    "Ztrain,Xtrain,Ytrain = Z[:ntrain],X[:ntrain],Y[:ntrain]\n",
    "Ztest,Xtest,Ytest = Z[ntrain:],X[ntrain:],Y[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a641773-bdfa-421e-a072-1250c20c0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(2.5889)\n",
      "99.9  % completion\n",
      "Currently optimising model  14 , for fold  0\n"
     ]
    }
   ],
   "source": [
    "# Getting loss functon and training model\n",
    "loss_fn =  likelihood_loss(base_distribution)\n",
    "models_validation,val_losses = validate(models_validation,\n",
    "                                         loss_fn,\n",
    "                                         X,\n",
    "                                         Y,\n",
    "                                         validation_method,\n",
    "                                         train_val_split,\n",
    "                                         opt_args,\n",
    "                                         opt_argvals,\n",
    "                                         hyper_args,\n",
    "                                         hyper_argvals)\n",
    "best_ind = torch.where(val_losses ==val_losses.min())[0][0]\n",
    "final_model = models_validation[best_ind]\n",
    "final_model.transformer.logdet = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef5a9625-85eb-4bf1-9d37-c48304850b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining outcome model and feature of interest (i.e. cdf)\n",
    "ntest = 5000\n",
    "feature = lambda x,t: (torch.sigmoid(x)[None]<= t[...,None]).float()\n",
    "Usample = base_distribution.sample((ntest,1))\n",
    "conditional_mean_model = flow_outcome_model(final_model,Usample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da176518-dedd-4cee-a355-2c75ffe03734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from interventional distribution\n",
    "nintsample = 10**5\n",
    "Zintdist = Normal(1,0.5)\n",
    "Zshift,Xshift,Yshift = DGP(nintsample,D,partial(policy),Zcorr,Zintdist,noise_dist)\n",
    "Zint,Xint,Yint = DGP(nintsample,D,partial(new_policy),Zcorr,Zintdist,noise_dist)\n",
    "Zshift_train,Xshift_train = Zshift[:ntest],Xshift[:ntest]\n",
    "Xint_train = Xint[:ntest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c466bd6e-4f1e-4412-a334-774146cae7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cdf value batch  0 / 10\n",
      "getting cdf value batch  1 / 10\n",
      "getting cdf value batch  2 / 10\n",
      "getting cdf value batch  3 / 10\n",
      "getting cdf value batch  4 / 10\n",
      "getting cdf value batch  5 / 10\n",
      "getting cdf value batch  6 / 10\n",
      "getting cdf value batch  7 / 10\n",
      "getting cdf value batch  8 / 10\n",
      "getting cdf value batch  9 / 10\n"
     ]
    }
   ],
   "source": [
    "# cdf values\n",
    "t = torch.linspace(0,1,1000)[:,None]\n",
    "\n",
    "# Cocycle model cdf\n",
    "batch = 100\n",
    "nbatch = int(len(t)/batch)\n",
    "SCM_cdf_int = torch.zeros(len(t))\n",
    "SCM_cdf_shift = torch.zeros(len(t))\n",
    "for i in range(nbatch):\n",
    "    SCM_cdf_int[i*batch:(i+1)*batch] = conditional_mean_model(Xint_train,\n",
    "                                                              partial(feature,t = t[i*batch:(i+1)*batch])).mean(1)\n",
    "    SCM_cdf_shift[i*batch:(i+1)*batch] = conditional_mean_model(Xshift_train,\n",
    "                                                              partial(feature,t = t[i*batch:(i+1)*batch])).mean(1)\n",
    "    \n",
    "    print(\"getting cdf value batch \",i+1,\"/\",nbatch)\n",
    "\n",
    "# True cdf\n",
    "true_cdf_int = feature(Yint,t).mean((1,2))\n",
    "true_cdf_shift = feature(Yshift,t).mean((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8660d1f-01ce-4d2c-a2ff-b854bc393244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training propensity score models\n",
    "Propensity_score_model_est = []\n",
    "Propensity_score_model_policy = []\n",
    "Propensity_score_model_new_policy = []\n",
    "\n",
    "# Estimating mistae probabilities\n",
    "Xtrue = policy(Ztrain)\n",
    "states = torch.unique(X[:,0]).int()\n",
    "nstate = len(states)\n",
    "P = torch.zeros((nstate,nstate))\n",
    "for i in range(nstate):\n",
    "    for j in range(nstate):\n",
    "        P[i,j] = ((Xtrain[:,0]==states[i])*(Xtrue[:,0]==states[j])).float().sum()\n",
    "P *= 1/P.sum(0)\n",
    "\n",
    "propensity_model_est = propensity_score(P,policy)\n",
    "propensity_model_new_policy = propensity_score(torch.eye(len(P)),new_policy)  \n",
    "propensity_model_policy = propensity_score(torch.eye(len(P)),policy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af76e467-3df9-40d0-af2d-5f834e93508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 , loss =  tensor(68976.0859)\n",
      "iter 10 , loss =  tensor(68891.7734)\n",
      "iter 20 , loss =  tensor(68879.0703)\n",
      "iter 30 , loss =  tensor(68875.4609)\n",
      "iter 40 , loss =  tensor(68874.4062)\n",
      "iter 50 , loss =  tensor(68874.)\n",
      "iter 60 , loss =  tensor(68873.8047)\n",
      "iter 70 , loss =  tensor(68873.7031)\n",
      "iter 80 , loss =  tensor(68873.6875)\n",
      "iter 90 , loss =  tensor(68873.6875)\n",
      "iter 0 , loss =  tensor(56175.3359)\n",
      "iter 10 , loss =  tensor(39026.2188)\n",
      "iter 20 , loss =  tensor(39731.8789)\n",
      "iter 30 , loss =  tensor(38435.4258)\n",
      "iter 40 , loss =  tensor(38606.7148)\n",
      "iter 50 , loss =  tensor(38439.5352)\n",
      "iter 60 , loss =  tensor(38442.9180)\n",
      "iter 70 , loss =  tensor(38438.7500)\n",
      "iter 80 , loss =  tensor(38431.1484)\n",
      "iter 90 , loss =  tensor(38431.6094)\n"
     ]
    }
   ],
   "source": [
    "# Training density models\n",
    "kde_learn_rate = 0.1\n",
    "kde_miniter = 100\n",
    "kde_maxiter = 100\n",
    "kde_tol = 1e-2\n",
    "kde_nfold = 3\n",
    "kde_reg = 1e-6\n",
    "\n",
    "Densities_Z = []\n",
    "Densities_Z_shift = []\n",
    "    \n",
    "    \n",
    "kernel = inverse_gaussian_kernel(lengthscale = torch.ones(D),scale = 1.0)\n",
    "density_z = KDE(kernel)\n",
    "losses = density_z.optimise(Ztrain,kde_learn_rate,kde_miniter,kde_maxiter,kde_tol,kde_nfold,kde_reg)\n",
    "\n",
    "kernel_shift = inverse_gaussian_kernel(lengthscale = torch.ones(D),scale = 1.0)\n",
    "density_zshift = KDE(kernel_shift)\n",
    "losses_shift = density_zshift.optimise(Zshift_train,kde_learn_rate,kde_miniter,kde_maxiter,kde_tol,kde_nfold,kde_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1e3759c-dd67-4033-9371-044b784ad83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting IPW estimator\n",
    "weights_shift = ((propensity_model_policy(Xtest,Ztest)*\n",
    "                density_zshift.forward(Ztest,Zshift_train))/\n",
    "                (propensity_model_est(Xtest,Ztest)*\n",
    "                density_z.forward(Ztest,Ztrain))).detach()\n",
    "\n",
    "weights_int = ((propensity_model_new_policy(Xtest,Ztest)*\n",
    "                density_zshift.forward(Ztest,Zshift_train))/\n",
    "                (propensity_model_est(Xtest,Ztest)*\n",
    "                density_z.forward(Ztest,Ztrain))).detach()\n",
    "\n",
    "IPW_cdf_shift = (weights_shift[None,:,None]*feature(Ytest,t)).mean((1,2))\n",
    "IPW_cdf_int = (weights_int[None,:,None]*feature(Ytest,t)).mean((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e2d0129-9e1e-4bdb-8f05-9ab661bc1683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cdf value batch  0 / 10\n",
      "getting cdf value batch  1 / 10\n",
      "getting cdf value batch  2 / 10\n",
      "getting cdf value batch  3 / 10\n",
      "getting cdf value batch  4 / 10\n",
      "getting cdf value batch  5 / 10\n",
      "getting cdf value batch  6 / 10\n",
      "getting cdf value batch  7 / 10\n",
      "getting cdf value batch  8 / 10\n",
      "getting cdf value batch  9 / 10\n"
     ]
    }
   ],
   "source": [
    "# Getting DR estimator (start by adding on IPW term to outcome model\n",
    "cocycle_DR_cdf_shift = cocycle_cdf_shift + IPW_cdf_shift\n",
    "cocycle_DR_cdf_int = cocycle_cdf_int + IPW_cdf_int\n",
    "\n",
    "for i in range(nbatch):\n",
    "    # Getting batch of conditional means and propensity weights\n",
    "    conditional_mean_batch = conditional_mean_model(Xtest,partial(feature,t = t[i*batch:(i+1)*batch]))\n",
    "\n",
    "    # Updating DR estimator\n",
    "    SCM_DR_cdf_shift[i*batch:(i+1)*batch] -= (weights_shift*conditional_mean_batch).mean(1)\n",
    "    SCM_DR_cdf_int[i*batch:(i+1)*batch] -= (weights_int*conditional_mean_batch).mean(1)\n",
    "\n",
    "    print(\"getting cdf value batch \",i+1,\"/\",nbatch)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
