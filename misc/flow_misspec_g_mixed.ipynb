{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628a7a6e-2a1c-4c30-9f81-859c1b55c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from zuko.flows.autoregressive import MaskedAutoregressiveTransform\n",
    "from zuko.transforms import (\n",
    "    MonotonicAffineTransform,\n",
    "    MonotonicRQSTransform,\n",
    ")\n",
    "from zuko.flows import UnconditionalDistribution\n",
    "from torch.distributions import Cauchy, Normal, Laplace, Bernoulli, Uniform\n",
    "from causalflows.flows import CausalFlow\n",
    "import copy\n",
    "from causal_cocycle.ssvkernel import ssvkernel\n",
    "import numpy as np\n",
    "from scipy.stats import betaprime, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fd7403-1381-42ec-85d0-3fb058d35398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- helper: draw |NBP(0.1,0.1)| ---------\n",
    "def draw_abs_nbp(size):\n",
    "    \"\"\"\n",
    "    |NBP(0.1,0.1)|  where  NB P  is  Normal–Beta-Prime:\n",
    "      τ   ~ BetaPrime(0.1, 0.1)\n",
    "      V|τ ~ N(0, τ)\n",
    "      return |V|\n",
    "    \"\"\"\n",
    "    τ   = betaprime.rvs(0.1, 0.1, size=size)\n",
    "    v   = norm.rvs(scale=np.sqrt(τ))\n",
    "    return np.abs(v)\n",
    "\n",
    "class MixedTails:\n",
    "\n",
    "    def __init__(self,):\n",
    "        return\n",
    "\n",
    "    def sample(self, size):\n",
    "        b = Bernoulli(1/2).sample(size)\n",
    "        u1 = Normal(0,1).sample(size).abs()\n",
    "        u2 = torch.tensor(draw_abs_nbp(size))\n",
    "        return u1*b - u2*(1-b)\n",
    "    \n",
    "\n",
    "# ── Data ─────────────────────────────────────────────────────\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "N_train = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Y0 = MixedTails().sample((N_train,1)).to(device)\n",
    "Y1 = 2*Y0\n",
    "\n",
    "# ── Build base and transforms ───────────────────────────────\n",
    "d = 1   # dim(Y)\n",
    "# two bases (here both Gaussian; you could swap in Laplace etc.)\n",
    "base0 = UnconditionalDistribution(Normal, loc=torch.zeros(d), scale=torch.ones(d), buffer=True)\n",
    "base1 = UnconditionalDistribution(Normal, loc=torch.zeros(d), scale=torch.ones(d), buffer=True)\n",
    "\n",
    "# transform\n",
    "maf0 = [MaskedAutoregressiveTransform(\n",
    "            features=1,\n",
    "            context=0,\n",
    "            hidden_features=(),\n",
    "            univariate=MonotonicAffineTransform,\n",
    "            shapes=[(), ()],\n",
    "        ),\n",
    "        MaskedAutoregressiveTransform(\n",
    "            features=1,\n",
    "            context=0,\n",
    "            hidden_features=(),\n",
    "            univariate=MonotonicRQSTransform,\n",
    "            shapes=[(8,), (8,), (9,)],\n",
    "        ),\n",
    "       MaskedAutoregressiveTransform(\n",
    "            features=1,\n",
    "            context=0,\n",
    "            hidden_features=(),\n",
    "            univariate=MonotonicAffineTransform,\n",
    "            shapes=[(), ()],\n",
    "        ),\n",
    "    ]\n",
    "maf1 = [MaskedAutoregressiveTransform(\n",
    "            features=1,\n",
    "            context=0,\n",
    "            hidden_features=(),\n",
    "            univariate=MonotonicAffineTransform,\n",
    "            shapes=[(), ()],\n",
    "        ),\n",
    "        MaskedAutoregressiveTransform(\n",
    "            features=1,\n",
    "            context=0,\n",
    "            hidden_features=(),\n",
    "            univariate=MonotonicRQSTransform,\n",
    "            shapes=[(8,), (8,), (9,)],\n",
    "        ),\n",
    "       MaskedAutoregressiveTransform(\n",
    "            features=1,\n",
    "            context=0,\n",
    "            hidden_features=(),\n",
    "            univariate=MonotonicAffineTransform,\n",
    "            shapes=[(), ()],\n",
    "        ),\n",
    "    ]\n",
    "# instantiate two flows\n",
    "flow0 = CausalFlow(transform=maf0, base=base0).to(device)\n",
    "flow1 = copy.deepcopy(CausalFlow(transform=maf1, base=base1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f274ccf-711e-4085-9a3d-a7472a785f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | flow0 NLL = 71415279915679965184.000 | flow1 NLL = 22858605360209354752.000\n",
      "Epoch   50 | flow0 NLL = 71374002540892364800.000 | flow1 NLL = 21646607792035971072.000\n",
      "Epoch  100 | flow0 NLL = 71374002540567289856.000 | flow1 NLL = 21646607791935729664.000\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# common hyper‐params\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "\n",
    "# ── DataLoaders ──────────────────────────────────────────────\n",
    "loader0 = DataLoader(TensorDataset(Y0), batch_size=batch_size, shuffle=True)\n",
    "loader1 = DataLoader(TensorDataset(Y1), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ── Optimizers ───────────────────────────────────────────────\n",
    "opt0 = Adam(flow0.parameters(), lr=lr)\n",
    "opt1 = Adam(flow1.parameters(), lr=lr)\n",
    "\n",
    "# ── 3) Training loops ─────────────────────────────────────────────────────────\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # train flow0 on Y0\n",
    "    running0 = 0.0\n",
    "    for (y_batch,) in loader0:\n",
    "        y_batch = y_batch.to(device)\n",
    "        dist0 = flow0()                           # returns a Distribution over Y\n",
    "        loss0 = -dist0.log_prob(y_batch).mean()   # negative log-likelihood\n",
    "        opt0.zero_grad()\n",
    "        loss0.backward()\n",
    "        opt0.step()\n",
    "        running0 += loss0.item() * y_batch.size(0)\n",
    "        \n",
    "    # train flow1 on Y1\n",
    "    running1 = 0.0\n",
    "    for (y_batch,) in loader1:\n",
    "        y_batch = y_batch.to(device)\n",
    "        dist1 = flow1()\n",
    "        loss1 = -dist1.log_prob(y_batch).mean()\n",
    "        opt1.zero_grad()\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "        running1 += loss1.item() * y_batch.size(0)\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d} | flow0 NLL = {running0/N_train:.3f} | flow1 NLL = {running1/N_train:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162af55-1d51-44c8-8623-103315a8d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4) Paired sampling via reseeding ──────────────────────────────────────────\n",
    "torch.manual_seed(0)\n",
    "N_effect = 100000\n",
    "dist0 = flow0()\n",
    "with torch.no_grad():\n",
    "    u = dist0.base.sample((N_effect,1))\n",
    "    Y0_eff = dist0.transform.inv(u).cpu().squeeze()\n",
    "dist1 = flow1()\n",
    "with torch.no_grad():\n",
    "    Y1_eff = dist1.transform.inv(u).cpu().squeeze()\n",
    "\n",
    "delta = (Y1_eff - Y0_eff)\n",
    "Y0 = MixedTails().sample((N_effect,)).to(device)\n",
    "Y1 = Y0*2\n",
    "true_delta = Y1-Y0\n",
    "\n",
    "def t(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "# ── 5) Plot empirical histogram of ΔY ────────────────────────────────────────\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(t(true_delta), bins=50, density=True, alpha=0.8)\n",
    "plt.hist(t(delta), bins=50, density=True, alpha=0.8)\n",
    "plt.title(\"Empirical distribution of $Y(1)-Y(0)$\")\n",
    "plt.xlabel(\"$Y(1)-Y(0)$\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266980b-593c-429e-b68f-186f3a61c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- plot ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy.stats import norm                  # for the true density\n",
    "from ssvkernel import ssvkernel               # ← same function as before\n",
    "\n",
    "# ---------- aesthetics (unchanged) ----------\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "})\n",
    "sns.set_style(\"whitegrid\")\n",
    "col_data, col_true = sns.color_palette(\"colorblind\", 2)\n",
    "\n",
    "x_grid = np.linspace(0, 1, 200)\n",
    "y_hat = ssvkernel(t(delta).numpy(), tin=x_grid, M=80, nbs=100, WinFunc='Gauss')[0]\n",
    "y_true = ssvkernel(t(true_delta).numpy(), tin=x_grid, M=80, nbs=100, WinFunc='Gauss')[0]\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.plot(x_grid, y_hat, color=col_data, lw=10, label=\"Estimated\")\n",
    "ax.fill_between(x_grid, 0, y_hat, color=col_data, alpha=0.4)\n",
    "ax.plot(x_grid, y_true, color=col_true, lw=15, ls=\"--\", label=\"True\")\n",
    "ax.set_ylim(0,4.5)\n",
    "ax.set_xlabel(r\"$Y(1) - Y(0)$\", fontsize = 60)\n",
    "ax.set_ylabel(\"Density\", fontsize = 60)\n",
    "ax.tick_params(axis='x', labelsize=48)  # or whatever size you want\n",
    "ax.tick_params(axis='y', labelsize=48)  # or whatever size you want\n",
    "ax.set_title(r\"CNF(G) - Mixed Noise\", fontsize=60)\n",
    "ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\n",
    "#ax.legend(frameon=False, fontsize=48)\n",
    "ax.grid(alpha = 0.5)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"flow_misspec_mixed_g.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f54a1-358b-437b-ac3e-e806383038f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "\n",
    "# Colors\n",
    "col_data, col_true = sns.color_palette(\"colorblind\", 2)\n",
    "\n",
    "# KDE grids\n",
    "x_grid = np.linspace(min(t(delta).min(), t(true_delta).min()) - 0.1,\n",
    "                     max(t(delta).max(), t(true_delta).max()) + 0.1, 1000)\n",
    "\n",
    "# KDE estimation\n",
    "kde_est = gaussian_kde(t(delta))\n",
    "kde_true = gaussian_kde(t(true_delta))\n",
    "\n",
    "y_hat = kde_est(x_grid)\n",
    "y_true = kde_true(x_grid)\n",
    "\n",
    "# Plot with formatting\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "ax.plot(x_grid, y_hat, color=col_data, lw=5, label=\"Estimated\")\n",
    "ax.fill_between(x_grid, 0, y_hat, color=col_data, alpha=0.4)\n",
    "ax.plot(x_grid, y_true, color=col_true, lw=7.5, ls=\"--\", label=\"True\")\n",
    "\n",
    "ax.set_xlabel(r\"$Y(1) - Y(0)$\", fontsize=48)\n",
    "ax.set_ylabel(\"Density\", fontsize=48)\n",
    "ax.set_title(r\"Gaussian Base + Mixed Tails Noise\", fontsize=48)\n",
    "ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\n",
    "ax.legend(frameon=False, fontsize=36)\n",
    "ax.grid(alpha=0.5)\n",
    "fig.tight_layout()\n",
    "ax.set_ylim(0,4)\n",
    "fig.savefig(\"flow_misspec_mixed_g.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
