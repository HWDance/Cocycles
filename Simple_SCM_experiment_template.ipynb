{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Csuite data simulation testing template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User\n",
    "user = \"nk1922\"\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal,Laplace,Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "os.chdir('C:/Users/{0}/OneDrive/Documents/Cocycles project/Cocycle_code'.format(user))\n",
    "from Cocycle_CDAGM import *\n",
    "from Cocycle_model import *\n",
    "from Cocycle_optimise import *\n",
    "from Cocycle_loss_functions import *\n",
    "from Conditioners import *\n",
    "from Transformers import *\n",
    "from KDE_estimation import *\n",
    "from Kernels import *\n",
    "from Helper_functions import *\n",
    "os.chdir('C:/Users/{0}/OneDrive/Documents/Cocycles project/Experiments_code'.format(user))\n",
    "from Simple_SCM import simple_SCM as DGP\n",
    "from Simple_SCM import simple_SCM_conditioner as Conditioner\n",
    "os.chdir('C:/Users/{0}/OneDrive/Documents/Cocycles project/Experimental_results'.format(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental set up\n",
    "parents = [[],[0],[1]]\n",
    "trials = 10\n",
    "N,Nint = 1250,10**7\n",
    "int_levels = list(torch.linspace(0,2,20))\n",
    "intervention = lambda a,x : a+x\n",
    "quantiles = [0.1,0.5,0.9]\n",
    "adversarial_distribution = True\n",
    "store_quantiles = False\n",
    "R2 = 0.5\n",
    "\n",
    "# Cocycle Estimation set up\n",
    "cocycle_estimators = [\"CMMD_M_RFF\",\"JMMD_M_RFF\"]\n",
    "RFF_features = [True,True]\n",
    "n_RFF = [100,100,100]\n",
    "median_heuristic = [True,True]\n",
    "\n",
    "# NN set up\n",
    "train_val_split = 0.8\n",
    "ntrain = int(train_val_split*N)\n",
    "conditioner_learn_rate = 1e-2\n",
    "transformer_learn_rate = 1e-2\n",
    "val_tol = 1\n",
    "scheduler = True\n",
    "batch_size = N\n",
    "maxiter = 1000\n",
    "miniter = 1000\n",
    "\n",
    "# KDE set up\n",
    "kde_learn_rate = 0.1\n",
    "kde_miniter = 200\n",
    "kde_maxiter = 1000\n",
    "kde_tol = 1e-4\n",
    "kde_nfold = 5\n",
    "kde_reg = 1e-10\n",
    "\n",
    "# Samples to draw \n",
    "mc_samples = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names and dimensions\n",
    "names = ([\"Gaussian SCM\"]+\n",
    "         [\"RQS SCM {0}\".format(i) for i in RQS_bins]+\n",
    "         [\"Cocycles {0}\".format(i) for i in cocycle_estimators]+\n",
    "         [\"Cocycles {0} KDE\".format(i) for i in cocycle_estimators]+\n",
    "         [\"True\"])\n",
    "n_model = len(names)\n",
    "n_int = len(int_levels)\n",
    "n_quantile = len(quantiles)\n",
    "\n",
    "# Storage objects\n",
    "ATE = torch.zeros((n_model,n_int,trials))\n",
    "QTE = torch.zeros((n_model,n_int,trials,n_quantile))\n",
    "E_DO = torch.zeros((n_model,n_int,trials))\n",
    "Q_DO = torch.zeros((n_model,n_int,trials,n_quantile))\n",
    "Training_time = torch.zeros((n_model,trials))\n",
    "\n",
    "# True model construction\n",
    "true_models = []\n",
    "for i in range(len(parents)):\n",
    "    true_models.append(cocycle_model([Conditioner(i)],Shift_Transformer()))\n",
    "ccdagm_true = CCDAGM(true_models,parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss last 10 avg is : tensor(0.0320)\n",
      "Validation loss last 10 avg is : tensor(0.0126)\n",
      "Completion % : 98.2\n",
      "[Parameter containing:\n",
      "tensor([[0.9962]], requires_grad=True)]\n",
      "iter 0 , loss =  tensor(5537.1279)\n",
      "iter 10 , loss =  tensor(3573.7812)\n",
      "iter 20 , loss =  tensor(2492.8853)\n",
      "iter 30 , loss =  tensor(2185.9031)\n",
      "iter 40 , loss =  tensor(2172.2859)\n",
      "iter 50 , loss =  tensor(2161.5571)\n",
      "iter 60 , loss =  tensor(2144.9177)\n",
      "iter 70 , loss =  tensor(2147.3618)\n",
      "iter 80 , loss =  tensor(2145.0269)\n",
      "iter 90 , loss =  tensor(2144.9583)\n",
      "iter 100 , loss =  tensor(2144.8535)\n",
      "iter 110 , loss =  tensor(2144.7705)\n",
      "iter 120 , loss =  tensor(2144.7764)\n",
      "iter 130 , loss =  tensor(2144.7627)\n",
      "iter 140 , loss =  tensor(2144.7637)\n",
      "iter 150 , loss =  tensor(2144.7595)\n",
      "iter 160 , loss =  tensor(2144.7676)\n",
      "iter 170 , loss =  tensor(2144.7646)\n",
      "iter 180 , loss =  tensor(2144.7561)\n",
      "iter 190 , loss =  tensor(2144.7546)\n",
      "iter 200 , loss =  tensor(2144.7510)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initiating trials\n",
    "for t in range(trials):\n",
    "    \n",
    "    torch.manual_seed(t)\n",
    "    \n",
    "    # Draw data\n",
    "    Xobs,Xint = DGP(N, Nint, True, intervention, int_levels, adversarial_distribution, R2 = R2)\n",
    "    Xobs,Xobstest = Xobs[:N],Xobs[N:]\n",
    "    \n",
    "    # Estimate models\n",
    "    ls_models = []\n",
    "    cocycle_models = [[] for i in range(len(cocycle_estimators))]\n",
    "    RQS_models = [[]for i in range(len(RQS_bins))]\n",
    "    \n",
    "    for i in range(len(parents)):\n",
    "        \n",
    "        # Getting relevant variables from graph\n",
    "        index_x,index_y = parents[i],[i]\n",
    "        X,Y = Xobs[:,index_x].view(N,len(index_x)),Xobs[:,index_y].view(N,len(index_y))\n",
    "        if i==2:\n",
    "            X = torch.exp(X)\n",
    "          \n",
    "        # Data Preprocessing\n",
    "        Xtrain,Ytrain,Xval,Yval = X[:ntrain],Y[:ntrain],X[ntrain:],Y[ntrain:]\n",
    "        inputs_train,outputs_train, inputs_val,outputs_val = Xtrain,Ytrain,Xval,Yval\n",
    "\n",
    "        # Defining and training Gaussian SCM\n",
    "        if Gaussian_SCM:\n",
    "            start_time = time.time()\n",
    "            loss_fn = Loss(loss_fn = \"MLE\")\n",
    "            if len(parents[i])>0:\n",
    "                conditioner_shift = Lin_Conditioner(1,1, True)\n",
    "                conditioner_scale = Constant_Conditioner(init = torch.log(outputs_train.var()**0.5/2))\n",
    "            else:\n",
    "                conditioner_shift = Constant_Conditioner(init = outputs_train.mean())\n",
    "                conditioner_scale = Constant_Conditioner(init = torch.log(outputs_train.var()**0.5))\n",
    "            transformer = Affine_Transformer(log_det = True)\n",
    "            ls_model = cocycle_model([conditioner_shift,conditioner_scale],transformer)\n",
    "            ls_model = train(ls_model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,\n",
    "                                               scheduler = scheduler)\n",
    "            ls_model.transformer.ld = False # set log determinant to false after\n",
    "            ls_models.append(ls_model)\n",
    "\n",
    "            # Storing training time\n",
    "            Training_time[0,t] +=  time.time() - start_time\n",
    "\n",
    "        # Defining and training cocycle models\n",
    "        if len(parents[i])>0:\n",
    "            for m in range(len(cocycle_estimators)):\n",
    "                start_time = time.time()\n",
    "                loss_fn = Loss(loss_fn = cocycle_estimators[m],kernel = [gaussian_kernel(torch.ones(1),1),gaussian_kernel(torch.ones(1),1)])\n",
    "                if RFF_features[m]:\n",
    "                    loss_fn.get_RFF_features(n_RFF[m])\n",
    "                if median_heuristic[m]:\n",
    "                    loss_fn.median_heuristic(inputs_train,outputs_train, subsamples = 10**4)\n",
    "                conditioner = Lin_Conditioner(1,1,False)\n",
    "                transformer = Shift_Transformer()\n",
    "                model = cocycle_model([conditioner],transformer)\n",
    "                model = train(model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,\n",
    "                                             scheduler = scheduler)\n",
    "                cocycle_models[m].append(model)\n",
    "                \n",
    "                end_time = time.time()\n",
    "\n",
    "                Training_time[1+len(RQS_bins)+m,t] += end_time - start_time\n",
    "                Training_time[1+len(RQS_bins)+len(cocycle_estimators)+m,t] += end_time - start_time\n",
    "        else:\n",
    "            for m in range(len(cocycle_estimators)):\n",
    "                cocycle_models[m].append([])\n",
    "\n",
    "        # Defining and training RQS flow models\n",
    "        start_time = time.time()\n",
    "        loss_fn = Loss(loss_fn = \"MLE\")\n",
    "        for m in range(len(RQS_bins)):\n",
    "            if len(parents[i])>0:\n",
    "                conditioner = Lin_Conditioner(1,1,True)\n",
    "            else:\n",
    "                conditioner = Empty_Conditioner()\n",
    "            transformer =  RQS_Shift_Transformer(widths = torch.zeros((1,RQS_bins[m]),requires_grad = True),\n",
    "                                   heights = torch.zeros((1,RQS_bins[m]), requires_grad = True),\n",
    "                                   derivatives = torch.zeros((1,RQS_bins[m]+1), requires_grad = True)\n",
    "                                   )\n",
    "            RQS_model = cocycle_model([conditioner],transformer)\n",
    "            RQS_model = train(RQS_model).optimise(loss_fn,inputs_train,outputs_train,inputs_val,outputs_val, batch_size = batch_size,conditioner_learn_rate = conditioner_learn_rate,\n",
    "                                                     transformer_learn_rate = transformer_learn_rate,print_ = True,plot = False, miniter = miniter,maxiter = maxiter, val_tol = val_tol,\n",
    "                                                 scheduler = scheduler)\n",
    "            RQS_model.transformer.ld = False # set log determinant to false after\n",
    "            RQS_models[m].append(RQS_model)\n",
    "\n",
    "            Training_time[1+m,t] += time.time() - start_time\n",
    "\n",
    "    # KDE estimation for cocycles\n",
    "    start_time = time.time()\n",
    "    kernel = inverse_gaussian_kernel(torch.ones(len(Xobs.T),requires_grad = True),1)\n",
    "    KDE_model = KDE(kernel)\n",
    "    losses = KDE_model.optimise(Xobs,kde_learn_rate,kde_miniter,kde_maxiter,kde_tol,kde_nfold,kde_reg)\n",
    "    Training_time[1+len(cocycle_estimators)+len(RQS_bins):-1,t] += time.time() - start_time\n",
    "    \n",
    "    # Defining cocycle CDAGMs\n",
    "    ccdagm = []\n",
    "    for m in range(len(cocycle_estimators)):\n",
    "        ccdagm.append(CCDAGM(cocycle_models[m],parents))\n",
    "\n",
    "    # Interventional prediction\n",
    "    print(t)\n",
    "    for i in range(len(int_levels)):\n",
    "        \n",
    "        # Setting intervention level\n",
    "        a = int_levels[i]\n",
    "        \n",
    "        # Getting interventional_samples\n",
    "        Xpred = [],[]\n",
    "        for m in range(n_model):\n",
    "            if m == 0 and Gaussian_SCM:\n",
    "                Xpred,Xintpred = SCM_intervention_sample(parents,ls_models,[Normal(0,1)]*len(parents),intervention,[[a,\"id\",\"id\",\"id\"]],mc_samples)\n",
    "                Xintpred = Xintpred[0]\n",
    "            elif m > 0 and m <= len(RQS_bins):\n",
    "                Xpred,Xintpred = SCM_intervention_sample(parents,RQS_models[m-1],[Normal(0,1)]*len(parents),intervention,[[a,\"id\",\"id\",\"id\"]],mc_samples)\n",
    "                Xintpred = Xintpred[0]\n",
    "            elif m > len(RQS_bins) and m <= len(RQS_bins)+len(cocycle_estimators):\n",
    "                Xpred,Xintpred = ccdagm[m-len(RQS_bins)-1].interventional_dist_sample(Xobs,intervention,[a,\"id\",\"id\",\"id\"],len(Xobs),uniform_subsample = False)\n",
    "            elif m > len(RQS_bins) + len(cocycle_estimators) and m < n_model -1:\n",
    "                Xpred,Xintpred = ccdagm[m-len(RQS_bins)-len(cocycle_estimators)-1].interventional_dist_sample(Xobs,intervention,[a,\"id\",\"id\",\"id\"],mc_samples,density = KDE_model,uniform_subsample = True)\n",
    "            else:\n",
    "                Xpred,Xintpred = ccdagm_true.interventional_dist_sample(Xobstest,intervention,[a,\"id\",\"id\",\"id\"],samples = 10**7,density = [],uniform_subsample = False)\n",
    "                                  \n",
    "            E_DO[m,i,t] = Xintpred[:,-1].mean()\n",
    "            ATE[m,i,t] = E_DO[m,i,t] - Xpred[:,-1].mean()\n",
    "        \n",
    "            if store_quantiles:\n",
    "                k = 0\n",
    "                for q in quantiles:\n",
    "                    Q_DO[m,i,t,k] = Xintpred[:,-1].quantile(q)\n",
    "                    QTE[m,i,t,k] = Q_DO[m,i,t,k] - Xintpred[:,-1].quantile(q)\n",
    "                    k += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving output\n",
    "os.chdir('C:/Users/{0}/OneDrive/Documents/Cocycles project'.format(user))\n",
    "torch.save({ \"names\": names, \n",
    "            \"ATE (models x int_levels x trials)\": ATE,\n",
    "            \"QTE (models x int_levels x trials x quantiles)\": QTE,\n",
    "            \"EY|DO(X) (models x int_levels x trials)\": E_DO,\n",
    "            \"QY|DO(X) (models x int_levels x trials x quantiles)\": Q_DO,\n",
    "            \"Training time ( models x trials)\": Training_time\n",
    "           },\n",
    "           f = f'Experimental_results/'+'Simple_SCM_adversarial={0}_trials={1}_R2={2}_batchsize={3}.pt'.format(adversarial_distribution,trials, R2,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.7747, 2.4163, 5.5251, 9.7310]) Gaussian SCM\n",
      "tensor([0.0000, 0.7272, 2.3211, 5.3824, 9.5502]) Cocycles CMMD_M_RFF\n",
      "tensor([0.0000, 0.7217, 2.3102, 5.3660, 9.5295]) Cocycles JMMD_M_RFF\n",
      "tensor([0.0000, 0.7272, 2.3211, 5.3824, 9.5502]) Cocycles CMMD_M_RFF KDE\n",
      "tensor([0.0000, 0.7217, 2.3102, 5.3660, 9.5295]) Cocycles JMMD_M_RFF KDE\n",
      "tensor([0., 0., 0., 0., 0.]) True\n"
     ]
    }
   ],
   "source": [
    "ints = [0,5,10,15,19]\n",
    "for i in range(n_model):\n",
    "    print ((torch.abs(ATE[i,ints,:t+1]-ATE[-1,ints,:t+1])**2).mean(1)**0.5, names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0165, 0.0427, 0.0197, 0.0297, 0.0818]) Gaussian SCM\n",
      "tensor([0.0596, 0.0625, 0.0208, 0.0455, 0.1094]) RQS SCM 2\n",
      "tensor([0.0052, 0.0531, 0.0200, 0.0372, 0.0107]) RQS SCM 4\n",
      "tensor([0.0142, 0.0149, 0.0351, 0.0058, 0.0230]) RQS SCM 8\n",
      "tensor([0.0210, 0.0338, 0.0419, 0.0574, 0.0411]) Cocycles CLS_M\n",
      "tensor([0.0182, 0.0134, 0.0132, 0.1109, 0.0947]) Cocycles CMMD_M_RFF\n",
      "tensor([0.0223, 0.0012, 0.0526, 0.0574, 0.1265]) Cocycles JMMD_M_RFF\n",
      "tensor([0.0227, 0.0362, 0.0438, 0.0603, 0.0411]) Cocycles CLS_M KDE\n",
      "tensor([0.0197, 0.0141, 0.0157, 0.1124, 0.0963]) Cocycles CMMD_M_RFF KDE\n",
      "tensor([0.0237, 0.0065, 0.0503, 0.0560, 0.1282]) Cocycles JMMD_M_RFF KDE\n",
      "tensor([0., 0., 0., 0., 0.]) True\n"
     ]
    }
   ],
   "source": [
    "# NonlinGauss Gauss\n",
    "ints = [0,1,2,3,4]\n",
    "for i in range(n_model):\n",
    "    print ((torch.abs(ATE[i,ints,:t+1]-ATE[-1,ints,:t+1])**2).mean(1)**0.5, names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2679, 0.5764, 1.0512, 0.8755, 4.2517]) Gaussian SCM\n",
      "tensor([0.4845, 0.2934, 0.4476, 0.3895, 4.1084]) RQS SCM 2\n",
      "tensor([0.5963, 0.2579, 0.4390, 0.3138, 4.5994]) RQS SCM 4\n",
      "tensor([0.2796, 0.3244, 0.2105, 0.4409, 4.5236]) RQS SCM 8\n",
      "tensor([0.3786, 0.0422, 0.0498, 0.1297, 3.9917]) Cocycles CLS_M\n",
      "tensor([0.1821, 0.0924, 0.1078, 0.2357, 4.7863]) Cocycles CMMD_M_RFF\n",
      "tensor([0.2309, 0.0479, 0.0422, 0.3763, 4.4364]) Cocycles JMMD_M_RFF\n",
      "tensor([0.3225, 0.0517, 0.1307, 0.2402, 3.9522]) Cocycles CLS_M KDE\n",
      "tensor([0.1354, 0.0807, 0.0176, 0.3041, 4.6749]) Cocycles CMMD_M_RFF KDE\n",
      "tensor([0.1768, 0.1176, 0.0385, 0.4557, 4.3503]) Cocycles JMMD_M_RFF KDE\n",
      "tensor([0., 0., 0., 0., 0.]) True\n"
     ]
    }
   ],
   "source": [
    "# Fork adverse 1\n",
    "ints = [0,1,2,3,4]\n",
    "for i in range(n_model):\n",
    "    print ((torch.abs(ATE[i,ints,:t+1]-ATE[-1,ints,:t+1])**2).mean(1)**0.5, names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1955, 0.1644, 0.2406, 0.1722, 0.2436]) Gaussian SCM\n",
      "tensor([0.0591, 0.0347, 0.0025, 0.0220, 0.0417]) RQS SCM 2\n",
      "tensor([0.0002, 0.0729, 0.1041, 0.0864, 0.1201]) RQS SCM 4\n",
      "tensor([0.0834, 0.0011, 0.0267, 0.0383, 0.0899]) RQS SCM 8\n",
      "tensor([0.0135, 0.0074, 0.0161, 0.0627, 0.0023]) Cocycles CLS_M\n",
      "tensor([0.0017, 0.0219, 0.0094, 0.0228, 0.0984]) Cocycles CMMD_M_RFF\n",
      "tensor([0.0128, 0.0260, 0.0336, 0.0334, 0.0701]) Cocycles JMMD_M_RFF\n",
      "tensor([0.0123, 0.0094, 0.0141, 0.0594, 0.0004]) Cocycles CLS_M KDE\n",
      "tensor([0.0029, 0.0183, 0.0056, 0.0231, 0.0967]) Cocycles CMMD_M_RFF KDE\n",
      "tensor([0.0143, 0.0247, 0.0342, 0.0350, 0.0709]) Cocycles JMMD_M_RFF KDE\n",
      "tensor([0., 0., 0., 0., 0.]) True\n"
     ]
    }
   ],
   "source": [
    "# NonlinG adverse 1\n",
    "ints = [0,1,2,3,4]\n",
    "for i in range(n_model):\n",
    "    print ((torch.abs(ATE[i,ints,:t+1]-ATE[-1,ints,:t+1])**2).mean(1)**0.5, names[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
